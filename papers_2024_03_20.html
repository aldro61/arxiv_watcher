
        <html>
        <head>
            <title>Report Generated on March 20, 2024</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .paper-box {
                    background-color: #f0f0f0;
                    margin-bottom: 20px;
                    padding: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                h1 { text-align: center; }
                h2 {
                    color: #333;
                    border-bottom: 2px solid #666;
                }
                a { color: #337ab7; text-decoration: none; }
                a:hover { text-decoration: underline; }
                h3 { color: #337ab7; }
                .timestamp { text-align: center; font-size: small; margin-top: 40px; }
            </style>
        </head>
        <body>
        <h1>Report for March 20, 2024</h1>
        <h2>Time-series</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems</h3>
            <p><strong>Authors:</strong> Indranil Nayak, Debdipta Goswami, Mrinal Kumar, Fernando Teixeira</p>
            <p><strong>Summary:</strong> arXiv:2403.12335v1 Announce Type: new 
Abstract: Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE's superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12335" target="_blank">https://arxiv.org/abs/2403.12335</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Advancing Time Series Classification with Multimodal Language Modeling</h3>
            <p><strong>Authors:</strong> Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo</p>
            <p><strong>Summary:</strong> arXiv:2403.12371v1 Announce Type: new 
Abstract: For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specific instructions and raw time series are treated as multimodal inputs while the label information is represented by texts. To accomplish this goal, three distinct designs are developed in the InstructTime. Firstly, a time series discretization module is designed to convert continuous time series into a sequence of hard tokens to solve the inconsistency issue across modal inputs. To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models. For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance. Extensive experiments are conducted over benchmark datasets, whose results uncover the superior performance of InstructTime and the potential for a universal foundation model in time series classification.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12371" target="_blank">https://arxiv.org/abs/2403.12371</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Multimodal methods for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model</h3>
            <p><strong>Authors:</strong> Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, Chenyi Lei</p>
            <p><strong>Summary:</strong> arXiv:2403.12372v1 Announce Type: new 
Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a sequence of discrete tokens based on a reconstruction optimization process. Besides, we highlight that predicting a high proportion of corrupted tokens can be very helpful for extracting informative patterns across different domains during SSL pre-training, which has been largely overlooked in past years. Furthermore, unlike previous works, our work treats the pre-training language model (PLM) as the initialization of the encoder network, investigating the feasibility of transferring the knowledge learned by the PLM to the time series area. Through these efforts, the path to cross-domain pre-training of a generic time series model can be effectively paved. We conduct extensive experiments in a real-world scenario across various time series classification domains. The experimental results clearly confirm CrossTimeNet's superior performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12372" target="_blank">https://arxiv.org/abs/2403.12372</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Automated Contrastive Learning Strategy Search for Time Series</h3>
            <p><strong>Authors:</strong> Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, Kan Ren</p>
            <p><strong>Summary:</strong> arXiv:2403.12641v1 Announce Type: new 
Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses. Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space. Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task. From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets. We also provide empirical analysis as a guidance for future design of CLS.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12641" target="_blank">https://arxiv.org/abs/2403.12641</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection</h3>
            <p><strong>Authors:</strong> Feiyi Chen, Yingying zhang, Zhen Qin, Lunting Fan, Renhe Jiang, Yuxuan Liang, Qingsong Wen, Shuiguang Deng</p>
            <p><strong>Summary:</strong> arXiv:2311.16191v2 Announce Type: replace 
Abstract: Anomaly detection significantly enhances the robustness of cloud systems. While neural network-based methods have recently demonstrated strong advantages, they encounter practical challenges in cloud environments: the contradiction between the impracticality of maintaining a unique model for each service and the limited ability to deal with diverse normal patterns by a unified model, as well as issues with handling heavy traffic in real time and short-term anomaly detection sensitivity.
  Thus, we propose MACE, a multi-normal-pattern accommodated and efficient anomaly detection method in the frequency domain for time series anomaly detection. There are three novel characteristics of it: (i) a pattern extraction mechanism excelling at handling diverse normal patterns with a unified model, which enables the model to identify anomalies by examining the correlation between the data sample and its service normal pattern, instead of solely focusing on the data sample itself; (ii) a dualistic convolution mechanism that amplifies short-term anomalies in the time domain and hinders the reconstruction of anomalies in the frequency domain, which enlarges the reconstruction error disparity between anomaly and normality and facilitates anomaly detection; (iii) leveraging the sparsity and parallelism of frequency domain to enhance model efficiency. We theoretically and experimentally prove that using a strategically selected subset of Fourier bases can not only reduce computational overhead but is also profitable to distinguish anomalies, compared to using the complete spectrum. Moreover, extensive experiments demonstrate MACE's effectiveness in handling diverse normal patterns with a unified model and it achieves state-of-the-art performance with high efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.16191" target="_blank">https://arxiv.org/abs/2311.16191</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Large-scale flood modeling and forecasting with FloodCast</h3>
            <p><strong>Authors:</strong> Qingsong Xu, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang Zhu</p>
            <p><strong>Summary:</strong> arXiv:2403.12226v1 Announce Type: new 
Abstract: Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12226" target="_blank">https://arxiv.org/abs/2403.12226</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection</h3>
            <p><strong>Authors:</strong> Daniel Lakey, Tim Schlippe</p>
            <p><strong>Summary:</strong> arXiv:2403.12864v1 Announce Type: new 
Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12864" target="_blank">https://arxiv.org/abs/2403.12864</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Copula Conformal Prediction for Multi-step Time Series Forecasting</h3>
            <p><strong>Authors:</strong> Sophia Sun, Rose Yu</p>
            <p><strong>Summary:</strong> arXiv:2212.03281v4 Announce Type: replace 
Abstract: Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper, we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2212.03281" target="_blank">https://arxiv.org/abs/2212.03281</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram</h3>
            <p><strong>Authors:</strong> Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo</p>
            <p><strong>Summary:</strong> arXiv:2402.09450v3 Announce Type: replace-cross 
Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is adaptable to various lead combinations. Through quantitative and qualitative analysis, we show a spatio-temporal relationship within ECG data. Our code is available at https://github.com/bakqui/ST-MEM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09450" target="_blank">https://arxiv.org/abs/2402.09450</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        </div><h2>Llm-agents</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</h3>
            <p><strong>Authors:</strong> Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan Du, Yongfeng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.07311v4 Announce Type: replace-cross 
Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.07311" target="_blank">https://arxiv.org/abs/2403.07311</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>MELTing point: Mobile Evaluation of Language Transformers</h3>
            <p><strong>Authors:</strong> Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, Hamed Haddadi</p>
            <p><strong>Summary:</strong> arXiv:2403.12844v1 Announce Type: new 
Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.
  Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound. Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12844" target="_blank">https://arxiv.org/abs/2403.12844</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Are LLMs Good Cryptic Crossword Solvers?</h3>
            <p><strong>Authors:</strong> Abdelrahman "Boda" Sadallah, Daria Kotova, Ekaterina Kochmar</p>
            <p><strong>Summary:</strong> arXiv:2403.12094v1 Announce Type: cross 
Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12094" target="_blank">https://arxiv.org/abs/2403.12094</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Reference-based Metrics Disprove Themselves in Question Generation</h3>
            <p><strong>Authors:</strong> Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang</p>
            <p><strong>Summary:</strong> arXiv:2403.12242v1 Announce Type: cross 
Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12242" target="_blank">https://arxiv.org/abs/2403.12242</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales</h3>
            <p><strong>Authors:</strong> Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu</p>
            <p><strong>Summary:</strong> arXiv:2403.12403v1 Announce Type: cross 
Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12403" target="_blank">https://arxiv.org/abs/2403.12403</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based web agents)</p>
        </div>
        
        <div class="paper-box">
            <h3>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</h3>
            <p><strong>Authors:</strong> Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</p>
            <p><strong>Summary:</strong> arXiv:2403.12503v1 Announce Type: cross 
Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12503" target="_blank">https://arxiv.org/abs/2403.12503</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Yell At Your Robot: Improving On-the-Fly from Language Corrections</h3>
            <p><strong>Authors:</strong> Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn</p>
            <p><strong>Summary:</strong> arXiv:2403.12910v1 Announce Type: cross 
Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a bit to the left"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12910" target="_blank">https://arxiv.org/abs/2403.12910</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</h3>
            <p><strong>Authors:</strong> Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R\"uhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.12968v1 Announce Type: cross 
Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12968" target="_blank">https://arxiv.org/abs/2403.12968</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h3>
            <p><strong>Authors:</strong> Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2308.07269v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.07269" target="_blank">https://arxiv.org/abs/2308.07269</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</h3>
            <p><strong>Authors:</strong> Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang</p>
            <p><strong>Summary:</strong> arXiv:2310.12547v2 Announce Type: replace-cross 
Abstract: Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user's environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant efficiency while previous LCRG systems rely on resource-intensive human annotations -- necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA's real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12547" target="_blank">https://arxiv.org/abs/2310.12547</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</h3>
            <p><strong>Authors:</strong> Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran</p>
            <p><strong>Summary:</strong> arXiv:2311.10081v2 Announce Type: replace-cross 
Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.10081" target="_blank">https://arxiv.org/abs/2311.10081</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</h3>
            <p><strong>Authors:</strong> Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</p>
            <p><strong>Summary:</strong> arXiv:2402.09283v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09283" target="_blank">https://arxiv.org/abs/2402.09283</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</h3>
            <p><strong>Authors:</strong> Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C. Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace</p>
            <p><strong>Summary:</strong> arXiv:2402.10109v2 Announce Type: replace-cross 
Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10109" target="_blank">https://arxiv.org/abs/2402.10109</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Interactive Continual Learning: Fast and Slow Thinking</h3>
            <p><strong>Authors:</strong> Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou</p>
            <p><strong>Summary:</strong> arXiv:2403.02628v2 Announce Type: replace-cross 
Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods. Code is available at github.com/ICL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.02628" target="_blank">https://arxiv.org/abs/2403.02628</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents (topic: LLM-based agents that interact with software)</p>
        </div>
        </div><h2>Causality</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects</h3>
            <p><strong>Authors:</strong> Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke</p>
            <p><strong>Summary:</strong> arXiv:2402.04906v2 Announce Type: replace 
Abstract: Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04906" target="_blank">https://arxiv.org/abs/2402.04906</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal representation learning)</p>
        </div>
        
        <div class="paper-box">
            <h3>Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation</h3>
            <p><strong>Authors:</strong> Yiling Kuang, Chao Yang, Yang Yang, Shuang Li</p>
            <p><strong>Summary:</strong> arXiv:2402.05946v2 Announce Type: replace 
Abstract: In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using synthetic and real healthcare datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05946" target="_blank">https://arxiv.org/abs/2402.05946</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal discovery)</p>
        </div>
        
        <div class="paper-box">
            <h3>Toward a Theory of Causation for Interpreting Neural Code Models</h3>
            <p><strong>Authors:</strong> David N. Palacio, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk</p>
            <p><strong>Summary:</strong> arXiv:2302.03788v2 Announce Type: replace-cross 
Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of $do_{code}$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of $do_{code}$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2302.03788" target="_blank">https://arxiv.org/abs/2302.03788</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal representation learning)</p>
        </div>
        </div><div class='timestamp'>Report generated on March 20, 2024 at 23:39:02</div></body></html>