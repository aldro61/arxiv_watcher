
        <html>
        <head>
            <title>Report Generated on March 20, 2024</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .paper-box {
                    background-color: #f0f0f0;
                    margin-bottom: 20px;
                    padding: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                h1 { text-align: center; }
                h2 {
                    color: #333;
                    border-bottom: 2px solid #666;
                }
                a { color: #337ab7; text-decoration: none; }
                a:hover { text-decoration: underline; }
                h3 { color: #337ab7; }
                .timestamp { text-align: center; font-size: small; margin-top: 40px; }
            </style>
        </head>
        <body>
        <h1>Report for March 20, 2024</h1>
        <h2>Causality</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>GCAM: Gaussian and causal-attention model of food fine-grained recognition</h3>
            <p><strong>Authors:</strong> Guohang Zhuang, Yue Hu, Tianxing Yan, JiaZhan Gao</p>
            <p><strong>Summary:</strong> arXiv:2403.12109v1 Announce Type: new 
Abstract: Currently, most food recognition relies on deep learning for category classification. However, these approaches struggle to effectively distinguish between visually similar food samples, highlighting the pressing need to address fine-grained issues in food recognition. To mitigate these challenges, we propose the adoption of a Gaussian and causal-attention model for fine-grained object recognition.In particular, we train to obtain Gaussian features over target regions, followed by the extraction of fine-grained features from the objects, thereby enhancing the feature mapping capabilities of the target regions. To counteract data drift resulting from uneven data distributions, we employ a counterfactual reasoning approach. By using counterfactual interventions, we analyze the impact of the learned image attention mechanism on network predictions, enabling the network to acquire more useful attention weights for fine-grained image recognition. Finally, we design a learnable loss strategy to balance training stability across various modules, ultimately improving the accuracy of the final target recognition. We validate our approach on four relevant datasets, demonstrating its excellent performance across these four datasets.We experimentally show that GCAM surpasses state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and Vireo-FOOD172 datasets. Furthermore, our approach also achieves state-of-the-art performance on the CUB-200 dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12109" target="_blank">https://arxiv.org/abs/2403.12109</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal representation learning)</p>
        </div>
        </div><h2>Time-series</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems</h3>
            <p><strong>Authors:</strong> Indranil Nayak, Debdipta Goswami, Mrinal Kumar, Fernando Teixeira</p>
            <p><strong>Summary:</strong> arXiv:2403.12335v1 Announce Type: new 
Abstract: Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE's superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12335" target="_blank">https://arxiv.org/abs/2403.12335</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        </div><div class='timestamp'>Report generated on March 20, 2024 at 22:58:28</div></body></html>