
        <html>
        <head>
            <title>Report Generated on March 20, 2024</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .paper-box {
                    background-color: #f0f0f0;
                    margin-bottom: 20px;
                    padding: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                h1 { text-align: center; }
                h2 {
                    color: #333;
                    border-bottom: 2px solid #666;
                }
                a { color: #337ab7; text-decoration: none; }
                a:hover { text-decoration: underline; }
                h3 { color: #337ab7; }
                .timestamp { text-align: center; font-size: small; margin-top: 40px; }
            </style>
        </head>
        <body>
        <h1>Report for March 20, 2024</h1>
        <h2>Time-series</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Advancing Time Series Classification with Multimodal Language Modeling</h3>
            <p><strong>Authors:</strong> Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo</p>
            <p><strong>Summary:</strong> arXiv:2403.12371v1 Announce Type: new 
Abstract: For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specific instructions and raw time series are treated as multimodal inputs while the label information is represented by texts. To accomplish this goal, three distinct designs are developed in the InstructTime. Firstly, a time series discretization module is designed to convert continuous time series into a sequence of hard tokens to solve the inconsistency issue across modal inputs. To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models. For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance. Extensive experiments are conducted over benchmark datasets, whose results uncover the superior performance of InstructTime and the potential for a universal foundation model in time series classification.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12371" target="_blank">https://arxiv.org/abs/2403.12371</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model</h3>
            <p><strong>Authors:</strong> Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, Chenyi Lei</p>
            <p><strong>Summary:</strong> arXiv:2403.12372v1 Announce Type: new 
Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a sequence of discrete tokens based on a reconstruction optimization process. Besides, we highlight that predicting a high proportion of corrupted tokens can be very helpful for extracting informative patterns across different domains during SSL pre-training, which has been largely overlooked in past years. Furthermore, unlike previous works, our work treats the pre-training language model (PLM) as the initialization of the encoder network, investigating the feasibility of transferring the knowledge learned by the PLM to the time series area. Through these efforts, the path to cross-domain pre-training of a generic time series model can be effectively paved. We conduct extensive experiments in a real-world scenario across various time series classification domains. The experimental results clearly confirm CrossTimeNet's superior performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12372" target="_blank">https://arxiv.org/abs/2403.12372</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Large-scale flood modeling and forecasting with FloodCast</h3>
            <p><strong>Authors:</strong> Qingsong Xu, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang Zhu</p>
            <p><strong>Summary:</strong> arXiv:2403.12226v1 Announce Type: new 
Abstract: Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12226" target="_blank">https://arxiv.org/abs/2403.12226</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Automated Contrastive Learning Strategy Search for Time Series</h3>
            <p><strong>Authors:</strong> Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, Kan Ren</p>
            <p><strong>Summary:</strong> arXiv:2403.12641v1 Announce Type: new 
Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses. Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space. Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task. From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets. We also provide empirical analysis as a guidance for future design of CLS.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12641" target="_blank">https://arxiv.org/abs/2403.12641</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection</h3>
            <p><strong>Authors:</strong> Daniel Lakey, Tim Schlippe</p>
            <p><strong>Summary:</strong> arXiv:2403.12864v1 Announce Type: new 
Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12864" target="_blank">https://arxiv.org/abs/2403.12864</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection</h3>
            <p><strong>Authors:</strong> Feiyi Chen, Yingying zhang, Zhen Qin, Lunting Fan, Renhe Jiang, Yuxuan Liang, Qingsong Wen, Shuiguang Deng</p>
            <p><strong>Summary:</strong> arXiv:2311.16191v2 Announce Type: replace 
Abstract: Anomaly detection significantly enhances the robustness of cloud systems. While neural network-based methods have recently demonstrated strong advantages, they encounter practical challenges in cloud environments: the contradiction between the impracticality of maintaining a unique model for each service and the limited ability to deal with diverse normal patterns by a unified model, as well as issues with handling heavy traffic in real time and short-term anomaly detection sensitivity.
  Thus, we propose MACE, a multi-normal-pattern accommodated and efficient anomaly detection method in the frequency domain for time series anomaly detection. There are three novel characteristics of it: (i) a pattern extraction mechanism excelling at handling diverse normal patterns with a unified model, which enables the model to identify anomalies by examining the correlation between the data sample and its service normal pattern, instead of solely focusing on the data sample itself; (ii) a dualistic convolution mechanism that amplifies short-term anomalies in the time domain and hinders the reconstruction of anomalies in the frequency domain, which enlarges the reconstruction error disparity between anomaly and normality and facilitates anomaly detection; (iii) leveraging the sparsity and parallelism of frequency domain to enhance model efficiency. We theoretically and experimentally prove that using a strategically selected subset of Fourier bases can not only reduce computational overhead but is also profitable to distinguish anomalies, compared to using the complete spectrum. Moreover, extensive experiments demonstrate MACE's effectiveness in handling diverse normal patterns with a unified model and it achieves state-of-the-art performance with high efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.16191" target="_blank">https://arxiv.org/abs/2311.16191</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        
        <div class="paper-box">
            <h3>Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram</h3>
            <p><strong>Authors:</strong> Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo</p>
            <p><strong>Summary:</strong> arXiv:2402.09450v3 Announce Type: replace-cross 
Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is adaptable to various lead combinations. Through quantitative and qualitative analysis, we show a spatio-temporal relationship within ECG data. Our code is available at https://github.com/bakqui/ST-MEM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09450" target="_blank">https://arxiv.org/abs/2402.09450</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series (topic: Deep learning models for time series)</p>
        </div>
        </div><h2>Agents</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>AffineQuant: Affine Transformation Quantization for Large Language Models</h3>
            <p><strong>Authors:</strong> Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong Ji</p>
            <p><strong>Summary:</strong> arXiv:2403.12544v1 Announce Type: new 
Abstract: The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. As a result, significant performance improvements are evident across different LLMs on diverse datasets. To illustrate, we attain a C4 perplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model of W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using 4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art benchmark for PTQ in LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12544" target="_blank">https://arxiv.org/abs/2403.12544</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>MELTing point: Mobile Evaluation of Language Transformers</h3>
            <p><strong>Authors:</strong> Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, Hamed Haddadi</p>
            <p><strong>Summary:</strong> arXiv:2403.12844v1 Announce Type: new 
Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.
  Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound. Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12844" target="_blank">https://arxiv.org/abs/2403.12844</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Are LLMs Good Cryptic Crossword Solvers?</h3>
            <p><strong>Authors:</strong> Abdelrahman "Boda" Sadallah, Daria Kotova, Ekaterina Kochmar</p>
            <p><strong>Summary:</strong> arXiv:2403.12094v1 Announce Type: cross 
Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12094" target="_blank">https://arxiv.org/abs/2403.12094</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification</h3>
            <p><strong>Authors:</strong> Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</p>
            <p><strong>Summary:</strong> arXiv:2403.12151v1 Announce Type: cross 
Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art performance achieved by the proposed approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12151" target="_blank">https://arxiv.org/abs/2403.12151</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning</h3>
            <p><strong>Authors:</strong> Antonio Lopez, David Fridovich-Keil</p>
            <p><strong>Summary:</strong> arXiv:2403.12210v1 Announce Type: cross 
Abstract: Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we address by using a system decomposition technique to compute what we call Decomposed Control Lyapunov Functions (DCLFs). We use the computed DCLF for reward shaping, which we show improves RL performance. Through multiple examples, we demonstrate the effectiveness of this approach, where our method finds a policy to successfully land a quadcopter in less than half the amount of real-world data required by the state-of-the-art Soft-Actor Critic algorithm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12210" target="_blank">https://arxiv.org/abs/2403.12210</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Reference-based Metrics Disprove Themselves in Question Generation</h3>
            <p><strong>Authors:</strong> Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang</p>
            <p><strong>Summary:</strong> arXiv:2403.12242v1 Announce Type: cross 
Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12242" target="_blank">https://arxiv.org/abs/2403.12242</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>On Predictive planning and counterfactual learning in active inference</h3>
            <p><strong>Authors:</strong> Aswin Paul, Takuya Isomura, Adeel Razi</p>
            <p><strong>Summary:</strong> arXiv:2403.12417v1 Announce Type: cross 
Abstract: Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12417" target="_blank">https://arxiv.org/abs/2403.12417</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</h3>
            <p><strong>Authors:</strong> Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</p>
            <p><strong>Summary:</strong> arXiv:2403.12503v1 Announce Type: cross 
Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12503" target="_blank">https://arxiv.org/abs/2403.12503</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Yell At Your Robot: Improving On-the-Fly from Language Corrections</h3>
            <p><strong>Authors:</strong> Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn</p>
            <p><strong>Summary:</strong> arXiv:2403.12910v1 Announce Type: cross 
Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a bit to the left"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12910" target="_blank">https://arxiv.org/abs/2403.12910</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</h3>
            <p><strong>Authors:</strong> Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R\"uhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.12968v1 Announce Type: cross 
Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12968" target="_blank">https://arxiv.org/abs/2403.12968</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Language Modeling Is Compression</h3>
            <p><strong>Authors:</strong> Gr\'egoire Del\'etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness</p>
            <p><strong>Summary:</strong> arXiv:2309.10668v2 Announce Type: replace 
Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.10668" target="_blank">https://arxiv.org/abs/2309.10668</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Pointer Networks with Q-Learning for OP Combinatorial Optimization</h3>
            <p><strong>Authors:</strong> Alessandro Barro</p>
            <p><strong>Summary:</strong> arXiv:2311.02629v2 Announce Type: replace 
Abstract: The Orienteering Problem (OP) presents a unique challenge in Combinatorial Optimization (CO), emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP, and duties requiring focus on future return or exploration, leaves room for improvement. Recognizing the potency combining Reinforcement Learning (RL) methods with sequence-to-sequence models, this research unveils the Pointer Q-Network (PQN). This method combines Ptr-Nets and Q-learning, which, thanks to its critic only nature, outstands in its capability of capturing relationships within an embedded graph, a fundamental requirement in order to effectively address the specific challenges presented by OP. We explore the architecture and functionality of the PQN system, while showcasing its theoretical and practical advantages in terms of efficiency for combinatorial optimization problems such as the Orienteering Problem.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.02629" target="_blank">https://arxiv.org/abs/2311.02629</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>TrojFSP: Trojan Insertion in Few-shot Prompt Tuning</h3>
            <p><strong>Authors:</strong> Mengxin Zheng, Jiaqi Xue, Xun Chen, YanShan Wang, Qian Lou, Lei Jiang</p>
            <p><strong>Summary:</strong> arXiv:2312.10467v3 Announce Type: replace 
Abstract: Prompt tuning is one of the most effective solutions to adapting a fixed pre-trained language model (PLM) for various downstream tasks, especially with only a few input samples. However, the security issues, e.g., Trojan attacks, of prompt tuning on a few data samples are not well-studied. Transferring established data poisoning attacks directly to few-shot prompt tuning presents multiple challenges. One significant issue is the \textit{poisoned imbalance issue}, where non-target class samples are added to the target class, resulting in a greater number of target-class samples compared to non-target class. While this issue is not critical in regular tuning, it significantly hampers the few-shot prompt tuning, making it difficult to simultaneously achieve a high attack success rate (ASR) and maintain clean data accuracy (CDA). Additionally, few-shot prompting is prone to overfitting in terms of both ASR and CDA. In this paper, we introduce \textit{TrojFSP}, a method designed to address the challenges. To solve the poisoned imbalance issue, we develop a \textit{Target-Class Shrink (TC-Shrink)} technique, which aims to equalize the number of poisoning samples. To combat overfitting, we employ a \textit{Selective Token Poisoning} technique to boost attack performance. Furthermore, we introduce a \textit{Trojan-Trigger Attention} objective function to amplify the attention of the poisoned trojan prompt on triggers. Experiments show that our TrojFSP achieves an ASR of over 99\% while maintaining negligible decreases in CDA across various PLMs and datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.10467" target="_blank">https://arxiv.org/abs/2312.10467</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Riemannian Stochastic Gradient Method for Nested Composition Optimization</h3>
            <p><strong>Authors:</strong> Dewei Zhang, Sam Davanloo Tajbakhsh</p>
            <p><strong>Summary:</strong> arXiv:2207.09350v2 Announce Type: replace-cross 
Abstract: This work considers optimization of composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\epsilon$, in $O(\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algorithms for problems with multi-level nested compositional structures, with the same complexity of $O(\epsilon^{-2})$ for the first-order stochastic oracle. Finally, the performance of the R-SCGD method is numerically evaluated over a policy evaluation problem in reinforcement learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2207.09350" target="_blank">https://arxiv.org/abs/2207.09350</a></p>
            <p><strong>Category:</strong> math.OC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h3>
            <p><strong>Authors:</strong> Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2308.07269v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.07269" target="_blank">https://arxiv.org/abs/2308.07269</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</h3>
            <p><strong>Authors:</strong> Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang</p>
            <p><strong>Summary:</strong> arXiv:2310.12547v2 Announce Type: replace-cross 
Abstract: Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user's environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant efficiency while previous LCRG systems rely on resource-intensive human annotations -- necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA's real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12547" target="_blank">https://arxiv.org/abs/2310.12547</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</h3>
            <p><strong>Authors:</strong> Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran</p>
            <p><strong>Summary:</strong> arXiv:2311.10081v2 Announce Type: replace-cross 
Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.10081" target="_blank">https://arxiv.org/abs/2311.10081</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</h3>
            <p><strong>Authors:</strong> Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</p>
            <p><strong>Summary:</strong> arXiv:2402.09283v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09283" target="_blank">https://arxiv.org/abs/2402.09283</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>Interactive Continual Learning: Fast and Slow Thinking</h3>
            <p><strong>Authors:</strong> Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou</p>
            <p><strong>Summary:</strong> arXiv:2403.02628v2 Announce Type: replace-cross 
Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods. Code is available at github.com/ICL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.02628" target="_blank">https://arxiv.org/abs/2403.02628</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        
        <div class="paper-box">
            <h3>RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models</h3>
            <p><strong>Authors:</strong> Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.06420v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present a method for extracting the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated in a user-friendly manner. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ TD3, the widely-used RL baseline method, and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua can significantly reduce the sample complexity of TD3 in four robot tasks of panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot tasks in RLBench, where the standard TD3 fails. Additionally, We validated RLingua's effectiveness in real-world robot experiments through Sim2Real, demonstrating that the learned policies are effectively transferable to real robot tasks. Further details about our work are available at our project website https://rlingua.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.06420" target="_blank">https://arxiv.org/abs/2403.06420</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> agents (topic: Agents that interact with software)</p>
        </div>
        </div><h2>Causality</h2><div class='papers-container'>
        <div class="paper-box">
            <h3>Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation</h3>
            <p><strong>Authors:</strong> Yiling Kuang, Chao Yang, Yang Yang, Shuang Li</p>
            <p><strong>Summary:</strong> arXiv:2402.05946v2 Announce Type: replace 
Abstract: In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using synthetic and real healthcare datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05946" target="_blank">https://arxiv.org/abs/2402.05946</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal discovery)</p>
        </div>
        
        <div class="paper-box">
            <h3>Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects</h3>
            <p><strong>Authors:</strong> Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke</p>
            <p><strong>Summary:</strong> arXiv:2402.04906v2 Announce Type: replace 
Abstract: Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04906" target="_blank">https://arxiv.org/abs/2402.04906</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal representation learning)</p>
        </div>
        
        <div class="paper-box">
            <h3>Toward a Theory of Causation for Interpreting Neural Code Models</h3>
            <p><strong>Authors:</strong> David N. Palacio, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk</p>
            <p><strong>Summary:</strong> arXiv:2302.03788v2 Announce Type: replace-cross 
Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of $do_{code}$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of $do_{code}$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2302.03788" target="_blank">https://arxiv.org/abs/2302.03788</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality (topic: Causal representation learning)</p>
        </div>
        </div><div class='timestamp'>Report generated on March 20, 2024 at 23:27:40</div></body></html>