
            <html>
            <head>
                <title>Report Generated on March 26, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for March 26, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15711" target="_blank">Identifiable Latent Neural Causal Models</a></h3>
            <a href="https://arxiv.org/html/2403.15711v1/extracted/5490232/Figs/MLPGaussian.png" target="_blank"><img src="https://arxiv.org/html/2403.15711v1/extracted/5490232/Figs/MLPGaussian.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</p>
            <p><strong>Summary:</strong> arXiv:2403.15711v1 Announce Type: new 
Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15711">https://arxiv.org/abs/2403.15711</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'Causal representation learning' as it discusses the acquisition of reliable latent causal representations and how they can be identified from distribution shifts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.02695" target="_blank">Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions</a></h3>
            
            <p><strong>Authors:</strong> Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge</p>
            <p><strong>Summary:</strong> arXiv:2311.02695v2 Announce Type: replace-cross 
Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.02695">https://arxiv.org/abs/2311.02695</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper addresses your interest in 'Causal representation learning' and 'Causal discovery'. It presents a new approach for inferring high-level causal variables, which falls under your interest in proposing new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15499" target="_blank">A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners</a></h3>
            <a href="https://arxiv.org/html/2403.15499v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15499v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Iman Emtiazi Naeini, Zahra Saberi, Khadijeh Hassanzadeh</p>
            <p><strong>Summary:</strong> arXiv:2403.15499v1 Announce Type: new 
Abstract: This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study's findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question's specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development practices, emphasizing the importance of considering unintended consequences in policy formulation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15499">https://arxiv.org/abs/2403.15499</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses the implementation of machine learning methods in identifying causal relationships, specifically in terms of carbon dioxide levels and electricity pricing policies. It's a practical application of causal discovery and therefore is quite relevant to your interest in causality and machine learning. However, it might not thoroughly cover all aspects of causal representation learning or the use of large language models you've mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15500" target="_blank">Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View</a></h3>
            <a href="https://arxiv.org/html/2403.15500v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15500v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoyue Dai, Ignavier Ng, Gongxu Luo, Peter Spirtes, Petar Stojanov, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.15500v1 Announce Type: cross 
Abstract: Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptotically identical to the CI relations in the original data without dropouts. This particular test-wise deletion procedure, in which we perform CI tests on the samples without zeros for the conditioned variables, can be seamlessly integrated with existing structure learning approaches including constraint-based and greedy score-based methods, thus giving rise to a principled framework for GRNI in the presence of dropouts. We further show that the causal dropout model can be validated from data, and many existing statistical models to handle dropouts fit into our model as specific parametric instances. Empirical evaluation on synthetic, curated, and real-world experimental transcriptomic data comprehensively demonstrate the efficacy of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15500">https://arxiv.org/abs/2403.15500</a></p>
            <p><strong>Category:</strong> q-bio.QM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it deals with the topic of causal discovery. It presents the Causal Dropout Model for handling dropout error in gene regulatory network inference, a process that fits directly into your interest in causal representation learning and discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16031" target="_blank">Learning Directed Acyclic Graphs from Partial Orderings</a></h3>
            <a href="https://arxiv.org/html/2403.16031v1/" target="_blank"><img src="https://arxiv.org/html/2403.16031v1/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ali Shojaie, Wenyu Chen</p>
            <p><strong>Summary:</strong> arXiv:2403.16031v1 Announce Type: cross 
Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16031">https://arxiv.org/abs/2403.16031</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'Causal discovery' as it discusses learning directed acyclic graphs (DAGs), which are used to model causal relationships among variables. The paper proposes a new estimation framework for leveraging partial causality information in observational data which is an important aspect of causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16523" target="_blank">Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis</a></h3>
            <a href="https://arxiv.org/html/2403.16523v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16523v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jie Qiao, Yu Xiang, Zhengming Chen, Ruichu Cai, Zhifeng Hao</p>
            <p><strong>Summary:</strong> arXiv:2403.16523v1 Announce Type: cross 
Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and has at least two directed paths to $Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed path to $Y$ without passing $X$. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants. Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant. With the path information, causal order is identifiable under some graphical conditions. A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16523">https://arxiv.org/abs/2403.16523</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it explores causal discovery, specifically for count data. It proposes a Poisson Branching Structure Causal Model (PB-SCM) for learning causal structure, which aligns well with your observed interest in causal discovery and new methods in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2304.01391" target="_blank">Counterfactual Learning on Graphs: A Survey</a></h3>
            
            <p><strong>Authors:</strong> Zhimeng Guo, Teng Xiao, Zongyu Wu, Charu Aggarwal, Hui Liu, Suhang Wang</p>
            <p><strong>Summary:</strong> arXiv:2304.01391v2 Announce Type: replace 
Abstract: Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of data and cannot model casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on problems studied. For each category, we provide background and motivating examples, a general framework summarizing existing works and a detailed review of these works. We point out promising future research directions at the intersection of graph-structured data, counterfactual learning, and real-world applications. To offer a comprehensive view of resources for future studies, we compile a collection of open-source implementations, public datasets, and commonly-used evaluation metrics. This survey aims to serve as a ``one-stop-shop'' for building a unified understanding of graph counterfactual learning categories and current resources. We also maintain a repository for papers and resources and will keep updating the repository https://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2304.01391">https://arxiv.org/abs/2304.01391</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses counterfactual learning on graphs, a concept closely tied to causal discovery. Although it does not directly discuss using large language models in this context, it may still offer useful insights for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.05884" target="_blank">A Meta-Learning Perspective on Transformers for Causal Language Modeling</a></h3>
            <a href="https://arxiv.org/html/2310.05884v2/extracted/5493393/latex/images/clustering_sgd_val_f1.png" target="_blank"><img src="https://arxiv.org/html/2310.05884v2/extracted/5493393/latex/images/clustering_sgd_val_f1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinbo Wu, Lav R. Varshney</p>
            <p><strong>Summary:</strong> arXiv:2310.05884v2 Announce Type: replace 
Abstract: The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.05884">https://arxiv.org/abs/2310.05884</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper delves into the Transformer architecture, which is primarily used for developing large causal language models. It also emphasizes on mechanisms to explain its capabilities, something you're interested in when looking for papers on 'Causal discovery' and 'Using large language models in causal discovery'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.06721" target="_blank">Differentially Private Conditional Independence Testing</a></h3>
            <a href="https://arxiv.org/html/2306.06721v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.06721v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Iden Kalemaj, Shiva Prasad Kasiviswanathan, Aaditya Ramdas</p>
            <p><strong>Summary:</strong> arXiv:2306.06721v3 Announce Type: replace-cross 
Abstract: Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests with rigorous theoretical guarantees that work for the general case when $Z$ is continuous.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.06721">https://arxiv.org/abs/2306.06721</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses conditional independence tests, a foundational element of many algorithms designed for causal graph discovery, in a new context of differential privacy. This could be insightful for understanding new aspects of causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.13339" target="_blank">Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</a></h3>
            <a href="https://arxiv.org/html/2309.13339v3/" target="_blank"><img src="https://arxiv.org/html/2309.13339v3/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter</p>
            <p><strong>Summary:</strong> arXiv:2309.13339v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: \url{https://github.com/xf-zhao/LoT}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.13339">https://arxiv.org/abs/2309.13339</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper describes a method for improving the reasoning process in large language models, including tasks related to causal inference, which fits your interest in causal discovery and the use of large language models. However, it does not seem to focus specifically on proposing a new technique for causal discovery or causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.02760" target="_blank">Causal Question Answering with Reinforcement Learning</a></h3>
            
            <p><strong>Authors:</strong> Lukas Bl\"ubaum, Stefan Heindorf</p>
            <p><strong>Summary:</strong> arXiv:2311.02760v2 Announce Type: replace-cross 
Abstract: Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a large-scale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sparse rewards. Our evaluation shows that the agent successfully prunes the search space to answer binary causal questions by visiting less than 30 nodes per question compared to over 3,000 nodes by a naive breadth-first search. Our ablation study indicates that our supervised learning strategy provides a strong foundation upon which our reinforcement learning agent improves. The paths returned by our agent explain the mechanisms by which a cause produces an effect. Moreover, for each edge on a path, our causality graph provides its original source allowing for easy verification of paths.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.02760">https://arxiv.org/abs/2311.02760</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in causality and machine learning, particularly in the subtopic of causal discovery. The research presented makes use of a causality graph for answering causal questions, which aligns with your interest in learning about new methods in this area.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.01327" target="_blank">TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series</a></h3>
            <a href="https://arxiv.org/html/2310.01327v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.01327v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Arjun Ashok, \'Etienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, Alexandre Drouin</p>
            <p><strong>Summary:</strong> arXiv:2310.01327v2 Announce Type: replace 
Abstract: We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series. Code is made available at https://github.com/ServiceNow/TACTiS.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.01327">https://arxiv.org/abs/2310.01327</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest especially in 'New transformer-like models for time series'. It introduces a new model for multivariate probabilistic time series prediction based on transformer, which addresses various tasks such as forecasting, interpolation and their combinations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02023" target="_blank">Self-Supervised Contrastive Learning for Long-term Forecasting</a></h3>
            <a href="https://arxiv.org/html/2402.02023v2/" target="_blank"><img src="https://arxiv.org/html/2402.02023v2/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi</p>
            <p><strong>Summary:</strong> arXiv:2402.02023v2 Announce Type: replace 
Abstract: Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, especially in challenging scenarios that require a significantly long output for forecasting. Source code is available at https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02023">https://arxiv.org/abs/2402.02023</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest as it introduces a new deep learning method for long-term forecasting in time series. Furthermore, it uses new approach, contrastive learning and enhanced decomposition architecture, focusing on long-term variations of time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.11959" target="_blank">A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis</a></h3>
            <a href="https://arxiv.org/html/2310.11959v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.11959v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shuhan Zhong, Sizhe Song, Weipeng Zhuo, Guanyao Li, Yang Liu, S. -H. Gary Chan</p>
            <p><strong>Summary:</strong> arXiv:2310.11959v2 Announce Type: replace 
Abstract: Time series data, including univariate and multivariate ones, are characterized by unique composition and complex multi-scale temporal variations. They often require special consideration of decomposition and multi-scale modeling to analyze. Existing deep learning methods on this best fit to univariate time series only, and have not sufficiently considered sub-series modeling and decomposition completeness. To address these challenges, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer, which learns to explicitly decompose and represent the input time series in its different layers. To handle the multi-scale temporal patterns and multivariate dependencies, we propose a novel temporal patching approach to model the time series as multi-scale patches, and employ MLPs to capture intra- and inter-patch variations and channel-wise correlations. In addition, we propose a novel loss function to constrain both the mean and the autocorrelation of the decomposition residual for better decomposition completeness. Through extensive experiments on various real-world datasets for five common time series analysis tasks, we demonstrate that MSD-Mixer consistently and significantly outperforms other state-of-the-art algorithms with better efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.11959">https://arxiv.org/abs/2310.11959</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper 'A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis' makes relevant contributions to your aspects of interest. It presents a new deep learning method for time series analysis, specifically for both univariate and multivariate data. It not just enhances methodological aspects but introduces a novel dataset. While it does not mention the use of transformer-like models or multimodal ones, it proposes a promising decomposition approach for time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16108" target="_blank">A Transformer approach for Electricity Price Forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.16108v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16108v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Oscar Llorente Gonzalez, Jose Portela</p>
            <p><strong>Summary:</strong> arXiv:2403.16108v1 Announce Type: new 
Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16108">https://arxiv.org/abs/2403.16108</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of the Transformer model, a deep learning method, for time series forecasting. Specifically, it proposes a new approach in Electricity Price Forecasting (EPF) without relying on other recurrent networks. It might not specifically focus on novel foundation models or multimodal approach, therefore a 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16377" target="_blank">Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes</a></h3>
            <a href="https://arxiv.org/html/2403.16377v1/extracted/5491107/figs/NP.png" target="_blank"><img src="https://arxiv.org/html/2403.16377v1/extracted/5491107/figs/NP.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Seokhyun Chung, Raed Al Kontar</p>
            <p><strong>Summary:</strong> arXiv:2403.16377v1 Announce Type: new 
Abstract: Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction. Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered. Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units. This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels. Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16377">https://arxiv.org/abs/2403.16377</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a novel neural process-based approach for real-time condition monitoring signals, which falls within the purview of time series forecasting. However, it doesn't explicitly mention being a deep learning or a foundation model for time series, neither does it make use of transformer-like models, and hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16495" target="_blank">LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.16495v1/extracted/5493108/intro.png" target="_blank"><img src="https://arxiv.org/html/2403.16495v1/extracted/5493108/intro.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li</p>
            <p><strong>Summary:</strong> arXiv:2403.16495v1 Announce Type: new 
Abstract: Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series. Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers. For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features. Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results. Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\% and a maximum improvement of 16.78\% over baseline models. The source code is available at https://github.com/GeoX-Lab/LSTTN.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16495">https://arxiv.org/abs/2403.16495</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Time series and deep learning'. It outlines a new deep learning approach for time series forecasting and introduces a new model, LSTTN, which considers long- and short-term features. However, it doesn't explicitly mention multimodal deep learning or foundation models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16899" target="_blank">State Space Models as Foundation Models: A Control Theoretic Overview</a></h3>
            <a href="https://arxiv.org/html/2403.16899v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16899v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger</p>
            <p><strong>Summary:</strong> arXiv:2403.16899v1 Announce Type: cross 
Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16899">https://arxiv.org/abs/2403.16899</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses State Space Models (SSMs) in deep neural network architectures of foundation models, and compares it with transformer architectures. Given your interest in foundation models for time series and transformer-like models in time series, this paper could provide useful insights, even though it doesn't propose a new method but rather an analysis and perspective of existing methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2301.12528" target="_blank">Sequential Estimation of Gaussian Process-based Deep State-Space Models</a></h3>
            <a href="https://arxiv.org/html/2301.12528v2/extracted/5491216/AQ/ci.png" target="_blank"><img src="https://arxiv.org/html/2301.12528v2/extracted/5491216/AQ/ci.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuhao Liu, Marzieh Ajirak, Petar Djuric</p>
            <p><strong>Summary:</strong> arXiv:2301.12528v2 Announce Type: replace 
Abstract: We consider the problem of sequential estimation of the unknowns of state-space and deep state-space models that include estimation of functions and latent processes of the models. The proposed approach relies on Gaussian and deep Gaussian processes that are implemented via random feature-based Gaussian processes. In these models, we have two sets of unknowns, highly nonlinear unknowns (the values of the latent processes) and conditionally linear unknowns (the constant parameters of the random feature-based Gaussian processes). We present a method based on particle filtering where the parameters of the random feature-based Gaussian processes are integrated out in obtaining the predictive density of the states and do not need particles. We also propose an ensemble version of the method, with each member of the ensemble having its own set of features. With several experiments, we show that the method can track the latent processes up to a scale and rotation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2301.12528">https://arxiv.org/abs/2301.12528</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes novel methods for sequential estimation using Gaussian processes and deep state-space models, which can be relevant for your interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.17548" target="_blank">Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators</a></h3>
            <a href="https://arxiv.org/html/2401.17548v3/training_sample.pdf" target="_blank"><img src="https://arxiv.org/html/2401.17548v3/training_sample.pdf" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lifan Zhao, Yanyan Shen</p>
            <p><strong>Summary:</strong> arXiv:2401.17548v3 Announce Type: replace 
Abstract: Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments on six real-world datasets demonstrate that LIFT improves the state-of-the-art methods by 5.5% in average forecasting performance. Our code is available at https://github.com/SJTU-Quant/LIFT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.17548">https://arxiv.org/abs/2401.17548</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper explores a new method for multivariate time series forecasting, specifically taking into account lead-lag relationships between variates. It would be of interest due to your focus on new deep learning methods and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11922" target="_blank">Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation</a></h3>
            <a href="https://arxiv.org/html/2402.11922v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11922v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li</p>
            <p><strong>Summary:</strong> arXiv:2402.11922v3 Announce Type: replace 
Abstract: Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11922">https://arxiv.org/abs/2402.11922</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper falls under 'time-series'. It introduces a new method for spatio-temporal modeling, which can be classified as a form of time series forecasting. While not specifically mentioning deep learning, it incorporates a generative pre-training framework and a Transformer-based denoising diffusion model. However, it's more application-focussed.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.11722" target="_blank">Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation</a></h3>
            
            <p><strong>Authors:</strong> Johannes P\"oppelbaum, Andreas Schwung</p>
            <p><strong>Summary:</strong> arXiv:2403.11722v2 Announce Type: replace 
Abstract: We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and in a semi supervised, contrastive learning setting. Both times, we were able to outperform real valued counterparts as well as two baseline models: one with the uncompressed time-series as the input and the other with a regular downsampling using the mean. Further, we could improve the classification benchmark set by SimCLR-TS from 81.43% to 83.90%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.11722">https://arxiv.org/abs/2403.11722</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper should be of interest as it proposes a novel deep learning method for time series compression, and provides new methodology of handling data in a time-series, which may also consider a new foundation model for Time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.13841" target="_blank">Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.13841v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.13841v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhongqi Yang, Yuning Wang, Ken S. Yamashita, Maryam Sabah, Elahe Khatibi, Iman Azimi, Nikil Dutt, Jessica L. Borelli, Amir M. Rahmani</p>
            <p><strong>Summary:</strong> arXiv:2403.13841v2 Announce Type: replace 
Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the participants. Our results demonstrate that the proposed model achieves predictive accuracy of 82.50% for positive affect and 82.76% for negative affect, a full week in advance. The effectiveness of our model is further elevated by its explainability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.13841">https://arxiv.org/abs/2403.13841</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new multimodal deep learning model for affect status forecasting, which is a kind of time series forecasting. It also introduces an extensive dataset which you might find useful. However, the focus is on affect forecasting rather than general time series.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16843" target="_blank">Do LLM Agents Have Regret? A Case Study in Online Learning and Games</a></h3>
            
            <p><strong>Authors:</strong> Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.16843v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16843">https://arxiv.org/abs/2403.16843</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly addresses the third topic of your interest, discussing the performance of Large Language Model (LLM) agents in interactive environments and decision-making scenarios. Although it does not focus on controlling software or web browsers specifically, it does provide insights into the behavior of LLM agents in multi-agent settings, which might be applicable to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15502" target="_blank">Sequential Decision-Making for Inline Text Autocomplete</a></h3>
            <a href="https://arxiv.org/html/2403.15502v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15502v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rohan Chitnis, Shentao Yang, Alborz Geramifard</p>
            <p><strong>Summary:</strong> arXiv:2403.15502v1 Announce Type: cross 
Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretical and experimental evidence that, under certain objectives, the sequential decision-making formulation of the autocomplete problem provides a better suggestion policy than myopic single-step reasoning. However, aligning these objectives with real users requires further exploration. In particular, we hypothesize that the objectives under which sequential decision-making can improve autocomplete systems are not tailored solely to text entry speed, but more broadly to metrics such as user satisfaction and convenience.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15502">https://arxiv.org/abs/2403.15502</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant as it utilizes large language models for text autocomplete. However, it's not directly about controlling software or web browsers but pertains to improving user interaction, which could be beneficial in these contexts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15886" target="_blank">Leveraging Zero-Shot Prompting for Efficient Language Model Distillation</a></h3>
            <a href="https://arxiv.org/html/2403.15886v1/extracted/5490966/step_by_step.png" target="_blank"><img src="https://arxiv.org/html/2403.15886v1/extracted/5490966/step_by_step.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lukas V\"oge, Vincent Gurgul, Stefan Lessmann</p>
            <p><strong>Summary:</strong> arXiv:2403.15886v1 Announce Type: cross 
Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact of explanation properties on distillation efficiency, demonstrating that minimal performance loss occurs even when rationale augmentation is not applied across the entire dataset, facilitating further reductions of tokens. This research marks a step toward the efficient training of task-specific models with minimal human intervention, offering substantial cost-savings while maintaining, or even enhancing, performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15886">https://arxiv.org/abs/2403.15886</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses methods for efficiently distilling large language models (LLMs) into smaller, application-specific ones, which could be pertinent to using LLMs to control software or web browsers. However, it doesn't propose a brand-new method, but rather presents a novel application of existing techniques, primarily focusing on lowering costs and improving efficiency.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15938" target="_blank">LlamBERT: Large-scale low-cost data annotation in NLP</a></h3>
            <a href="https://arxiv.org/html/2403.15938v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15938v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> B\'alint Csan\'ady, Lajos Muzsai, P\'eter Vedres, Zolt\'an N\'adasdy, Andr\'as Luk\'acs</p>
            <p><strong>Summary:</strong> arXiv:2403.15938v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15938">https://arxiv.org/abs/2403.15938</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant under the 'agents based on large language models' topic. It discusses using large language models (LlamBERT) for large-scale low-cost data annotation in NLP. However, it does not directly talk about using LLMs to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15941" target="_blank">Explore until Confident: Efficient Exploration for Embodied Question Answering</a></h3>
            <a href="https://arxiv.org/html/2403.15941v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15941v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh</p>
            <p><strong>Summary:</strong> arXiv:2403.15941v1 Announce Type: cross 
Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15941">https://arxiv.org/abs/2403.15941</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it does not propose new large language models for controlling software or web browsers specifically, the paper presents a method for efficient exploration using large vision-language models, which is relevant to your interest in automation and control using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16087" target="_blank">LLMs as Compiler for Arabic Programming Language</a></h3>
            <a href="https://arxiv.org/html/2403.16087v1/extracted/5491777/figures/modelarc.jpeg" target="_blank"><img src="https://arxiv.org/html/2403.16087v1/extracted/5491777/figures/modelarc.jpeg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa</p>
            <p><strong>Summary:</strong> arXiv:2403.16087v1 Announce Type: cross 
Abstract: In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16087">https://arxiv.org/abs/2403.16087</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses using large language models as a semi-compiler for an Arabic Programming Language, thus relating to control of software applications using large language models. Although not directly in English, the concepts can be valuable and transferable.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16218" target="_blank">CoverUp: Coverage-Guided LLM-Based Test Generation</a></h3>
            <a href="https://arxiv.org/html/2403.16218v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16218v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Juan Altmayer Pizzorno, Emery D. Berger</p>
            <p><strong>Summary:</strong> arXiv:2403.16218v1 Announce Type: cross 
Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16218">https://arxiv.org/abs/2403.16218</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about using Large Language Models in Python test generation which falls under your interest in computer automation using large language models. However, its specific use is for testing and not directly controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16354" target="_blank">ChatDBG: An AI-Powered Debugging Assistant</a></h3>
            <a href="https://arxiv.org/html/2403.16354v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16354v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund</p>
            <p><strong>Summary:</strong> arXiv:2403.16354v1 Announce Type: cross 
Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16354">https://arxiv.org/abs/2403.16354</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not propose a new specific method, it demonstrates a novel application of large language models in controlling a software debugging assistant. It's highly relevant to your interest in using large language models to control software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16689" target="_blank">Synapse: Learning Preferential Concepts from Visual Demonstrations</a></h3>
            <a href="https://arxiv.org/html/2403.16689v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16689v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas</p>
            <p><strong>Summary:</strong> arXiv:2403.16689v1 Announce Type: cross 
Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving. Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations. The code and other details can be found on the project website https://amrl.cs.utexas.edu/synapse .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16689">https://arxiv.org/abs/2403.16689</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The mentioned paper is suitable for your interests as it discusses the use of large language models in combination with program synthesis to learn user-specific preferences, which falls under computer automation using large language models. However, it does not directly control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16809" target="_blank">An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</a></h3>
            <a href="https://arxiv.org/html/2403.16809v1/extracted/5493971/SysDiagram.png" target="_blank"><img src="https://arxiv.org/html/2403.16809v1/extracted/5493971/SysDiagram.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hanqing Yang, Marie Siew, Carlee Joe-Wong</p>
            <p><strong>Summary:</strong> arXiv:2403.16809v1 Announce Type: cross 
Abstract: The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16809">https://arxiv.org/abs/2403.16809</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a case study employing large language models (LLMs) in software control, specifically in real-time control of systems for CPS-IoT applications. However, it does not directly focus on controlling web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16952" target="_blank">Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance</a></h3>
            <a href="https://arxiv.org/html/2403.16952v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16952v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, Xipeng Qiu</p>
            <p><strong>Summary:</strong> arXiv:2403.16952v1 Announce Type: cross 
Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture. Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16952">https://arxiv.org/abs/2403.16952</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper seems significantly related to your interest in large language models. It discusses optimizing the training of such models and their performance, which might provide fruitful information for using large language models to control software or automate computers, though it does not specifically focus on these applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.15531" target="_blank">Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2309.15531v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.15531v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</p>
            <p><strong>Summary:</strong> arXiv:2309.15531v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output-channel (per-OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is available at https://github.com/johnheo/adadim-llm</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.15531">https://arxiv.org/abs/2309.15531</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it explores quantization techniques for Large Language Models which could be used for controlling software or web browsers more efficiently. Although it doesn't specifically focus on agents, the new method has potential applications in this area.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08446" target="_blank">Towards Robust Multi-Modal Reasoning via Model Selection</a></h3>
            <a href="https://arxiv.org/html/2310.08446v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.08446v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin</p>
            <p><strong>Summary:</strong> arXiv:2310.08446v2 Announce Type: replace 
Abstract: The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08446">https://arxiv.org/abs/2310.08446</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models for agent control, as it proposes a new framework that improves model selection for the agent. Moreover, the large language model serves as the 'brain' of the agent, which aligns with your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.13231" target="_blank">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model</a></h3>
            <a href="https://arxiv.org/html/2311.13231v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.13231v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li</p>
            <p><strong>Summary:</strong> arXiv:2311.13231v3 Announce Type: replace 
Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.13231">https://arxiv.org/abs/2311.13231</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the practical application of using optimization methods for tuning large language models and thus is related to large language model-based agents. However, it does not touch specifically upon controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.16430" target="_blank">Preference as Reward, Maximum Preference Optimization with Importance Sampling</a></h3>
            <a href="https://arxiv.org/html/2312.16430v5/extracted/5492959/pic/process.jpg" target="_blank"><img src="https://arxiv.org/html/2312.16430v5/extracted/5492959/pic/process.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zaifan Jiang, Xing Huang, Chao Wei</p>
            <p><strong>Summary:</strong> arXiv:2312.16430v5 Announce Type: replace 
Abstract: Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model-based algorithm to optimize preference learning, which first fits a reward model for preference scores and then optimizes the generating policy with an on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming, and unstable. The Direct Preference Optimization (DPO) algorithm uses an off-policy algorithm to directly optimize the generating policy and eliminates the need for a reward model. DPO is more data-efficient and stable. However, DPO has a drawback of overfitting to the preference data and ignoring the KL-regularization term when the preference is deterministic. Identity mapping Preference Optimization(IPO) uses a root-finding MSE loss to incorporate KL-regularization. However, both DPO and IPO fail to properly address the KL-regularization term because the support of the preference distribution is not equal to the reference distribution. In this paper, we propose a simple and intuitive off-policy preference optimization algorithm from an importance sampling view, which we call Maximum Preference Optimization (MPO). MPO incorporates the off-policy KL-regularization term, making regularization truly effective. MPO achieves the best of both worlds by combining the objectives of RLHF and IPO while being an off-policy algorithm. Furthermore, MPO eliminates the need for a reward model and reference policy, simplifying the learning process and reducing memory usage.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.16430">https://arxiv.org/abs/2312.16430</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on the concept of aligning language models, which falls under the large language models category, with a focus on optimization and reinforcement learning. It does not directly mention controlling software or web browsers with the use of large language models, but the concepts presented could be relevant and applied to your area of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.04870" target="_blank">Lemur: Integrating Large Language Models in Automated Program Verification</a></h3>
            <a href="https://arxiv.org/html/2310.04870v3/extracted/5492254/figs/histogram_esbmc.png" target="_blank"><img src="https://arxiv.org/html/2310.04870v3/extracted/5492254/figs/histogram_esbmc.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoze Wu, Clark Barrett, Nina Narodytska</p>
            <p><strong>Summary:</strong> arXiv:2310.04870v3 Announce Type: replace-cross 
Abstract: The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.04870">https://arxiv.org/abs/2310.04870</a></p>
            <p><strong>Category:</strong> cs.FL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses using Large Language Models (LLMs) in automated program verification, which aligns with your interest in computer automation using large language models. While it doesn't specifically mention controlling software or web browsers, the skills described could potentially be applied to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07240" target="_blank">CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving</a></h3>
            <a href="https://arxiv.org/html/2310.07240v4/" target="_blank"><img src="https://arxiv.org/html/2310.07240v4/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang</p>
            <p><strong>Summary:</strong> arXiv:2310.07240v4 Announce Type: replace-cross 
Abstract: As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays.
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache's distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3.2x while having negligible impact on the LLM response quality in accuracy or perplexity.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07240">https://arxiv.org/abs/2310.07240</a></p>
            <p><strong>Category:</strong> cs.NI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not directly discuss using large language models to control software or web browsers, it presents 'CacheGen' method, a context-loading module for large language models, which could be a significant piece in such applications. However, as it doesn't directly align with the user's stated interest in control and automation applications of large language models, it gets a 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.11667" target="_blank">SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</a></h3>
            
            <p><strong>Authors:</strong> Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap</p>
            <p><strong>Summary:</strong> arXiv:2310.11667v2 Announce Type: replace-cross 
Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.11667">https://arxiv.org/abs/2310.11667</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper, though does not focus on software or web browsers control by the large language models, does provide insights into the role-play interactions of the llm-based agents with humans in a variety of scenarios and their evaluation. It could be related to your interest in the broad exploration of large language model based-agents in a different context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.07311" target="_blank">Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</a></h3>
            <a href="https://arxiv.org/html/2403.07311v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.07311v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan Du, Yongfeng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.07311v5 Announce Type: replace-cross 
Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.07311">https://arxiv.org/abs/2403.07311</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> You should consider reading this paper as it explores the use of Large Language Models (LLMs) for the purpose of link prediction in knowledge graphs, which could be seen as a form of software control. However, it doesn't directly address the large language model controlling software or web browsers, hence the 4 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14589" target="_blank">ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training</a></h3>
            <a href="https://arxiv.org/html/2403.14589v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.14589v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2403.14589v2 Announce Type: replace-cross 
Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14589">https://arxiv.org/abs/2403.14589</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant as it deals with the training and refinement of language agents, a subset of large language models, for better performance and autonomous decision-making. While it's not directly about controlling software or browsers, it contributes to the topic of computer automation using LLMs.</p>
        </div>
        </div><div class='timestamp'>Report generated on March 26, 2024 at 10:29:06</div></body></html>