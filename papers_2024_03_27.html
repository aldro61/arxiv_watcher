
            <html>
            <head>
                <title>Report Generated on March 27, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for March 27, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17753" target="_blank">CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model</a></h3>
            <a href="https://arxiv.org/html/2403.17753v1/extracted/5496690/Figures/RoadGraph.png" target="_blank"><img src="https://arxiv.org/html/2403.17753v1/extracted/5496690/Figures/RoadGraph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiqi Shao, Michael G. H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao, Junbin Gao</p>
            <p><strong>Summary:</strong> arXiv:2403.17753v1 Announce Type: new 
Abstract: Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real-world datasets highlight CCDSReFormer's superior performance. An ablation study also confirms the significant impact of each component on the model's predictive accuracy, showcasing our model's ability to forecast traffic flow effectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17753">https://arxiv.org/abs/2403.17753</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in Time series and deep learning, specifically on forecasting. It introduces a new transformer-like model CCDSReFormer for traffic forecast, which is a type of time series prediction task. A few new sub-models (ReSSA, ReDASA, ReTSA) are also proposed to address issues in existing transformer models, aligning with your interest in new deep learning methods for time-series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.09428" target="_blank">Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity</a></h3>
            <a href="https://arxiv.org/html/2403.09428v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.09428v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues</p>
            <p><strong>Summary:</strong> arXiv:2403.09428v2 Announce Type: replace 
Abstract: Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a higher degree of sample efficiency and is empirically demonstrated to enhance the classification model's performance on both full- and missing-modality data in the low-data regime across various multimodal learning tasks. When only 1% of the training data are available, our proposed method demonstrates an average improvement of 6.1% over a recent strong baseline across various datasets and missing states. Notably, our method also reduces the performance gap between full-modality and missing-modality data compared with the baseline.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.09428">https://arxiv.org/abs/2403.09428</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper is highly relevant to your interests as it introduces a new method for handling multimodal learning with missing modalities, a configuration that can apply to time series data. It's dealing with new multimodal deep learning models, which is one of the subtopics in your interest list.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17445" target="_blank">Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model</a></h3>
            <a href="https://arxiv.org/html/2403.17445v1/extracted/5495941/ETS.png" target="_blank"><img src="https://arxiv.org/html/2403.17445v1/extracted/5495941/ETS.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiqun Chu, Zuoquan Lin</p>
            <p><strong>Summary:</strong> arXiv:2403.17445v1 Announce Type: new 
Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17445">https://arxiv.org/abs/2403.17445</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new architecture combining ETS and MLP to model long-range sequences. It falls under your 'new deep learning methods for time series' subtopic interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17698" target="_blank">MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation</a></h3>
            <a href="https://arxiv.org/html/2403.17698v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17698v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Weiguo Gao</p>
            <p><strong>Summary:</strong> arXiv:2403.17698v1 Announce Type: new 
Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function. Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model's extrapolation capabilities. Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores. We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques. Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17698">https://arxiv.org/abs/2403.17698</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it introduces a new method, MEP, for enhancing the accuracy of transformer models used in sequence length prediction, which includes time series forecasting. It may not specifically mention 'time-series', but the proposed technique can be applied to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17016" target="_blank">HEAL-ViT: Vision Transformers on a spherical mesh for medium-range weather forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.17016v1/extracted/5409470/images/biscuit.png" target="_blank"><img src="https://arxiv.org/html/2403.17016v1/extracted/5409470/images/biscuit.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vivek Ramavajjala</p>
            <p><strong>Summary:</strong> arXiv:2403.17016v1 Announce Type: cross 
Abstract: In recent years, a variety of ML architectures and techniques have seen success in producing skillful medium range weather forecasts. In particular, Vision Transformer (ViT)-based models (e.g. Pangu-Weather, FuXi) have shown strong performance, working nearly "out-of-the-box" by treating weather data as a multi-channel image on a rectilinear grid. While a rectilinear grid is appropriate for 2D images, weather data is inherently spherical and thus heavily distorted at the poles on a rectilinear grid, leading to disproportionate compute being used to model data near the poles. Graph-based methods (e.g. GraphCast) do not suffer from this problem, as they map the longitude-latitude grid to a spherical mesh, but are generally more memory intensive and tend to need more compute resources for training and inference. While spatially homogeneous, the spherical mesh does not lend itself readily to be modeled by ViT-based models that implicitly rely on the rectilinear grid structure. We present HEAL-ViT, a novel architecture that uses ViT models on a spherical mesh, thus benefiting from both the spatial homogeneity enjoyed by graph-based models and efficient attention-based mechanisms exploited by transformers. HEAL-ViT produces weather forecasts that outperform the ECMWF IFS on key metrics, and demonstrate better bias accumulation and blurring than other ML weather prediction models. Further, the lowered compute footprint of HEAL-ViT makes it attractive for operational use as well, where other models in addition to a 6-hourly prediction model may be needed to produce the full set of operational forecasts required.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17016">https://arxiv.org/abs/2403.17016</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a novel deep learning architecture (HEAL-ViT) for time series forecasting in medium-range weather prediction. Although it is not directly related to all the subtopics you listed, it can provide insights into transformer-like models applied to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17729" target="_blank">EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention</a></h3>
            <a href="https://arxiv.org/html/2403.17729v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17729v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen</p>
            <p><strong>Summary:</strong> arXiv:2403.17729v1 Announce Type: cross 
Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17729">https://arxiv.org/abs/2403.17729</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is fairly relevant to your interest in time-series and deep learning, especially in the context of new transformer-like models for time series. It proposes a new variant of the transformer model (EulerFormer) which introduces several technical improvements for modeling sequence data, a key aspect of time series analysis.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00093" target="_blank">ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation</a></h3>
            <a href="https://arxiv.org/html/2402.00093v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.00093v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan Karfa, Ramesh Karri</p>
            <p><strong>Summary:</strong> arXiv:2402.00093v2 Announce Type: replace-cross 
Abstract: System Verilog Assertion (SVA) formulation- a critical yet complex task is a prerequisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications, which is timeconsuming and prone to human error. However, LLM-informed automatic assertion generation is gaining interest. We designeda novel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA assertions from natural language specifications. ChIRAAG constitutes the systematic breakdown of design specifications into a standardized format, further generating assertions from formatted specifications using LLM. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Automatic feedback of log files from the simulation tool to the LLM ensures that the framework can generate correc SVAs automatically. Only 33% of LLM-generated raw assertions had errors. Our results on OpenTitan designs shows that LLMs can streamline and assist engineers in the assertion generation process, reshaping verification workflows.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00093">https://arxiv.org/abs/2402.00093</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, 'ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation', is highly relevant to your interest in 'Agents based on large-language models'. It describes the use of a large language model, OpenAI GPT4, for automation in the context of System Verilog Assertion (SVA) formulation, a task tied to software control. It evaluates the automatic generation of SVA assertions from natural language specifications, aligning with your interest in 'using large language models to control software'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17844" target="_blank">Mechanistic Design and Scaling of Hybrid Architectures</a></h3>
            
            <p><strong>Authors:</strong> Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\"orn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\'e, Ce Zhang, Stefano Massaroli</p>
            <p><strong>Summary:</strong> arXiv:2403.17844v1 Announce Type: new 
Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17844">https://arxiv.org/abs/2403.17844</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLMs) and their development. It speaks about the design of deep learning architectures, a basic component of LLMs. While not explicitly addressing your subtopics, the methodological discussion could provide valuable insights for constructing LLMs for specific tasks like controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17919" target="_blank">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a></h3>
            <a href="https://arxiv.org/html/2403.17919v1/extracted/5497360/figs/LLaMA-2-7B_gpt4_loss_step.png" target="_blank"><img src="https://arxiv.org/html/2403.17919v1/extracted/5497360/figs/LLaMA-2-7B_gpt4_loss_step.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.17919v1 Announce Type: new 
Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17919">https://arxiv.org/abs/2403.17919</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is directly relevant to the 'large-language models agents' topic. It proposes a new method, LISA, for fine-tuning large language models, which can be essential for controlling software or web browsers or for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17218" target="_blank">A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection</a></h3>
            <a href="https://arxiv.org/html/2403.17218v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17218v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza Sanjida Alam, Earl T. Barr, Wei Le</p>
            <p><strong>Summary:</strong> arXiv:2403.17218v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of-thought, and proposed three of our own prompting methods. Our results show that while our prompting methods improved the models' performance, LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that 57% of LLM responses contained errors, and the models frequently predicted incorrect locations of buggy code and misidentified bug types. LLMs only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants. These findings suggest that despite their potential for other tasks, LLMs may fail to properly comprehend critical code structures and security-related concepts. Our data and code are available at https://figshare.com/s/78fe02e56e09ec49300b.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17218">https://arxiv.org/abs/2403.17218</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on the use of Large Language Models (LLMs) for a specific task (vulnerability detection), which is a kind of software control. While it does not exactly match your interest in LLMs for general software or web browser control, it provides valuable insights into the capabilities and limitations of LLMs which could be applicable to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17312" target="_blank">ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</a></h3>
            <a href="https://arxiv.org/html/2403.17312v1/extracted/5495383/imgs/intro_imgs/intro_revised_v.png" target="_blank"><img src="https://arxiv.org/html/2403.17312v1/extracted/5495383/imgs/intro_imgs/intro_revised_v.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Youpeng Zhao, Di Wu, Jun Wang</p>
            <p><strong>Summary:</strong> arXiv:2403.17312v1 Announce Type: cross 
Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17312">https://arxiv.org/abs/2403.17312</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper delves into the subject of large language models (LLMs) and proposes a novel solution for practical inference challenges related to LLMs. While it does not touch directly on controlling software or web browsers, its findings could be relevant to those working on problems related to the automation of computer software using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17431" target="_blank">Robust and Scalable Model Editing for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2403.17431v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17431v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun</p>
            <p><strong>Summary:</strong> arXiv:2403.17431v1 Announce Type: cross 
Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at https://github.com/thunlp/EREN.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17431">https://arxiv.org/abs/2403.17431</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While not directly related to controlling software or web browsers, this paper discusses a method for improving the controllability of large language models, which could be beneficial in terms of developing agents based on these models. It provides insight into modifying the knowledge of a LLM which could contribute to computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17582" target="_blank">Towards a Zero-Data, Controllable, Adaptive Dialog System</a></h3>
            <a href="https://arxiv.org/html/2403.17582v1/extracted/5496431/imgs/task_example.png" target="_blank"><img src="https://arxiv.org/html/2403.17582v1/extracted/5496431/imgs/task_example.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dirk V\"ath, Lindsey Vanderlyn, Ngoc Thang Vu</p>
            <p><strong>Summary:</strong> arXiv:2403.17582v1 Announce Type: cross 
Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17582">https://arxiv.org/abs/2403.17582</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of a Large Language Model for controlling dialog systems, which falls under the use of large language models for controlling software. It might be of interest due to its emphasis on conversational AI and its use of Large Language Models. The training and adaptation of these models shed light on the control mechanisms applicable to large language models in software applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17887" target="_blank">The Unreasonable Ineffectiveness of the Deeper Layers</a></h3>
            <a href="https://arxiv.org/html/2403.17887v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17887v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts</p>
            <p><strong>Summary:</strong> arXiv:2403.17887v1 Announce Type: cross 
Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17887">https://arxiv.org/abs/2403.17887</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses an aspect of large-language models, namely their optimization for low-resource tasks and inference in scenarios such as controlling software or automation. Though it doesn't explicitly cover computer automation or the control of specific applications like web browsers, it explores techniques for making large language models more efficient, pointing towards their applicability in the areas you're interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.10971" target="_blank">Context-Aware Meta-Learning</a></h3>
            <a href="https://arxiv.org/html/2310.10971v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.10971v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure Leskovec, Christopher Re, Sebastian Thrun</p>
            <p><strong>Summary:</strong> arXiv:2310.10971v2 Announce Type: replace 
Abstract: Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts visual meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks. Our code is available at https://github.com/cfifty/CAML.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.10971">https://arxiv.org/abs/2310.10971</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interests because it deals with Large Language Models (LLM) in the context of learning new visual concepts, an abstract concept which might be leveraged for controlling web browsers or other software applications. It's not a perfect match as it doesn't specifically deal with agents, but it could still provide valuable context for that field.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.15694" target="_blank">COPR: Continual Learning Human Preference through Optimal Policy Regularization</a></h3>
            <a href="https://arxiv.org/html/2310.15694v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.15694v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu</p>
            <p><strong>Summary:</strong> arXiv:2310.15694v5 Announce Type: replace 
Abstract: The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Regularization (COPR), in which we compute the distribution of optimal policy bypassing the partition function and then regularize the current policy based on the historically optimal distribution to mitigate Catastrophic Forgetting (CF). COPR involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability with RLHF to learn from unlabeled data by maintaining a scoring module, similar to reward model, making it flexible for continually learning without human feedback. Our experimental results show that COPR outperforms strong Continuous Learning (CL) baselines when it comes to consistently aligning with human preferences on incremental tasks and domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.15694">https://arxiv.org/abs/2310.15694</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses a method in enhancing pre-trained Language Models (LM) to better align with human preferences, which could be applicable for controlling software or automations. While it doesn't directly address your query on large language models and agents, it presents an important factor in the successful operation of these agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.08763" target="_blank">Simple and Scalable Strategies to Continually Pre-train Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Adam Ibrahim, Benjamin Th\'erien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timoth\'ee Lesort, Eugene Belilovsky, Irina Rish</p>
            <p><strong>Summary:</strong> arXiv:2403.08763v3 Announce Type: replace 
Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.08763">https://arxiv.org/abs/2403.08763</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses large language models and proposes efficient strategies for continually pre-training these models. The concepts presented may be relevant to understanding how large language models can be updated and used in the context of creating intelligent agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.13339" target="_blank">Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</a></h3>
            <a href="https://arxiv.org/html/2309.13339v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.13339v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter</p>
            <p><strong>Summary:</strong> arXiv:2309.13339v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.13339">https://arxiv.org/abs/2309.13339</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses reasoning abilities of large language models which could be key to tasks like controlling software/web browsers or automating computers. Although the primary focus of the paper is not directly related to controlling agents or automation, it can potentially offer valuable insights on how to improve the reasoning abilities of large-scale language models, which is essential for the tasks mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.02129" target="_blank">Unveiling the Pitfalls of Knowledge Editing for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.02129v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.02129v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2310.02129v4 Announce Type: replace-cross 
Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.02129">https://arxiv.org/abs/2310.02129</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be of interest to you as it explores knowledge editing within Large Language Models, a topic which can directly impact the use of LLMs for controlling software. It doesn't necessarily introduce a new method or application but provides significant insights about the challenges in the current approaches, which might be vital for future development.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.06795" target="_blank">AI and Generative AI for Research Discovery and Summarization</a></h3>
            <a href="https://arxiv.org/html/2401.06795v2/extracted/5497393/google-search-results1.png" target="_blank"><img src="https://arxiv.org/html/2401.06795v2/extracted/5497393/google-search-results1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mark Glickman, Yi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2401.06795v2 Announce Type: replace-cross 
Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.06795">https://arxiv.org/abs/2401.06795</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models (LLMs) like ChatGPT for tasks such as generating programming code and research discovery, which aligns with your interest in computer automation using LLMs. Although it does not specifically mention controlling software or web browsers, it is likely that the concepts discussed could be applied in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.10949" target="_blank">SelfIE: Self-Interpretation of Large Language Model Embeddings</a></h3>
            <a href="https://arxiv.org/html/2403.10949v2/extracted/5495356/figs/self-sq.png" target="_blank"><img src="https://arxiv.org/html/2403.10949v2/extracted/5495356/figs/self-sq.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haozhe Chen, Carl Vondrick, Chengzhi Mao</p>
            <p><strong>Summary:</strong> arXiv:2403.10949v2 Announce Type: replace-cross 
Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.10949">https://arxiv.org/abs/2403.10949</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interests in large-language models as it explores methods of interpreting the reasoning process of an LLM, including a potential avenue for controlling LLM reasoning. However, it doesn't directly address controlling software or web browsers with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.12151" target="_blank">Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification</a></h3>
            <a href="https://arxiv.org/html/2403.12151v2/figure1" target="_blank"><img src="https://arxiv.org/html/2403.12151v2/figure1" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</p>
            <p><strong>Summary:</strong> arXiv:2403.12151v2 Announce Type: replace-cross 
Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art performance achieved by the proposed approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12151">https://arxiv.org/abs/2403.12151</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it explores the usage of Large Language Models (LLMs) in the context of vision-based tasks. Though not directly about controlling software or web browsers, it expands on the capabilities of LLMs in other areas which could be applicable to the specified interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14438" target="_blank">A Multimodal Approach to Device-Directed Speech Detection with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2403.14438v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.14438v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</p>
            <p><strong>Summary:</strong> arXiv:2403.14438v2 Announce Type: replace-cross 
Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14438">https://arxiv.org/abs/2403.14438</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the usage of large language models for control (interaction with virtual assistants) which relates to your interest in 'Agents based on large-language models'. However, it does not specifically mention controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16950" target="_blank">Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</a></h3>
            <a href="https://arxiv.org/html/2403.16950v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16950v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuli\'c, Anna Korhonen, Nigel Collier</p>
            <p><strong>Summary:</strong> arXiv:2403.16950v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16950">https://arxiv.org/abs/2403.16950</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it doesn't focus on controlling browsers or software specifically, the paper broadly discusses large language models as automatic evaluators, which could inform their potential use in automation tasks. It presents a new approach (Pairwise-preference Search) for the assessment of generated language, which might be a new method/technique in the area of large language model evaluators.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17852" target="_blank">Counterfactual Fairness through Transforming Data Orthogonal to Bias</a></h3>
            <a href="https://arxiv.org/html/2403.17852v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17852v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shuyi Chen, Shixiang Zhu</p>
            <p><strong>Summary:</strong> arXiv:2403.17852v1 Announce Type: new 
Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization. Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17852">https://arxiv.org/abs/2403.17852</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses a new data pre-processing method, which falls under your causal representation learning and causal discovery subtopics. It uses a structural causal model (SCM) to facilitate counterfactual fairness in machine learning applications, which makes it relevant for your interests in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2302.03788" target="_blank">Toward a Theory of Causation for Interpreting Neural Code Models</a></h3>
            <a href="https://arxiv.org/html/2302.03788v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2302.03788v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk</p>
            <p><strong>Summary:</strong> arXiv:2302.03788v4 Announce Type: replace-cross 
Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of $do_{code}$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of $do_{code}$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2302.03788">https://arxiv.org/abs/2302.03788</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper introduces a new method based on causal inference for interpreting Neural Code Models, which may provide insights into causal discovery in machine learning.</p>
        </div>
        </div><div class='timestamp'>Report generated on March 27, 2024 at 21:34:51</div></body></html>