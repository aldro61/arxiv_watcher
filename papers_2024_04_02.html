
            <html>
            <head>
                <title>Report Generated on April 02, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 02, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00225" target="_blank">Heterogeneous Contrastive Learning for Foundation Models and Beyond</a></h3>
            
            <p><strong>Authors:</strong> Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He</p>
            <p><strong>Summary:</strong> arXiv:2404.00225v1 Announce Type: new 
Abstract: In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00225">https://arxiv.org/abs/2404.00225</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper may interest you as it provides a comprehensive overview on heterogeneous contrastive learning for foundation models and discusses its application for multi-view foundation models, which might have applications in time-series analysis. However, it does not explicitly focus on time-series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00618" target="_blank">A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours</a></h3>
            <a href="https://arxiv.org/html/2404.00618v1/extracted/5506872/gif/rbfmulti.png" target="_blank"><img src="https://arxiv.org/html/2404.00618v1/extracted/5506872/gif/rbfmulti.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aarush Sinha</p>
            <p><strong>Summary:</strong> arXiv:2404.00618v1 Announce Type: new 
Abstract: In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucidating hidden structures in complex physical systems while offering practical applications in various domains requiring accurate short-term forecasting capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00618">https://arxiv.org/abs/2404.00618</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in the time-series domain because it proposes a new neural network approach to predict complex and chaotic behaviors in time-series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00898" target="_blank">CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series</a></h3>
            <a href="https://arxiv.org/html/2404.00898v1/extracted/5503477/intro_case_inv.jpg" target="_blank"><img src="https://arxiv.org/html/2404.00898v1/extracted/5503477/intro_case_inv.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tien-Yu Chang, Hao Dai, Vincent S. Tseng</p>
            <p><strong>Summary:</strong> arXiv:2404.00898v1 Announce Type: new 
Abstract: Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.
  We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation. Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction. Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00898">https://arxiv.org/abs/2404.00898</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'time-series' tag as it presents a new deep learning-based approach for time-series data augmentation. However, it focuses more on Class-dependent Automatic Adaptive Policies (CAAP) for bias reduction and less on forecasting, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01122" target="_blank">Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution</a></h3>
            
            <p><strong>Authors:</strong> Ajay Devda, Akshay Sunil, Murthy R, B Deepthi</p>
            <p><strong>Summary:</strong> arXiv:2404.01122v1 Announce Type: new 
Abstract: Forecasting rainfall in tropical areas is challenging due to complex atmospheric behaviour, elevated humidity levels, and the common presence of convective rain events. In the Indian context, the difficulty is further exacerbated because of the monsoon intra seasonal oscillations, which introduce significant variability in rainfall patterns over short periods. Earlier investigations into rainfall prediction leveraged numerical weather prediction methods, along with statistical and deep learning approaches. This study introduces deep learning spatial model aimed at enhancing rainfall prediction accuracy on a finer scale. In this study, we hypothesize that integrating physical understanding improves the precipitation prediction skill of deep learning models with high precision for finer spatial scales, such as cities. To test this hypothesis, we introduce a physics informed ConvLSTM2D model to predict precipitation 6hr and 12hr ahead for Mumbai, India. We utilize ERA5 reanalysis data select predictor variables, across various geopotential levels. The ConvLSTM2D model was trained on the target variable precipitation for 4 different grids representing different spatial grid locations of Mumbai. Thus, the use of the ConvLSTM2D model for rainfall prediction, utilizing physics informed data from specific grids with limited spatial information, reflects current advancements in meteorological research that emphasize both efficiency and localized precision.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01122">https://arxiv.org/abs/2404.01122</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new deep learning model (ConvLSTM2D) for rainfall prediction, which aligns with your interest in new methods for time series forecasting. However, it doesn't provide methods specific to multimodal or transformer-like models, hence the score is 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01217" target="_blank">Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy</a></h3>
            
            <p><strong>Authors:</strong> Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam</p>
            <p><strong>Summary:</strong> arXiv:2404.01217v1 Announce Type: new 
Abstract: Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01217">https://arxiv.org/abs/2404.01217</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper may interest you as it discusses new deep learning models (RDGCN, SIRGCN) that incorporate domain differential equations for better accuracy and robustness in time series prediction. However, it doesn't specifically mention transformer-like models or foundation models for time series, which are part of your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00411" target="_blank">Aardvark Weather: end-to-end data-driven weather forecasting</a></h3>
            <a href="https://arxiv.org/html/2404.00411v1/extracted/5506137/figures/logos/aardvark.png" target="_blank"><img src="https://arxiv.org/html/2404.00411v1/extracted/5506137/figures/logos/aardvark.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Anna Vaughan, Stratis Markou, Will Tebbutt, James Requeima, Wessel P. Bruinsma, Tom R. Andersson, Michael Herzog, Nicholas D. Lane, J. Scott Hosking, Richard E. Turner</p>
            <p><strong>Summary:</strong> arXiv:2404.00411v1 Announce Type: cross 
Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are produced for temperature, mean sea level pressure, and wind speed at a geographically diverse set of weather stations, and are skillful with respect to an IFS-HRES interpolation baseline at multiple lead-times. Aardvark, by virtue of its simplicity and scalability, opens the door to a new paradigm for performing accurate and efficient data-driven medium-range weather forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00411">https://arxiv.org/abs/2404.00411</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Despite the paper's focus being on weather forecasting, it provides insight into a new end-to-end data-driven machine learning model for time series, addressing your interest in new deep learning and forecasting methods for time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00729" target="_blank">Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation</a></h3>
            <a href="https://arxiv.org/html/2404.00729v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00729v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Minghui Chen, Zichao Meng, Yanping Liu, Longbo Luo, Ye Guo, Kang Wang</p>
            <p><strong>Summary:</strong> arXiv:2404.00729v1 Announce Type: cross 
Abstract: In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the long short-term memory (LSTM) network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. Simulation results confirm the superior performance of our approach compared to existing alternatives.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00729">https://arxiv.org/abs/2404.00729</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new method for time series forecasting by using a nonparametric probabilistic forecast model with an LSTM network focusing on renewable generation outputs, which falls under your interest in new deep learning methods for time series. However, it doesn't specifically mention foundation models or transformer-like models, hence, the score is 4, not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.10707" target="_blank">Multimodal Representation Learning by Alternating Unimodal Adaptation</a></h3>
            <a href="https://arxiv.org/html/2311.10707v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.10707v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao</p>
            <p><strong>Summary:</strong> arXiv:2311.10707v2 Announce Type: replace 
Abstract: Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time uncertainty-based model fusion mechanism to integrate multimodal information. Extensive experiments are conducted on five diverse datasets, encompassing scenarios with complete modalities and scenarios with missing modalities. These experiments demonstrate the superiority of MLA over competing prior approaches. Our code is available at https://github.com/Cecile-hi/Multimodal-Learning-with-Alternating-Unimodal-Adaptation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.10707">https://arxiv.org/abs/2311.10707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new approach to multimodal learning, which is one of your subtopics under Time Series and Deep Learning. It does not specifically talk about time series, but the methods proposed could potentially be applied to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00976" target="_blank">Investigating Recurrent Transformers with Dynamic Halt</a></h3>
            
            <p><strong>Authors:</strong> Jishnu Ray Chowdhury, Cornelia Caragea</p>
            <p><strong>Summary:</strong> arXiv:2402.00976v2 Announce Type: replace 
Abstract: In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00976">https://arxiv.org/abs/2402.00976</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper presents novel ways to improve Transformers with recurrent mechanisms, which can be applied in time-series studies. Although the paper does not specifically mention time-series, the introduced methods may be beneficial for your interest in transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.12418" target="_blank">STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model</a></h3>
            <a href="https://arxiv.org/html/2403.12418v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.12418v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster</p>
            <p><strong>Summary:</strong> arXiv:2403.12418v2 Announce Type: replace 
Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks. STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph Neural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12418">https://arxiv.org/abs/2403.12418</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Although the paper discusses key aspects of time series, such as spatial-temporal graph modeling, it does not explicitly state any advancements in foundation models for time series or transformer-like models. However, it should provide valuable insights into state space models and STG learning, relevant to new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16108" target="_blank">A Transformer approach for Electricity Price Forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.16108v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16108v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Oscar Llorente, Jose Portela</p>
            <p><strong>Summary:</strong> arXiv:2403.16108v2 Announce Type: replace 
Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16108">https://arxiv.org/abs/2403.16108</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper fits into your time-series category and involves the use of a new transformer-like model for electricity price forecasting, correlating with the subtopic 'New transformer-like models for time series'. However, it doesn't fully cover all your areas of interest in new methods for time series such as multimodal methods or foundation models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.03897" target="_blank">DANSE: Data-driven Non-linear State Estimation of Model-free Process in Unsupervised Learning Setup</a></h3>
            
            <p><strong>Authors:</strong> Anubhab Ghosh, Antoine Honor\'e, Saikat Chatterjee</p>
            <p><strong>Summary:</strong> arXiv:2306.03897v2 Announce Type: replace-cross 
Abstract: We address the tasks of Bayesian state estimation and forecasting for a model-free process in an unsupervised learning setup. For a model-free process, we do not have any a-priori knowledge of the process dynamics. In the article, we propose DANSE -- a Data-driven Nonlinear State Estimation method. DANSE provides a closed-form posterior of the state of the model-free process, given linear measurements of the state. In addition, it provides a closed-form posterior for forecasting. A data-driven recurrent neural network (RNN) is used in DANSE to provide the parameters of a prior of the state. The prior depends on the past measurements as input, and then we find the closed-form posterior of the state using the current measurement as input. The data-driven RNN captures the underlying non-linear dynamics of the model-free process. The training of DANSE, mainly learning the parameters of the RNN, is executed using an unsupervised learning approach. In unsupervised learning, we have access to a training dataset comprising only a set of measurement data trajectories, but we do not have any access to the state trajectories. Therefore, DANSE does not have access to state information in the training data and can not use supervised learning. Using simulated linear and non-linear process models (Lorenz attractor and Chen attractor), we evaluate the unsupervised learning-based DANSE. We show that the proposed DANSE, without knowledge of the process model and without supervised learning, provides a competitive performance against model-driven methods, such as the Kalman filter (KF), extended KF (EKF), unscented KF (UKF), a data-driven deep Markov model (DMM) and a recently proposed hybrid method called KalmanNet. In addition, we show that DANSE works for high-dimensional state estimation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.03897">https://arxiv.org/abs/2306.03897</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes a new nonlinear state estimation method for time series and discusses unsupervised learning and forecasting techniques. Moreover, while it doesn't specifically mention 'deep learning', the use of a recurrent neural network implies the application of deep learning principles.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00264" target="_blank">DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation</a></h3>
            <a href="https://arxiv.org/html/2404.00264v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00264v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aru Maekawa, Satoshi Kosugi, Kotaro Funakoshi, Manabu Okumura</p>
            <p><strong>Summary:</strong> arXiv:2404.00264v1 Announce Type: cross 
Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00264">https://arxiv.org/abs/2404.00264</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper titled 'DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation' aligns with your interest in large language models. It tackles the application of language models to generate synthetic training samples, which is an innovative method that may be relevant with regards to your interests in computer automation and in-context learning of such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00282" target="_blank">Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods</a></h3>
            <a href="https://arxiv.org/html/2404.00282v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00282v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua Zhao, Yun Li</p>
            <p><strong>Summary:</strong> arXiv:2404.00282v1 Announce Type: new 
Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospective opportunities and challenges of the $\textit{LLM-enhanced RL}$ are discussed.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00282">https://arxiv.org/abs/2404.00282</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper extensively reviews the use of large language models (LLMs) in reinforcing learning, particularly in areas such as multi-task learning, task planning, and improving sample efficiency. While not specifically about controlling software or web browsers, LLM-enhanced RL could have broad implications for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00456" target="_blank">QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.00456v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00456v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman</p>
            <p><strong>Summary:</strong> arXiv:2404.00456v1 Announce Type: new 
Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00456">https://arxiv.org/abs/2404.00456</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it discusses a new quantization scheme for Large Language Models (LLMs), which might have implications for their use in controlling software or in automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00474" target="_blank">Linguistic Calibration of Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.00474v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00474v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Neil Band, Xuechen Li, Tengyu Ma, Tatsunori Hashimoto</p>
            <p><strong>Summary:</strong> arXiv:2404.00474v1 Announce Type: new 
Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00474">https://arxiv.org/abs/2404.00474</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not directly cover using large language models to control software or apply to computer automation, it discusses an important aspect of improving large language models - linguistic calibration. The improved decision-making and enhanced accuracy of the models as a result of this, can potentially contribute towards better control and automation applications. Moreover, it talks about language models in the context of decision-making, which is a fundamental aspect of agent control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00859" target="_blank">Do language models plan ahead for future tokens?</a></h3>
            <a href="https://arxiv.org/html/2404.00859v1/extracted/5507796/figures/abstract_fig.png" target="_blank"><img src="https://arxiv.org/html/2404.00859v1/extracted/5507796/figures/abstract_fig.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wilson Wu, John X. Morris, Lionel Levine</p>
            <p><strong>Summary:</strong> arXiv:2404.00859v1 Announce Type: new 
Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00859">https://arxiv.org/abs/2404.00859</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the mechanics of transformers in language models and their planning abilities, which may be relevant to your interest in how large language models can control software or drive automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01041" target="_blank">Can LLMs get help from other LLMs without revealing private information?</a></h3>
            <a href="https://arxiv.org/html/2404.01041v1/extracted/5508589/img/system-colors.png" target="_blank"><img src="https://arxiv.org/html/2404.01041v1/extracted/5508589/img/system-colors.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor C\u{a}rbune, Blaise Aguera y Arcas</p>
            <p><strong>Summary:</strong> arXiv:2404.01041v1 Announce Type: new 
Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01041">https://arxiv.org/abs/2404.01041</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper appears to deal with large language models in a machine learning context and discusses their integration in systems where these models communicate with others. The user's interest in 'agents based on large-language models' and 'Using large language models to control software' might overlap with the paper's content.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01273" target="_blank">TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model</a></h3>
            <a href="https://arxiv.org/html/2404.01273v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.01273v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yue Wang, Yingzhou Lu, Yinlong Xu, Zihan Ma, Hongxia Xu, Bang Du, Honghao Gao, Jian Wu</p>
            <p><strong>Summary:</strong> arXiv:2404.01273v1 Announce Type: new 
Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost clinical trial outcome prediction, exceeding various previous prediction approaches. Besides, we also demonstrate that TWIN-GPT can generate high-fidelity trial data that closely approximate specific patients, aiding in more accurate result predictions in data-scarce situations. Moreover, our study provides practical evidence for the application of digital twins in healthcare, highlighting its potential significance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01273">https://arxiv.org/abs/2404.01273</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper doesn't specifically address software or browser control, it covers a novel application of large language models in healthcare which certainly could spark your interest. The paper significantly focuses on the practical application of large language models to create digital twins for better clinical predictions. However, it doesn't seem to propose any radically new machine learning methods, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00211" target="_blank">Multi-Conditional Ranking with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.00211v1/extracted/5505319/fig/overview-base.jpg" target="_blank"><img src="https://arxiv.org/html/2404.00211v1/extracted/5505319/fig/overview-base.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pouya Pezeshkpour, Estevam Hruschka</p>
            <p><strong>Summary:</strong> arXiv:2404.00211v1 Announce Type: cross 
Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs' performance significantly, achieving up to a 12% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and an encoder-type ranking model, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00211">https://arxiv.org/abs/2404.00211</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It discusses the capabilities of Large Language Models (LLMs) in ranking and recommendation systems and proposes a novel method (EXSIR) to enhance the performance of LLMs in specific tasks of multi-conditional ranking. Although it does not directly address controlling software or web browsers using LLMs, it could provide valuable insights into the reasoning and decision-making capabilities of these models which could be applied to your areas of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00399" target="_blank">Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</a></h3>
            <a href="https://arxiv.org/html/2404.00399v1/extracted/5506093/img/171122956339012p5wxga-Photoroom.png-Photoroom.png" target="_blank"><img src="https://arxiv.org/html/2404.00399v1/extracted/5506093/img/171122956339012p5wxga-Photoroom.png-Photoroom.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen, Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo Pyysalo</p>
            <p><strong>Summary:</strong> arXiv:2404.00399v1 Announce Type: cross 
Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, Aurora-M and its variants are released at https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00399">https://arxiv.org/abs/2404.00399</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it describes Aurora-M, a multilingual open-source large language model, demonstrating its potential for applications in software control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00413" target="_blank">Language Models are Spacecraft Operators</a></h3>
            
            <p><strong>Authors:</strong> Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli Scharf, Peng Mun Siew, Richard Linares</p>
            <p><strong>Summary:</strong> arXiv:2404.00413v1 Announce Type: cross 
Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. Code is available at https://github.com/ARCLab-MIT/kspdg.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00413">https://arxiv.org/abs/2404.00413</a></p>
            <p><strong>Category:</strong> physics.space-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research is relevant as it uses Large Language Models (LLMs) as autonomous agents to operate in software, which aligns well with your interest in using large language models for software control. However, instead of specifically controlling a web browser or a general computer, it operates in a specific software context, thus the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00489" target="_blank">PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression</a></h3>
            <a href="https://arxiv.org/html/2404.00489v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00489v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang</p>
            <p><strong>Summary:</strong> arXiv:2404.00489v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform. Experimental evaluation using benchmark datasets shows that prompts compressed by PROMPT-SAW are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original prompt text by 33.0 and 56.7.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00489">https://arxiv.org/abs/2404.00489</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of Large Language Models (LLMs) for natural language processing tasks using new methods. While it does not directly discuss the control of software or web browsers or computer automation, the research about effectively leveraging LLMs can certainly be relevant and beneficial to your study of LLMs used as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00530" target="_blank">Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.00530v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00530v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover</p>
            <p><strong>Summary:</strong> arXiv:2404.00530v1 Announce Type: cross 
Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at https://github.com/Hritikbansal/dove.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00530">https://arxiv.org/abs/2404.00530</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It discusses a technique for aligning large language models (LLMs) using human preferences. Furthermore, it involves the concept of eliciting preferences over instruction-response pairs, which is integral to controlling software or web browsers using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00604" target="_blank">Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2404.00604v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00604v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang</p>
            <p><strong>Summary:</strong> arXiv:2404.00604v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at https://github.com/THUDM/Self-Contrast.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00604">https://arxiv.org/abs/2404.00604</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a novel method for large language models training without the need for extensive human feedback, which could be relevant in the context of creating agents based on large language models. However, it doesn't directly address controlling software or web browsers through large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00725" target="_blank">The Larger the Better? Improved LLM Code-Generation via Budget Reallocation</a></h3>
            <a href="https://arxiv.org/html/2404.00725v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00725v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi</p>
            <p><strong>Summary:</strong> arXiv:2404.00725v1 Announce Type: cross 
Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00725">https://arxiv.org/abs/2404.00725</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is focused on using different scaled LLMs for effective code generation, a subcategory of software control. Although it doesn't propose a new method, it provides an important comparison between variously scaled LLMs, which could be beneficial for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00756" target="_blank">Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery</a></h3>
            <a href="https://arxiv.org/html/2404.00756v1/extracted/5507347/pipeline_final.png" target="_blank"><img src="https://arxiv.org/html/2404.00756v1/extracted/5507347/pipeline_final.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cristina Cornelio, Mohammed Diab</p>
            <p><strong>Summary:</strong> arXiv:2404.00756v1 Announce Type: cross 
Abstract: Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00756">https://arxiv.org/abs/2404.00756</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'agents based on large-language models', specifically 'computer automation using large language models'. It discusses the integration of large language models in robotics for task failure detection and recovery plans generation, showing their practical application in automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00914" target="_blank">Token-Efficient Leverage Learning in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.00914v1/extracted/5505845/framework_overview.png" target="_blank"><img src="https://arxiv.org/html/2404.00914v1/extracted/5505845/framework_overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuanhao Zeng, Min Wang, Yihang Wang, Yingxia Shao</p>
            <p><strong>Summary:</strong> arXiv:2404.00914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00914">https://arxiv.org/abs/2404.00914</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the concept of Leverage Learning in Large LLMs which could be beneficial in understanding how to control and automate software using large language models. Although it doesn't directly mention control of web browsers or specific software, it could provide a foundation in controlling tasks in low-resource scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00942" target="_blank">Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs</a></h3>
            <a href="https://arxiv.org/html/2404.00942v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00942v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao</p>
            <p><strong>Summary:</strong> arXiv:2404.00942v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at https://github.com/xz-liu/GraphEval.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00942">https://arxiv.org/abs/2404.00942</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not propose methods for using LLMs to control software and web browsers or automation, it addresses the factuality of LLMs which can be valuable in strengthening the reliable use of large language models in these fields.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01268" target="_blank">Mapping the Increasing Use of LLMs in Scientific Papers</a></h3>
            <a href="https://arxiv.org/html/2404.01268v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.01268v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou</p>
            <p><strong>Summary:</strong> arXiv:2404.01268v1 Announce Type: cross 
Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01268">https://arxiv.org/abs/2404.01268</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on the use of large language models (LLMs) in academic writing, which aligns with your interest in agents based on large-language models, specifically in understanding more about how LLMs are utilized. However, it doesn't explicitly mention the control of software or web browsers with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.03294" target="_blank">DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training</a></h3>
            <a href="https://arxiv.org/html/2310.03294v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.03294v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Xuezhe Ma, Ion Stoica, Joseph E. Gonzalez, Hao Zhang</p>
            <p><strong>Summary:</strong> arXiv:2310.03294v2 Announce Type: replace 
Abstract: FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DISTFLASHATTN, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence lengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 - 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26 - 1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.03294">https://arxiv.org/abs/2310.03294</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper describes the use of a Distributed Memory-efficient Attention mechanism optimized for training large language models. Although it does not directly address controlling software or a web browser, it is a part of the foundation for creating set models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.10908" target="_blank">Unlocking Emergent Modularity in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.10908v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.10908v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zihan Qiu, Zeyu Huang, Jie Fu</p>
            <p><strong>Summary:</strong> arXiv:2310.10908v2 Announce Type: replace 
Abstract: Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models. Existing MNNs are generally $\textit{explicit}$: their modular architectures are pre-defined, with individual modules expected to implement distinct functions. Recent works reveal that there exists $\textit{implicit}$ modularity in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures spontaneously exhibit during the early pre-training phase. Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized. In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE). Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning. Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.10908">https://arxiv.org/abs/2310.10908</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on modular structures in standard pre-trained transformers, which is highly relevant for agents based on large-language models. Although it doesn't specifically discuss controlling software or web browsers, unlocking the emergent modularity in language models could potentially enhance automation capabilities.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14608" target="_blank">Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</a></h3>
            
            <p><strong>Authors:</strong> Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.14608v2 Announce Type: replace 
Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14608">https://arxiv.org/abs/2403.14608</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper investigates Parameter-Efficient Fine-Tuning (PEFT) of large language models which is relevant to your interest in agents based on large-language models. While it doesn't specifically address controlling software or web browsers via large language models, it provides useful insights on how to adapt large models for specific tasks in a computationally efficient manner.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.09781" target="_blank">SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification</a></h3>
            <a href="https://arxiv.org/html/2305.09781v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.09781v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia</p>
            <p><strong>Summary:</strong> arXiv:2305.09781v4 Announce Type: replace-cross 
Abstract: This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.09781">https://arxiv.org/abs/2305.09781</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it centers on large language model (LLM) serving with a system called 'SpecInfer', which introduces an innovative method to reduce end-to-end latency and computational requirement, an essential aspect of computer automation. Although it doesn't directly address control of software/web browsers, the acceleration methods could be applied in those domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.13549" target="_blank">A Survey on Multimodal Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2306.13549v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.13549v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen</p>
            <p><strong>Summary:</strong> arXiv:2306.13549v2 Announce Type: replace-cross 
Abstract: Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.13549">https://arxiv.org/abs/2306.13549</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper surveys Multimodal Large Language Models (MLLMs), a recent and rapidly developing research area relevant to large language models. It provides extensive coverage of methods and strategies for training these models, including Large Language Models' (LLMs) application to multimodal tasks. Although it does not explicitly mention controlling software or web browsers, you might find the sections on MLLM architecture and granular support useful for understanding their potential in automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08461" target="_blank">DistillSpec: Improving Speculative Decoding via Knowledge Distillation</a></h3>
            <a href="https://arxiv.org/html/2310.08461v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.08461v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran\c{c}ois Kagy, Rishabh Agarwal</p>
            <p><strong>Summary:</strong> arXiv:2310.08461v2 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08461">https://arxiv.org/abs/2310.08461</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper mainly deals with enhancing the performance of large language models, which aligns with your interest in agents based on large-language models. However, it doesn't explicitly deal with controlling software or web browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.09518" target="_blank">Instruction Tuning with Human Curriculum</a></h3>
            
            <p><strong>Authors:</strong> Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo</p>
            <p><strong>Summary:</strong> arXiv:2310.09518v3 Announce Type: replace-cross 
Abstract: In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the potential advantages of employing diverse curriculum strategies, and (3) delineate a synthetic instruction-response generation framework that complements our theoretical approach. Distinct from the existing instruction tuning dataset, our generation pipeline is systematically structured to emulate the sequential and orderly characteristic of human learning. Additionally, we describe a methodology for generating instruction-response datasets that extensively span the various stages of human education, from middle school through the graduate level, utilizing educational subject catalogs.
  Before training, we meticulously organize the instruction data to ensure that questions escalate in difficulty regarding (A) the subject matter and (B) the intricacy of the instructions. The findings of our study reveal that substantial improvements in performance can be achieved through the mere application of curriculum ordering to instruction data (achieving gains of +4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard) compared to random shuffling. This enhancement is achieved without incurring additional computational expenses. Through comprehensive experimentation, we observe that the advantages of our proposed method are consistently evident across nine benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.09518">https://arxiv.org/abs/2310.09518</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It explores the concept of instruction tuning with large language models and uses curriculum strategies, which could be used to control software or for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.07772" target="_blank">In-context Learning and Gradient Descent Revisited</a></h3>
            <a href="https://arxiv.org/html/2311.07772v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.07772v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar</p>
            <p><strong>Summary:</strong> arXiv:2311.07772v4 Announce Type: replace-cross 
Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significantly.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.07772">https://arxiv.org/abs/2311.07772</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper explores the inner workings of In-context Learning (ICL) with respect to large models for natural language processing tasks. Although not explicitly focused on automation or control, its insights could potentially contribute to the usage of large models in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.17076" target="_blank">Compositional Chain-of-Thought Prompting for Large Multimodal Models</a></h3>
            <a href="https://arxiv.org/html/2311.17076v3/extracted/5507938/figs/fig1_v7.png" target="_blank"><img src="https://arxiv.org/html/2311.17076v3/extracted/5507938/figs/fig1_v7.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig</p>
            <p><strong>Summary:</strong> arXiv:2311.17076v3 Announce Type: replace-cross 
Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs. Code: https://github.com/chancharikmitra/CCoT</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.17076">https://arxiv.org/abs/2311.17076</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses Large Multimodal Models (LMMs) and their use in vision and language tasks, which could be of interest to you as it details the use of Large Language Models in managing complex tasks. However, it doesn't specifically mention software or web browser control, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.06742" target="_blank">Honeybee: Locality-enhanced Projector for Multimodal LLM</a></h3>
            <a href="https://arxiv.org/html/2312.06742v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.06742v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh</p>
            <p><strong>Summary:</strong> arXiv:2312.06742v2 Announce Type: replace-cross 
Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, our proposed MLLM, Honeybee, remarkably outperforms previous state-of-the-art methods across various benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly higher efficiency. Code and models are available at https://github.com/kakaobrain/honeybee.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.06742">https://arxiv.org/abs/2312.06742</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses a new design for MLLMs, which can be insightful for your interest in 'Using large language models to control software' and 'Computer automation using large language models'. However, it does not specifically mention controlling web browsers or software, so it scores 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.09238" target="_blank">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a></h3>
            
            <p><strong>Authors:</strong> Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</p>
            <p><strong>Summary:</strong> arXiv:2312.09238v2 Announce Type: replace-cross 
Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors. Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories. In the next round, Reward Designer will further refine and iterate the dense reward function based on feedback. Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse in the plains biome.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.09238">https://arxiv.org/abs/2312.09238</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper relates to your interest in agents based on large-language models. It discusses an advanced learning system, Auto MC-Reward, that makes use of Large Language Models to automatically design dense reward functions, a method related to the control of software systems with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.11511" target="_blank">ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity</a></h3>
            <a href="https://arxiv.org/html/2312.11511v2/extracted/5505359/Figures/ProblemStatement.png" target="_blank"><img src="https://arxiv.org/html/2312.11511v2/extracted/5505359/Figures/ProblemStatement.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Henry Bae, Aghyad Deeb, Alex Fleury, Kehang Zhu</p>
            <p><strong>Summary:</strong> arXiv:2312.11511v2 Announce Type: replace-cross 
Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Language Models. Our findings suggest a promising direction for optimizing LLM applications, especially in resource-constrained environments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.11511">https://arxiv.org/abs/2312.11511</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper corresponds to your interest in large language models controlling software and optimizing their application. 'ComplexityNet' presents a new method for assessing task complexity using Language models. This can be particularly useful in automating tasks with varying complexities.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.14295" target="_blank">Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts</a></h3>
            <a href="https://arxiv.org/html/2401.14295v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.14295v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwa\'sniewski, J\"urgen M\"uller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Aidan O'Mahony, Onur Mutlu, Torsten Hoefler</p>
            <p><strong>Summary:</strong> arXiv:2401.14295v2 Announce Type: replace-cross 
Abstract: The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.14295">https://arxiv.org/abs/2401.14295</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the impact of using large language models for tasks that could include software control and automation. A particular focus is on 'prompt engineering', a technique that refines the model's output in the context of thought structures which might be relevant for achieving controlled, task-oriented outputs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.15043" target="_blank">Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2401.15043v2/extracted/5502861/figures/fkgl_figure.png" target="_blank"><img src="https://arxiv.org/html/2401.15043v2/extracted/5502861/figures/fkgl_figure.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger</p>
            <p><strong>Summary:</strong> arXiv:2401.15043v2 Announce Type: replace-cross 
Abstract: Objective: The reading level of health educational materials significantly influences the understandability and accessibility of the information, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.
  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research, comprising educational content from the American Cancer Society, Centers for Disease Control and Prevention, and National Cancer Institute. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based approaches. Our experimentation encompasses Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a lightweight model adept at distinguishing between original and simplified texts, thereby enhancing the model's effectiveness with unlabeled data.
  Results: Fine-tuned Llama 2 models demonstrated high performance across various metrics. Our innovative RLHF reward function surpassed existing RL text simplification reward functions in effectiveness. The results underscore that RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text and improving performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.15043">https://arxiv.org/abs/2401.15043</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on Large Language Models (LLM) and their application in health text simplification - a particular type of software control. Although it does not explicitly deal with web browsers or more general computer automation, it provides an example of using LLMs for practical, real-world task automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04154" target="_blank">Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction</a></h3>
            <a href="https://arxiv.org/html/2402.04154v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04154v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Zhaofeng He, Jie Fu</p>
            <p><strong>Summary:</strong> arXiv:2402.04154v5 Announce Type: replace-cross 
Abstract: Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning. However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer. Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04154">https://arxiv.org/abs/2402.04154</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the implementation of textual and visual guidance in decision networks for task-specific purposes, which aligns with your interest in using large language models for controlling software. The paper's main emphasis is on developing an AI agent with the 'read-to-play' ability, leveraging multimodal instructions within a context of decision transformer. However, it doesn't entirely focus on automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04616" target="_blank">TinyLLM: Learning a Small Student from Multiple Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.04616v2/extracted/5507749/Figures/pipeline.png" target="_blank"><img src="https://arxiv.org/html/2402.04616v2/extracted/5507749/Figures/pipeline.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla</p>
            <p><strong>Summary:</strong> arXiv:2402.04616v2 Announce Type: replace-cross 
Abstract: Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04616">https://arxiv.org/abs/2402.04616</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models, specifically ones that are efficiently transferable to smaller ones. The research proposes a new method, TinyLLM, focusing on language model knowledge distillation. However, it doesn't directly address using LLMs to control software or web browsers, or computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.07148" target="_blank">X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</a></h3>
            <a href="https://arxiv.org/html/2402.07148v2/extracted/5506336/MJB_0.png" target="_blank"><img src="https://arxiv.org/html/2402.07148v2/extracted/5506336/MJB_0.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Eric L. Buehler, Markus J. Buehler</p>
            <p><strong>Summary:</strong> arXiv:2402.07148v2 Announce Type: replace-cross 
Abstract: We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable and adaptable models with strong domain knowledge and the capability to integrate across areas of knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, protein biophysics, mechanics and quantum-mechanics based molecular properties, we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, adversarial agentic modeling including ontological knowledge graph construction, as well as molecular design. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins or quantum mechanical molecular properties, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.07148">https://arxiv.org/abs/2402.07148</a></p>
            <p><strong>Category:</strong> cond-mat.soft</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the implementation of large language models (LLMs) for complex tasks, precisely within the domain of protein mechanics and molecular design. Although not specifically about controlling software or web browsers, it provides broad insight into the capabilities of LLMs that might underpin similar applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10038" target="_blank">RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.10038v2/extracted/5504598/RLHF_flowchart.png" target="_blank"><img src="https://arxiv.org/html/2402.10038v2/extracted/5504598/RLHF_flowchart.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra</p>
            <p><strong>Summary:</strong> arXiv:2402.10038v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10038">https://arxiv.org/abs/2402.10038</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper should be of interest because it explores the alignment of large language models to user intent using a combination of rejection sampling and direct preference optimization. This falls under the category of large language model agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.07378" target="_blank">SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression</a></h3>
            <a href="https://arxiv.org/html/2403.07378v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.07378v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.07378v3 Announce Type: replace-cross 
Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.07378">https://arxiv.org/abs/2403.07378</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the SVD-LLM method which could be considered as new method in the domain of large language models. It doesn't directly talk about controlling software or web browsers, which is why the score is not a 5. However, given that it does touch upon Large Language Model compression, it could be of interest for enhancing performance of large-scale agent systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14589" target="_blank">ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy</a></h3>
            <a href="https://arxiv.org/html/2403.14589v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.14589v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2403.14589v3 Announce Type: replace-cross 
Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14589">https://arxiv.org/abs/2403.14589</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper utilises large language models (LLMs) in language agents, focusing on training them for automated decision-making. Although they do not provide specific applications to controlled software or web browsing, they provide a demonstration of refining the decision-making process of these LLM Agents. Thus, it could provide insights into the potential control capabilities of software and web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15796" target="_blank">Understanding Emergent Abilities of Language Models from the Loss Perspective</a></h3>
            <a href="https://arxiv.org/html/2403.15796v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.15796v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang</p>
            <p><strong>Summary:</strong> arXiv:2403.15796v2 Announce Type: replace-cross 
Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15796">https://arxiv.org/abs/2403.15796</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the abilities of large language models which is one of your primary areas of interest. However, it does not specifically mention their application in software or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19928" target="_blank">DiJiang: Efficient Large Language Models through Compact Kernelization</a></h3>
            <a href="https://arxiv.org/html/2403.19928v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.19928v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe Wang</p>
            <p><strong>Summary:</strong> arXiv:2403.19928v2 Announce Type: replace-cross 
Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19928">https://arxiv.org/abs/2403.19928</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is important for the 'llm-agents' tag as it discusses advanced techniques to reduce computational load of large language models, which are key to controlling software and browsers. Although it does not directly discuss application to agents, this could be a key component for agent-based models in the future.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00462" target="_blank">Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models</a></h3>
            <a href="https://arxiv.org/html/2404.00462v1/extracted/5506318/imgs/loop.png" target="_blank"><img src="https://arxiv.org/html/2404.00462v1/extracted/5506318/imgs/loop.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhenjiang Mao, Siqi Dai, Yuang Geng, Ivan Ruchkin</p>
            <p><strong>Summary:</strong> arXiv:2404.00462v1 Announce Type: new 
Abstract: A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by comparing estimated states instead of aggregating observation-wide error.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00462">https://arxiv.org/abs/2404.00462</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning, as it discusses foundation world models that make use of causally latent representations. Moreover, it involves the use of a large language model, which meets your sub-topic interest in using large language models for causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00848" target="_blank">Predictive Performance Comparison of Decision Policies Under Confounding</a></h3>
            <a href="https://arxiv.org/html/2404.00848v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00848v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Luke Guerdan, Amanda Coston, Kenneth Holstein, Zhiwei Steven Wu</p>
            <p><strong>Summary:</strong> arXiv:2404.00848v1 Announce Type: new 
Abstract: Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00848">https://arxiv.org/abs/2404.00848</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper proposes a new method for evaluating decision-making policies through causal inference and off-policy evaluation, which relates to your interest in causal discovery and causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00207" target="_blank">Causal Inference for Human-Language Model Collaboration</a></h3>
            <a href="https://arxiv.org/html/2404.00207v1/extracted/5505158/img/sscsc.png" target="_blank"><img src="https://arxiv.org/html/2404.00207v1/extracted/5505158/img/sscsc.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bohan Zhang, Yixin Wang, Paramveer S. Dhillon</p>
            <p><strong>Summary:</strong> arXiv:2404.00207v1 Announce Type: cross 
Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00207">https://arxiv.org/abs/2404.00207</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This manuscript investigates the interaction between humans and large language models, and provides a causal interpretation. The specific focus on causal inference makes it relevant to your interest in causality. However, it doesn't propose a new method for causal discovery or representation learning, which is why it doesn't score a 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00463" target="_blank">Addressing Both Statistical and Causal Gender Fairness in NLP Models</a></h3>
            
            <p><strong>Authors:</strong> Hannah Chen, Yangfeng Ji, David Evans</p>
            <p><strong>Summary:</strong> arXiv:2404.00463v1 Announce Type: cross 
Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00463">https://arxiv.org/abs/2404.00463</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning. It discusses both statistical and causal fairness, two important components of causal discovery. While it does not propose a new method, it analyses the effectiveness of the existing methods in reducing bias, which can provide insights for your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.00751" target="_blank">C-XGBoost: A tree boosting model for causal effect estimation</a></h3>
            <a href="https://arxiv.org/html/2404.00751v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.00751v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Niki Kiriakidou, Ioannis E. Livieris, Christos Diou</p>
            <p><strong>Summary:</strong> arXiv:2404.00751v1 Announce Type: cross 
Abstract: Causal effect estimation aims at estimating the Average Treatment Effect as well as the Conditional Average Treatment Effect of a treatment to an outcome from the available data. This knowledge is important in many safety-critical domains, where it often needs to be extracted from observational data. In this work, we propose a new causal inference model, named C-XGBoost, for the prediction of potential outcomes. The motivation of our approach is to exploit the superiority of tree-based models for handling tabular data together with the notable property of causal inference neural network-based models to learn representations that are useful for estimating the outcome for both the treatment and non-treatment cases. The proposed model also inherits the considerable advantages of XGBoost model such as efficiently handling features with missing values requiring minimum preprocessing effort, as well as it is equipped with regularization techniques to avoid overfitting/bias. Furthermore, we propose a new loss function for efficiently training the proposed causal inference model. The experimental analysis, which is based on the performance profiles of Dolan and Mor{\'e} as well as on post-hoc and non-parametric statistical tests, provide strong evidence about the effectiveness of the proposed approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.00751">https://arxiv.org/abs/2404.00751</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it presents a new causal inference model, C-XGBoost, for predicting potential outcomes. This falls under your subcategory of 'Causal discovery'. However, it does not involve large language models which is one of your specified areas of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.01299" target="_blank">CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</a></h3>
            <a href="https://arxiv.org/html/2404.01299v1/extracted/5509333/Figs/tom_2.png" target="_blank"><img src="https://arxiv.org/html/2404.01299v1/extracted/5509333/Figs/tom_2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ting En Lam, Yuhan Chen, Elston Tan, Eric Peh, Ruirui Chen, Paritosh Parmar, Basura Fernando</p>
            <p><strong>Summary:</strong> arXiv:2404.01299v1 Announce Type: cross 
Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. We will release our dataset, codes, and models to help future efforts in this domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.01299">https://arxiv.org/abs/2404.01299</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> Although it does not focus on machine learning, it may still be relevant to your interest in causal discovery. The paper presents a unique dataset for exploring causal relationships in dynamic visual scenes which can be potentially used to develop and train ML models for causality.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2205.13589" target="_blank">Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes</a></h3>
            
            <p><strong>Authors:</strong> Miao Lu, Yifei Min, Zhaoran Wang, Zhuoran Yang</p>
            <p><strong>Summary:</strong> arXiv:2205.13589v3 Announce Type: replace 
Abstract: We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the dataset. To our best knowledge, \texttt{P3O} is the first provably efficient offline RL algorithm for POMDPs with a confounded dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2205.13589">https://arxiv.org/abs/2205.13589</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it addresses causal inference in the context of reinforcement learning, which aligns with your interest in causal discovery. Moreover, it proposes a new method (P3O) for addressing the confounding bias, which might be of your interest as it fits in the theme of new method development. However, it does not explicitly discuss the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2208.14161" target="_blank">Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift</a></h3>
            
            <p><strong>Authors:</strong> Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</p>
            <p><strong>Summary:</strong> arXiv:2208.14161v3 Announce Type: replace 
Abstract: Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics.
  Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, which we refer to as the latent content variable. Within this new paradigm, we present an intricate causal generative model by introducing latent noises across domains, along with a latent content variable and a latent style variable to achieve more nuanced rendering of observational data. We demonstrate that the latent content variable can be identified up to block identifiability due to its versatile yet distinct causal structure. We anchor our theoretical insights into a novel MSDA method, which learns the label distribution conditioned on the identifiable latent content variable, thereby accommodating more substantial distribution shifts. The proposed approach showcases exceptional performance and efficacy on both simulated and real-world datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2208.14161">https://arxiv.org/abs/2208.14161</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it presents a new method for tackling the challenge of multi-source domain adaptation using a novel paradigm known as latent covariate shift. This approach aims to discover the latent cause of the label variable, aligning closely with your interest in 'Causal discovery' within the 'Causality and machine learning' topic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2212.04612" target="_blank">Training Data Influence Analysis and Estimation: A Survey</a></h3>
            
            <p><strong>Authors:</strong> Zayd Hammoudeh, Daniel Lowd</p>
            <p><strong>Summary:</strong> arXiv:2212.04612v3 Announce Type: replace 
Abstract: Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future research directions to make influence analysis more useful in practice as well as more theoretically and empirically sound. A curated, up-to-date list of resources related to influence analysis is available at https://github.com/ZaydH/influence_analysis_papers.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2212.04612">https://arxiv.org/abs/2212.04612</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper does not propose new methods or models, but survey on influence analysis can be useful for your interest in causal discovery and causal representation learning. It gives comprehensive knowledge of how training data influence can be quantified and estimated, which is closely related to causal relationships in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.00104" target="_blank">Causal State Distillation for Explainable Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2401.00104v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.00104v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenhao Lu, Xufeng Zhao, Thilo Fryen, Jae Hee Lee, Mengdi Li, Sven Magg, Stefan Wermter</p>
            <p><strong>Summary:</strong> arXiv:2401.00104v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an extension of RD that goes beyond sub-rewards to provide more informative explanations. Our approach is centred on a causal learning framework that leverages information-theoretic measures for explanation objectives that encourage three crucial properties of causal factors: causal sufficiency, sparseness, and orthogonality. These properties help us distill the cause-and-effect relationships between the agent's states and actions or rewards, allowing for a deeper understanding of its decision-making processes. Our framework is designed to generate local explanations and can be applied to a wide range of RL tasks with multiple reward channels. Through a series of experiments, we demonstrate that our approach offers more meaningful and insightful explanations for the agent's action selections.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.00104">https://arxiv.org/abs/2401.00104</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper would be relevant as it discusses causal learning in reinforcement learning agents. Though it does not directly involve large language models, it could provide valuable insights with regard to causal discovery and understanding within machine learning models and thus falls within your interest in 'Causality and machine learning.'</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02277" target="_blank">Causal Bayesian Optimization via Exogenous Distribution Learning</a></h3>
            <a href="https://arxiv.org/html/2402.02277v3/extracted/5509173/image/endo_intv.png" target="_blank"><img src="https://arxiv.org/html/2402.02277v3/extracted/5509173/image/endo_intv.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shaogang Ren, Xiaoning Qian</p>
            <p><strong>Summary:</strong> arXiv:2402.02277v3 Announce Type: replace 
Abstract: Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods. Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is developed by leveraging the learned exogenous distribution. Experiments on different datasets and applications show the benefits of our proposed method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02277">https://arxiv.org/abs/2402.02277</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it presents a new method in the area of Causal Bayesian Optimization and causal discovery. The paper could be particularly useful because it extends CBO to general causal schemes and uses flexible priors for noise or unobserved variables, potentially leading to more accurate models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19647" target="_blank">Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a></h3>
            <a href="https://arxiv.org/html/2403.19647v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.19647v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller</p>
            <p><strong>Summary:</strong> arXiv:2403.19647v2 Announce Type: replace 
Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19647">https://arxiv.org/abs/2403.19647</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper introduces methods for discovering causally implicated subnetworks in language models, which fits into your interest in causal discovery and using large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2209.07028" target="_blank">Estimating large causal polytrees from small samples</a></h3>
            <a href="https://arxiv.org/html/2209.07028v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2209.07028v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sourav Chatterjee, Mathukumalli Vidyasagar</p>
            <p><strong>Summary:</strong> arXiv:2209.07028v3 Announce Type: replace-cross 
Abstract: We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2209.07028">https://arxiv.org/abs/2209.07028</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning. Specifically, it covers the subtopic of 'Causal discovery'. It discusses an algorithm for estimating a large causal structure from a small sample set, which could further your understanding of causal representation and discovery in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.16859" target="_blank">Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs</a></h3>
            <a href="https://arxiv.org/html/2308.16859v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.16859v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mishfad Shaikh Veedu, Deepjyoti Deka, Murti V. Salapaka</p>
            <p><strong>Summary:</strong> arXiv:2308.16859v2 Announce Type: replace-cross 
Abstract: In this article, the optimal sample complexity of learning the underlying interactions or dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. We call such a DAG underlying an LDS as dynamical DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static DAG setting, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. It is shown that the optimal sample complexity (or length of state trajectory) needed to learn the DDAG is $n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is the maximum number of parents per node. To prove the sample complexity upper bound, a concentration bound for the PSD estimation is derived, under two different sampling strategies. A matching min-max lower bound using generalized Fano's inequality also is provided, thus showing the order optimality of the proposed algorithm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.16859">https://arxiv.org/abs/2308.16859</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'causality and machine learning' as it discusses learning dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG), which indirectly relates to causal discovery. However, it does not specifically mention using large language models in causal discovery, hence the score is 4 and not 5.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 02, 2024 at 21:43:42</div></body></html>