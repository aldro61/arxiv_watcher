
            <html>
            <head>
                <title>Report Generated on April 04, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 04, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02402" target="_blank">Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM</a></h3>
            <a href="https://arxiv.org/html/2404.02402v1/extracted/5512948/logo1.png" target="_blank"><img src="https://arxiv.org/html/2404.02402v1/extracted/5512948/logo1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Md. Kowsher, Ritesh Panditi, Nusrat Jahan Prottasha, Prakash Bhat, Anupam Kumar Bairagi, Mohammad Shamsul Arefin</p>
            <p><strong>Summary:</strong> arXiv:2404.02402v1 Announce Type: cross 
Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02402">https://arxiv.org/abs/2404.02402</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests because it addresses the use of large language models in conversational AI, which is similar to controlling software or web browsers through language models. It proposes a new approach, Token Trails, for better conversation modeling, suggesting improvements in automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02325" target="_blank">Heat Death of Generative Models in Closed-Loop Learning</a></h3>
            
            <p><strong>Authors:</strong> Matteo Marchi, Stefano Soatto, Pratik Chaudhari, Paulo Tabuada</p>
            <p><strong>Summary:</strong> arXiv:2404.02325v1 Announce Type: new 
Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.
  Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been only limited theoretical understanding of this process, in part due to the complexity of the deep networks underlying these generative models.
  The aim of this paper is to provide insights into this process (that we refer to as "generative closed-loop learning") by studying the learning dynamics of generative models that are fed back their own produced content in addition to their original training dataset. The sampling of many of these models can be controlled via a "temperature" parameter. Using dynamical systems tools, we show that, unless a sufficient amount of external data is introduced at each iteration, any non-trivial temperature leads the model to asymptotically degenerate. In fact, either the generative distribution collapses to a small set of outputs, or becomes uniform over a large set of outputs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02325">https://arxiv.org/abs/2404.02325</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the dynamics of Large Language Models (LLMs) when their generated data is fed back into them during subsequent training. While it does not touch on the specific use of LLM in software control or web browser control, it does provide insights into the learning process and behaviour of LLMs. The understanding gained from this paper could help in effective usage and deployment of LLMs in various tasks, including those you are interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02450" target="_blank">Task Agnostic Architecture for Algorithm Induction via Implicit Composition</a></h3>
            <a href="https://arxiv.org/html/2404.02450v1/extracted/5513129/overview_high_res.png" target="_blank"><img src="https://arxiv.org/html/2404.02450v1/extracted/5513129/overview_high_res.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sahil J. Sindhi, Ignas Budvytis</p>
            <p><strong>Summary:</strong> arXiv:2404.02450v1 Announce Type: new 
Abstract: Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inherently operates sequentially. Second, recent Generative AI, especially Transformer-based models, demonstrate potential as an architecture capable of constructing algorithms for a wide range of domains. For example, GPT-4 shows exceptional capability at in-context learning of novel tasks which is hard to explain in any other way than the ability to compose novel solutions from fragments on previously learnt algorithms. Third, the observation that the main missing component in developing a truly generalised network is an efficient approach for self-consistent input of previously learnt sub-steps of an algorithm and their (implicit) composition during the network's internal forward pass. Our exploration delves into current capabilities and limitations of Transformer-based and other methods in efficient and correct algorithm composition and proposes a Transformer-like architecture as well as a discrete learning framework to overcome these limitations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02450">https://arxiv.org/abs/2404.02450</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper can be of interest to you as it discusses the development of a generalized network architecture capable of solving all tasks, applicable to your interest in using large language models for various tasks. It evaluates the capabilities of Transformer-based models in context learning which parallels your interest for new transformer-like models for time series but with a slight stretch in the direct correlation to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02649" target="_blank">On the Importance of Uncertainty in Decision-Making with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02649v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02649v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nicol\`o Felicioni, Lucas Maystre, Sina Ghiassian, Kamil Ciosek</p>
            <p><strong>Summary:</strong> arXiv:2404.02649v1 Announce Type: new 
Abstract: We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02649">https://arxiv.org/abs/2404.02649</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper delves into the application of Large Language Models as agents in decision-making tasks, which is one of your areas of interest. It doesn't propose a specific new method for using LLMs to control software or web browsers, but the exploration of uncertainty estimation in the context of LLM agents could be a valuable insight for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02650" target="_blank">Towards detecting unanticipated bias in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02650v1/extracted/5513897/img/height_bias2.png" target="_blank"><img src="https://arxiv.org/html/2404.02650v1/extracted/5513897/img/height_bias2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Anna Kruspe</p>
            <p><strong>Summary:</strong> arXiv:2404.02650v1 Announce Type: new 
Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of LLMs more transparent, thereby identifying and understanding biases that are not immediately apparent. Through this research, we aim to contribute to the development of fairer and more transparent AI systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02650">https://arxiv.org/abs/2404.02650</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper addresses the investigation of biases in Large Language Models (LLMs) which is closely aligned with your interest in agents based on LLMs. However, it doesn't directly cover using LLMs for control in software or browsers, or automation which is why it's not given the highest relevance score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02827" target="_blank">BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02827v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02827v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qijun Luo, Hengxu Yu, Xiao Li</p>
            <p><strong>Summary:</strong> arXiv:2404.02827v1 Announce Type: new 
Abstract: This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is available at https://github.com/Ledzy/BAdam.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02827">https://arxiv.org/abs/2404.02827</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper introduces BAdam, a new memory efficient optimizer for training large language models. Although it doesn't directly concern controlling software or web browsers, the advancement in training LLMs could potentially improve the performance of such tasks. And considering your focus on papers that introduce new methods rather than applications, this paper seems to be highly relevant.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02852" target="_blank">Toward Inference-optimal Mixture-of-Expert Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02852v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02852v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P Xing, Hao Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.02852v1 Announce Type: new 
Abstract: Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02852">https://arxiv.org/abs/2404.02852</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper is somewhat tangential to your specific interests, it discusses the issue of optimizing Large Language Models with a Mixture-of-Expert framework. This is relevant to your interest in 'agents based on large-language models', as these models can be instrumental for tasks like controlling software and web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02183" target="_blank">Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.02183v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02183v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yoichi Ishibashi, Yoshimasa Nishimura</p>
            <p><strong>Summary:</strong> arXiv:2404.02183v1 Announce Type: cross 
Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02183">https://arxiv.org/abs/2404.02183</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is mainly about using large language models in software development and code generation which is related to your interests in llm-agents. However, it does not cover using llm in web browser control or computer automation specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02204" target="_blank">Emergent Abilities in Reduced-Scale Generative Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02204v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02204v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sherin Muckatira, Vijeta Deshpande, Vladislav Lialin, Anna Rumshisky</p>
            <p><strong>Summary:</strong> arXiv:2404.02204v1 Announce Type: cross 
Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we find that these smaller models pre-trained on simplified data demonstrate a power law relationship between the evaluation loss and the three scaling factors: compute, dataset size, and model size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02204">https://arxiv.org/abs/2404.02204</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your research interest in large language models. It involves understanding the emergent properties and capabilities of these models in new tasks without the need for task-specific tuning. However, it does not explicitly focus on their application for software and web browser control, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02261" target="_blank">LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages</a></h3>
            <a href="https://arxiv.org/html/2404.02261v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02261v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer</p>
            <p><strong>Summary:</strong> arXiv:2404.02261v1 Announce Type: cross 
Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02261">https://arxiv.org/abs/2404.02261</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant because it covers the usage of large language models in annotation and potentially automation tasks for low-resource languages. It doesn't directly relate to controlling software or web browsers but it broadly features automation and large language models which aligns with your llm-agents interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02294" target="_blank">Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs</a></h3>
            <a href="https://arxiv.org/html/2404.02294v1/extracted/5512507/pics/rc_car.jpeg" target="_blank"><img src="https://arxiv.org/html/2404.02294v1/extracted/5512507/pics/rc_car.jpeg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David Meger, Gregory Dudek</p>
            <p><strong>Summary:</strong> arXiv:2404.02294v1 Announce Type: cross 
Abstract: This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02294">https://arxiv.org/abs/2404.02294</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method of using large language models for map-free robotic navigation, thus fits within your interest in computer automation using large language models. However, it does not directly touch on controlling software or web browsers, so it may be less relevant to some aspects of your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02319" target="_blank">Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.02319v1/extracted/5512623/figures/overview.png" target="_blank"><img src="https://arxiv.org/html/2404.02319v1/extracted/5512623/figures/overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tobias Schnabel, Jennifer Neville</p>
            <p><strong>Summary:</strong> arXiv:2404.02319v1 Announce Type: cross 
Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.
  We make all code available open-source at https://github.com/microsoft/sammo .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02319">https://arxiv.org/abs/2404.02319</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses optimization techniques for large language models when handling complex inputs and prompts, which directly relates to your interest in using large language models for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02407" target="_blank">Decision Transformer as a Foundation Model for Partially Observable Continuous Control</a></h3>
            <a href="https://arxiv.org/html/2404.02407v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02407v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangyuan Zhang, Weichao Mao, Haoran Qiu, Tamer Ba\c{s}ar</p>
            <p><strong>Summary:</strong> arXiv:2404.02407v1 Announce Type: cross 
Abstract: Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks. DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data. These findings highlight the potential of DT as a foundational controller for general control applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02407">https://arxiv.org/abs/2404.02407</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the Decision Transformer as a controller synthesis framework initialized with the Generative Pre-trained Transformer, which falls under your interest in agents based on large-language models. It focuses on controlling nonlinear dynamical systems and surpasses expert performance levels. However, it is not directly related to explicit software or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02474" target="_blank">uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?</a></h3>
            <a href="https://arxiv.org/html/2404.02474v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02474v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh</p>
            <p><strong>Summary:</strong> arXiv:2404.02474v1 Announce Type: cross 
Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02474">https://arxiv.org/abs/2404.02474</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents experiments that involve large language models to enhance performance and generate datasets, aligning with your interest in using large language models for tasks such as computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02491" target="_blank">Measuring Social Norms of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02491v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02491v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ye Yuan, Kexin Tang, Jianhao Shen, Ming Zhang, Chenguang Wang</p>
            <p><strong>Summary:</strong> arXiv:2404.02491v1 Announce Type: cross 
Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02491">https://arxiv.org/abs/2404.02491</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models (LLMs) and proposes a multi-agent framework based on LLMs to improve the models' understanding of social norms, implying a controlling ability relevant to your interest in 'Agents based on large-language models'. However, it doesn't explicitly discuss software or web browser control via LLMs, which slightly limits its relevance.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02717" target="_blank">Automatic Prompt Selection for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02717v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02717v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Viet-Tung Do, Van-Khanh Hoang, Duy-Hung Nguyen, Shahab Sabahi, Jeff Yang, Hajime Hotta, Minh-Tien Nguyen, Hung Le</p>
            <p><strong>Summary:</strong> arXiv:2404.02717v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference. It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02717">https://arxiv.org/abs/2404.02717</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly involves work on Large Language Models (LLMs), which is one of your specified areas of interest. The focus on manual instruction prompts and their automatic optimization may provide insights for your interests in computer automation and control of software through LLMs. However, it does not directly reference control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02823" target="_blank">Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02823v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02823v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang</p>
            <p><strong>Summary:</strong> arXiv:2404.02823v1 Announce Type: cross 
Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02823">https://arxiv.org/abs/2404.02823</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models for complex instruction following, which aligns with your interest in automation using large language models. However, it does not specifically indicate controls over web browsers and software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.18742" target="_blank">Understanding the Learning Dynamics of Alignment with Human Feedback</a></h3>
            <a href="https://arxiv.org/html/2403.18742v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.18742v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shawn Im, Yixuan Li</p>
            <p><strong>Summary:</strong> arXiv:2403.18742v2 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.18742">https://arxiv.org/abs/2403.18742</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models and how they align with human intentions. While it doesn't explicitly discuss control of software or browsers, the human-preference alignment of these models appears to be central to the automation processes that you specified in your interests. Therefore, it might be valuable to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.20208" target="_blank">Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science</a></h3>
            <a href="https://arxiv.org/html/2403.20208v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.20208v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu</p>
            <p><strong>Summary:</strong> arXiv:2403.20208v2 Announce Type: replace 
Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.20208">https://arxiv.org/abs/2403.20208</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper utilizes Large Language Models (LLMs) to deal with structured data, which is a form of automation. However, it does not directly deal with controlling software or web browsers, hence the slightly lower score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.16888" target="_blank">Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection</a></h3>
            <a href="https://arxiv.org/html/2307.16888v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.16888v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin</p>
            <p><strong>Summary:</strong> arXiv:2307.16888v3 Announce Type: replace-cross 
Abstract: Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model's instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.16888">https://arxiv.org/abs/2307.16888</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models and their implications. While it doesn't specifically focus on their use in software control or browser control, it does provide important insights into the potential vulnerabilities and risks of instruction-tuned LLMs, which could be a key aspect if deploying these models in practical applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.02263" target="_blank">Automatic Pair Construction for Contrastive Post-training</a></h3>
            <a href="https://arxiv.org/html/2310.02263v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.02263v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Canwen Xu, Corby Rosset, Ethan C. Chau, Luciano Del Corro, Shweti Mahajan, Julian McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</p>
            <p><strong>Summary:</strong> arXiv:2310.02263v2 Announce Type: replace-cross 
Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.02263">https://arxiv.org/abs/2310.02263</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a novel automatic way to construct contrastive data for LLMs, which can be crucial to automating tasks in a variety of fields. However, there seems to be more of a focus on improving alignment rather than controlling specific software or web applications which you have expressed an interest in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.01544" target="_blank">Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization</a></h3>
            <a href="https://arxiv.org/html/2311.01544v3/extracted/5513995/images/fdt-metric-example.png" target="_blank"><img src="https://arxiv.org/html/2311.01544v3/extracted/5513995/images/fdt-metric-example.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bj\"orn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias A{\ss}enmacher, Kristian Kersting</p>
            <p><strong>Summary:</strong> arXiv:2311.01544v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. However, their ever-increasing size has raised concerns about their effective deployment and the need for LLM compression. This study introduces the Divergent Token Metrics (DTMs), a novel approach to assessing compressed LLMs, addressing the limitations of traditional perplexity or accuracy measures that fail to accurately reflect text generation quality. DTMs measure token divergences that allow deeper insights into the subtleties of model compression, in particular, when evaluating components' impacts individually. Utilizing the First Divergent Token Metric (FDTM) in model sparsification reveals that 25% of all attention components can be pruned beyond 90% on the Llama-2 model family, still keeping SOTA performance. For quantization, FDTM suggests that more than 80% of parameters can be naively transformed to int8 without special outlier management. These evaluations indicate the necessity of choosing appropriate compressions for parameters individually -- and that FDTM can identify those -- while standard metrics result in deteriorated outcomes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.01544">https://arxiv.org/abs/2311.01544</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses large language model optimization, which includes pruning away components and size reduction. This could be crucial in building more efficient agents using these models, although it doesn't directly mention controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00097" target="_blank">Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM</a></h3>
            <a href="https://arxiv.org/html/2402.00097v2/extracted/5512616/figures/working_example_fm.png" target="_blank"><img src="https://arxiv.org/html/2402.00097v2/extracted/5512616/figures/working_example_fm.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, Baishakhi Ray</p>
            <p><strong>Summary:</strong> arXiv:2402.00097v2 Announce Type: replace-cross 
Abstract: Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent works using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00097">https://arxiv.org/abs/2402.00097</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'agents based on large-language models' topic. It explores Code-Aware Prompting in Large Language Models (LLMs) for test generation in software, which fits under the subtopic 'Using large language models to control software'. Notably, it discusses a new methodology (SymPrompt) applied to test generation, making it strongly aligned with your interest in new methods rather than applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01687" target="_blank">"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students</a></h3>
            
            <p><strong>Authors:</strong> Vibhor Agarwal, Madhav Krishan Garg, Sahiti Dharmavaram, Dhruv Kumar</p>
            <p><strong>Summary:</strong> arXiv:2402.01687v2 Announce Type: replace-cross 
Abstract: This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students in India. These tasks include code explanation and documentation, solving class assignments, technical interview preparation, learning new concepts and frameworks, and email writing. Evaluation for these tasks was carried out by pre-final year and final year undergraduate computer science students and provides insights into the models' strengths and limitations. This study aims to guide students as well as instructors in selecting suitable LLMs for any specific task and offers valuable insights on how LLMs can be used constructively by students and instructors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01687">https://arxiv.org/abs/2402.01687</a></p>
            <p><strong>Category:</strong> cs.CY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper aligns with your interests in large language models for task automation. However, it focuses more on task evaluations, rather than proposing a new method or tool to control software or browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12343" target="_blank">Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!</a></h3>
            <a href="https://arxiv.org/html/2402.12343v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12343v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao</p>
            <p><strong>Summary:</strong> arXiv:2402.12343v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training. Specifically, this reversal is achieved by contrasting the output token distribution of a safety-aligned language model (e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2) so that the token predictions are shifted towards the opposite direction of alignment. We name this method emulated disalignment (ED) because it uses pure sampling to provably emulate (or "approximate") the result of fine-tuning the pre-trained model to minimize a safety reward. Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Eventually, given ED's need for language model output token distributions, which particularly compromises open-source models, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12343">https://arxiv.org/abs/2402.12343</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses safety alignment issues regarding large language models, which is not directly linked to your preferred subtopics, but still highly relevant since ensuring safety is part of the process when using large language models to control software, web browsers, or computer automation.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.05713" target="_blank">tsGT: Stochastic Time Series Modeling With Transformer</a></h3>
            <a href="https://arxiv.org/html/2403.05713v3/extracted/5514808/images/strip.png" target="_blank"><img src="https://arxiv.org/html/2403.05713v3/extracted/5514808/images/strip.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> {\L}ukasz Kuci\'nski, Witold Drzewakowski, Mateusz Olko, Piotr Kozakowski, {\L}ukasz Maziarka, Marta Emilia Nowakowska, {\L}ukasz Kaiser, Piotr Mi{\l}o\'s</p>
            <p><strong>Summary:</strong> arXiv:2403.05713v3 Announce Type: replace 
Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing tsGT, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that tsGT outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of tsGT's ability to model the data distribution and predict marginal quantile values.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.05713">https://arxiv.org/abs/2403.05713</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces 'tsGT', a new transformer-like model for stochastic time series forecasting. It also delves into model performance on several datasets, which could be interesting for your subtopic of datasets used to train foundation models for time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02722" target="_blank">On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices</a></h3>
            <a href="https://arxiv.org/html/2404.02722v1/extracted/5513846/figures/score_quantiles.png" target="_blank"><img src="https://arxiv.org/html/2404.02722v1/extracted/5513846/figures/score_quantiles.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alessandro Brusaferri, Andrea Ballarino, Luigi Grossi, Fabrizio Laurini</p>
            <p><strong>Summary:</strong> arXiv:2404.02722v1 Announce Type: new 
Abstract: Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02722">https://arxiv.org/abs/2404.02722</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper appears relevant because it focuses on forecasting in the context of electricity prices, showing it's about time series. It also presents a new method which extends the state of the art neural networks ensembles with conformal inference techniques, demonstrating the aspect of novelty you're interested in. However, it doesn't explicitly mention deep learning or multimodal/transformer-like methodologies.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15027" target="_blank">Grey-informed neural network for time-series forecasting</a></h3>
            
            <p><strong>Authors:</strong> Wanli Xie, Ruibin Zhao, Zhenguo Xu, Tingting Liang</p>
            <p><strong>Summary:</strong> arXiv:2403.15027v2 Announce Type: replace 
Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15027">https://arxiv.org/abs/2403.15027</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper is relevant to your interest in time-series and deep learning as it proposes a new method, a Grey-informed Neural Network (GINN), for time-series forecasting. However, it doesn't seem to focus on transformer-like models or multimodal deep learning models for time series which lowers its score slightly.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19800" target="_blank">Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction</a></h3>
            <a href="https://arxiv.org/html/2403.19800v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.19800v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jhon A. Castro-Correa, Jhony H. Giraldo, Mohsen Badiey, Fragkiskos D. Malliaros</p>
            <p><strong>Summary:</strong> arXiv:2403.19800v2 Announce Type: replace 
Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand the complexity of the model and offer a more accurate solution for recovering time-varying graph signals. Building upon GegenConv, we design the Gegenbauer-based time Graph Neural Network (GegenGNN) architecture, which adopts an encoder-decoder structure. Likewise, our approach also utilizes a dedicated loss function that incorporates a mean squared error component alongside Sobolev smoothness regularization. This combination enables GegenGNN to capture both the fidelity to ground truth and the underlying smoothness properties of the signals, enhancing the reconstruction performance. We conduct extensive experiments on real datasets to evaluate the effectiveness of our proposed approach. The experimental results demonstrate that GegenGNN outperforms state-of-the-art methods, showcasing its superior capability in recovering time-varying graph signals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19800">https://arxiv.org/abs/2403.19800</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be of interest as it proposes a new method (Gegenbauer-based time Graph Neural Network) for accurately capturing spatio-temporal information, which is critical in time-series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.05743" target="_blank">Forecasting Electricity Market Signals via Generative AI</a></h3>
            <a href="https://arxiv.org/html/2403.05743v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.05743v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinyi Wang, Qing Zhao, Lang Tong</p>
            <p><strong>Summary:</strong> arXiv:2403.05743v2 Announce Type: replace-cross 
Abstract: This paper presents a generative artificial intelligence approach to probabilistic forecasting of electricity market signals, such as real-time locational marginal prices and area control error signals. Inspired by the Wiener-Kallianpur innovation representation of nonparametric time series, we propose a weak innovation autoencoder architecture and a novel deep learning algorithm that extracts the canonical independent and identically distributed innovation sequence of the time series, from which future time series samples are generated. The validity of the proposed approach is established by proving that, under ideal training conditions, the generated samples have the same conditional probability distribution as that of the ground truth. Three applications involving highly dynamic and volatile time series in real-time market operations are considered: (i) locational marginal price forecasting for self-scheduled resources such as battery storage participants, (ii) interregional price spread forecasting for virtual bidders in interchange markets, and (iii) area control error forecasting for frequency regulations. Numerical studies based on market data from multiple independent system operators demonstrate the superior performance of the proposed generative forecaster over leading classical and modern machine learning techniques under both probabilistic and point forecasting metrics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.05743">https://arxiv.org/abs/2403.05743</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces new deep learning algorithms for time series forecasting, specifically in the context of electricity market signals. It proposes a novel deep learning method called a weak innovation autoencoder. Although it doesn't directly describe foundation models, datasets, multimodal attributes, or transformer-like methods, the methods used might still be of interest.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2211.13715" target="_blank">Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery</a></h3>
            <a href="https://arxiv.org/html/2211.13715v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2211.13715v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mateusz Olko, Micha{\l} Zaj\k{a}c, Aleksandra Nowak, Nino Scherrer, Yashas Annadani, Stefan Bauer, {\L}ukasz Kuci\'nski, Piotr Mi{\l}o\'s</p>
            <p><strong>Summary:</strong> arXiv:2211.13715v5 Announce Type: replace-cross 
Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2211.13715">https://arxiv.org/abs/2211.13715</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it addresses the subtopic of 'causal discovery'. It proposes a new method, GIT, for determining the most informative intervention target to improve causal discovery. The method appears to perform well, especially in low-data situations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.01449" target="_blank">A Double Machine Learning Approach to Combining Experimental and Observational Data</a></h3>
            <a href="https://arxiv.org/html/2307.01449v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.01449v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Harsh Parikh, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</p>
            <p><strong>Summary:</strong> arXiv:2307.01449v2 Announce Type: replace-cross 
Abstract: Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one of these assumptions is violated, we provide semiparametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.01449">https://arxiv.org/abs/2307.01449</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper may be of interest to you as it delves into the combination of experimental and observational data using machine learning, which can be seen as an approach to causal discovery. Importantly, it has a focus on testing and estimating treatment effects, a key aspect of causal analysis.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 04, 2024 at 21:34:41</div></body></html>