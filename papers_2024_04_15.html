
            <html>
            <head>
                <title>Report Generated on April 15, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 15, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08008" target="_blank">Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition</a></h3>
            <a href="https://arxiv.org/html/2404.08008v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08008v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kehua Feng, Keyan Ding, Kede Ma, Zhihua Wang, Qiang Zhang, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2404.08008v1 Announce Type: new 
Abstract: The past years have witnessed a proliferation of large language models (LLMs). Yet, automated and unbiased evaluation of LLMs is challenging due to the inaccuracy of standard metrics in reflecting human preferences and the inefficiency in sampling informative and diverse test examples. While human evaluation remains the gold standard, it is expensive and time-consuming, especially when dealing with a large number of testing samples. To address this problem, we propose a sample-efficient human evaluation method based on MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative and diverse instructions, each adapted to two LLMs, whose responses are subject to three-alternative forced choice by human subjects. The pairwise comparison results are then aggregated into a global ranking using the Elo rating system. We select eight representative LLMs and compare them in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method achieves a reliable and sensible ranking of LLMs' capabilities, identifies their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08008">https://arxiv.org/abs/2404.08008</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the evaluation of large language models and their abilities in various skills including coding. This would inform you about the capabilities of large language models that could be used for controlling software or web browsers, and for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08029" target="_blank">A Multi-Expert Large Language Model Architecture for Verilog Code Generation</a></h3>
            <a href="https://arxiv.org/html/2404.08029v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08029v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bardia Nadimi, Hao Zheng</p>
            <p><strong>Summary:</strong> arXiv:2404.08029v1 Announce Type: new 
Abstract: Recently, there has been a surging interest in using large language models (LLMs) for Verilog code generation. However, the existing approaches are limited in terms of the quality of the generated Verilog code. To address such limitations, this paper introduces an innovative multi-expert LLM architecture for Verilog code generation (MEV-LLM). Our architecture uniquely integrates multiple LLMs, each specifically fine-tuned with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog code for each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08029">https://arxiv.org/abs/2404.08029</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large-language models (LLMs) as it discusses a novel method of using LLMs specifically for Verilog code generation, which is a form of software control. However, the context of use may not fully align with your specific interest in controlling web browsers or general computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08080" target="_blank">Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.08080v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08080v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tanmay Gautam, Youngsuk Park, Hao Zhou, Parameswaran Raman, Wooseok Ha</p>
            <p><strong>Summary:</strong> arXiv:2404.08080v1 Announce Type: new 
Abstract: Fine-tuning language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot and in-context learning when combined with suitable task prompts. In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts. Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO's peak test accuracy with a $2\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order SGD, i.e. by $2\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG's memory savings progressively improve compared to SGD with larger batch sizes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08080">https://arxiv.org/abs/2404.08080</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not directly relate to controlling software or web browsers with large language models, it discusses an optimization method for fine-tuning language models, which could be crucial in enhancing the performance of your large language model-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08189" target="_blank">Reducing hallucination in structured outputs via Retrieval-Augmented Generation</a></h3>
            <a href="https://arxiv.org/html/2404.08189v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08189v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Patrice B\'echard, Orlando Marquez Ayala</p>
            <p><strong>Summary:</strong> arXiv:2404.08189v1 Announce Type: new 
Abstract: A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08189">https://arxiv.org/abs/2404.08189</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper details Retrieval-Augmented Generation (RAG) as a method of reducing hallucinations and improving generalization in Large Language Models. Therefore, it may be deemed relevant to your interests on the use of large language models in controlling software and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08555" target="_blank">RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.08555v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08555v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva</p>
            <p><strong>Summary:</strong> arXiv:2404.08555v1 Announce Type: new 
Abstract: State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08555">https://arxiv.org/abs/2404.08555</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large language models. It analyzes reinforcement learning from human feedback (RLHF) techniques which are methods used to train large language models to serve as effective assistants. Although it does not directly address controlling software or web browsers, the insights from this paper could be applicable to these scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08001" target="_blank">Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics</a></h3>
            
            <p><strong>Authors:</strong> Zhengde Zhang, Yiyu Zhang, Haodong Yao, Jianwen Luo, Rui Zhao, Bo Huang, Jiameng Zhao, Yipu Liao, Ke Li, Lina Zhao, Jun Cao, Fazhi Qi, Changzheng Yuan</p>
            <p><strong>Summary:</strong> arXiv:2404.08001v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific field, it's challenging to acquire unique domain knowledge while keeping the model itself advanced. To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge. In this work, we will report on the best practices for applying LLMs in the field of high-energy physics (HEP), including: a seed fission technology is proposed and some data collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system has been developed to facilitate rapid training under a specified foundation model. The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna, ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model on the HEP knowledge question-and-answering and code generation. This strategy significantly enhances the potential for growth of our model's performance, with the hope of surpassing GPT-4 as it evolves with the development of open-source models. This work provides a customized LLM for the field of HEP, while also offering references for applying LLM to other fields, the corresponding codes are available on Github.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08001">https://arxiv.org/abs/2404.08001</a></p>
            <p><strong>Category:</strong> hep-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses advancements in Large Language Models (LLMs), including their application in a specific scientific field, with a focus on model training and features for domain knowledge application. Even though it doesn't directly mention controlling software or web browsers, it lays foundations & best practices on applying LLMs which can be extrapolated to your areas of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08078" target="_blank">SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions</a></h3>
            <a href="https://arxiv.org/html/2404.08078v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08078v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling</p>
            <p><strong>Summary:</strong> arXiv:2404.08078v1 Announce Type: cross 
Abstract: Stance detection is an important task for many applications that analyse or support online political discussions. Common approaches include fine-tuning transformer based models. However, these models require a large amount of labelled data, which might not be available. In this work, we present two different ways to leverage LLM-generated synthetic data to train and improve stance detection agents for online political discussions: first, we show that augmenting a small fine-tuning dataset with synthetic data can improve the performance of the stance detection model. Second, we propose a new active learning method called SQBC based on the "Query-by-Comittee" approach. The key idea is to use LLM-generated synthetic data as an oracle to identify the most informative unlabelled samples, that are selected for manual labelling. Comprehensive experiments show that both ideas can improve the stance detection performance. Curiously, we observed that fine-tuning on actively selected samples can exceed the performance of using the full dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08078">https://arxiv.org/abs/2404.08078</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines a new use of large language models to improve stance detection in online political discussions. Though it does not specifically mention control over software or web browsers directly, the method and results could potentially be applied to these domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08164" target="_blank">Language Model Prompt Selection via Simulation Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.08164v1/extracted/5531811/expro.png" target="_blank"><img src="https://arxiv.org/html/2404.08164v1/extracted/5531811/expro.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoting Zhang, Jinghai He, Rhonda Righter, Zeyu Zheng</p>
            <p><strong>Summary:</strong> arXiv:2404.08164v1 Announce Type: cross 
Abstract: With the advancement in generative language models, the selection of prompts has gained significant attention in recent years. A prompt is an instruction or description provided by the user, serving as a guide for the generative language model in content generation. Despite existing methods for prompt selection that are based on human labor, we consider facilitating this selection through simulation optimization, aiming to maximize a pre-defined score for the selected prompt. Specifically, we propose a two-stage framework. In the first stage, we determine a feasible set of prompts in sufficient numbers, where each prompt is represented by a moderate-dimensional vector. In the subsequent stage for evaluation and selection, we construct a surrogate model of the score regarding the moderate-dimensional vectors that represent the prompts. We propose sequentially selecting the prompt for evaluation based on this constructed surrogate model. We prove the consistency of the sequential evaluation procedure in our framework. We also conduct numerical experiments to demonstrate the efficacy of our proposed framework, providing practical instructions for implementation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08164">https://arxiv.org/abs/2404.08164</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper discusses the use of language models for content generation, which is closely related to your interest in utilizing large language models for controlling software and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08509" target="_blank">Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction</a></h3>
            <a href="https://arxiv.org/html/2404.08509v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08509v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Ba\c{s}ar, Ravishankar K. Iyer</p>
            <p><strong>Summary:</strong> arXiv:2404.08509v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving LLM inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing LLM serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of LLMs and enable efficient interactive LLM serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict LLM output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08509">https://arxiv.org/abs/2404.08509</a></p>
            <p><strong>Category:</strong> cs.DC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is primarily about using Large Language Models (LLMs) in the context of AI applications, with a focus on efficiency and performance which relates to controlling software. The paper discusses LLMs and the challenges around implementing them in serving systems, which is relevant to your interest in LLMs for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08517" target="_blank">Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward</a></h3>
            
            <p><strong>Authors:</strong> Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, Lei Ma</p>
            <p><strong>Summary:</strong> arXiv:2404.08517v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have seen widespread applications across numerous fields, their limited interpretability poses concerns regarding their safe operations from multiple aspects, e.g., truthfulness, robustness, and fairness. Recent research has started developing quality assurance methods for LLMs, introducing techniques such as offline detector-based or uncertainty estimation methods. However, these approaches predominantly concentrate on post-generation analysis, leaving the online safety analysis for LLMs during the generation phase an unexplored area. To bridge this gap, we conduct in this work a comprehensive evaluation of the effectiveness of existing online safety analysis methods on LLMs. We begin with a pilot study that validates the feasibility of detecting unsafe outputs in the early generation process. Following this, we establish the first publicly available benchmark of online safety analysis for LLMs, including a broad spectrum of methods, models, tasks, datasets, and evaluation metrics. Utilizing this benchmark, we extensively analyze the performance of state-of-the-art online safety analysis methods on both open-source and closed-source LLMs. This analysis reveals the strengths and weaknesses of individual methods and offers valuable insights into selecting the most appropriate method based on specific application scenarios and task requirements. Furthermore, we also explore the potential of using hybridization methods, i.e., combining multiple methods to derive a collective safety conclusion, to enhance the efficacy of online safety analysis for LLMs. Our findings indicate a promising direction for the development of innovative and trustworthy quality assurance methodologies for LLMs, facilitating their reliable deployments across diverse domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08517">https://arxiv.org/abs/2404.08517</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on safety and quality assurance methods for Large Language Models, which relates to your interest in agents based on large-language models. While it doesn't directly cover controlling software or web browsers, it addresses relevant aspects related to the deployment and reliability of LLMs, a key aspect in their practical application.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08634" target="_blank">Pre-training Small Base LMs with Fewer Tokens</a></h3>
            <a href="https://arxiv.org/html/2404.08634v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08634v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sunny Sanyal, Sujay Sanghavi, Alexandros G. Dimakis</p>
            <p><strong>Summary:</strong> arXiv:2404.08634v1 Announce Type: cross 
Abstract: We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few transformer blocks from the larger LM, and then train this smaller model on a very small subset (0.1\%) of the raw pretraining data of the larger model. We call our simple recipe Inheritune and first demonstrate it for building a small base LM with 1.5B parameters using 1B tokens (and a starting few layers of larger LM of 3B parameters); we do this using a single A6000 GPU for less than half a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark, the resulting model compares favorably to publicly available base models of 1B-2B size, some of which have been trained using 50-1000 times more tokens.
  We investigate Inheritune in a slightly different setting where we train small LMs utilizing larger LMs and their full pre-training dataset. Here we show that smaller LMs trained utilizing some of the layers of GPT2-medium (355M) and GPT-2-large (770M) can effectively match the val loss of their bigger counterparts when trained from scratch for the same number of training steps on OpenWebText dataset with 9B tokens. We analyze our recipe with extensive experiments and demonstrate it efficacy on diverse settings. Our code is available at https://github.com/sanyalsunny111/LLM-Inheritune.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08634">https://arxiv.org/abs/2404.08634</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the concept of using large language models for pre-training small base language models, which could possibly be applied to control software and provide computer automation as per your outlined interests in large-language models controlling systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.00409" target="_blank">Provably Robust DPO: Aligning Language Models with Noisy Feedback</a></h3>
            <a href="https://arxiv.org/html/2403.00409v2/extracted/5531911/Figures/Plots/sampling_temp.png" target="_blank"><img src="https://arxiv.org/html/2403.00409v2/extracted/5531911/Figures/Plots/sampling_temp.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan</p>
            <p><strong>Summary:</strong> arXiv:2403.00409v2 Announce Type: replace 
Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.
  In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset show that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.00409">https://arxiv.org/abs/2403.00409</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is pertinent to your interests because it explores the use of large language models in understanding and adapting to human intent, a vital feature of large language model-based agents. However, it doesn't directly address controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12730" target="_blank">UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation</a></h3>
            <a href="https://arxiv.org/html/2402.12730v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12730v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shubhashis Roy Dipta, Sai Vallurupalli</p>
            <p><strong>Summary:</strong> arXiv:2402.12730v2 Announce Type: replace-cross 
Abstract: The aim of SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages" is to develop models for identifying semantic textual relatedness (STR) between two sentences using multiple languages (14 African and Asian languages) and settings (supervised, unsupervised, and cross-lingual). Large language models (LLMs) have shown impressive performance on several natural language understanding tasks such as multilingual machine translation (MMT), semantic similarity (STS), and encoding sentence embeddings. Using a combination of LLMs that perform well on these tasks, we developed two STR models, $\textit{TranSem}$ and $\textit{FineSem}$, for the supervised and cross-lingual settings. We explore the effectiveness of several training methods and the usefulness of machine translation. We find that direct fine-tuning on the task is comparable to using sentence embeddings and translating to English leads to better performance for some languages. In the supervised setting, our model performance is better than the official baseline for 3 languages with the remaining 4 performing on par. In the cross-lingual setting, our model performance is better than the baseline for 3 languages (leading to $1^{st}$ place for Africaans and $2^{nd}$ place for Indonesian), is on par for 2 languages and performs poorly on the remaining 7 languages. Our code is publicly available at https://github.com/dipta007/SemEval24-Task8.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12730">https://arxiv.org/abs/2402.12730</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the use of large language models (LLMs) in a semantic textual relatedness task involving different languages. It potentially signifies an application of LLMs in controlling software such as language translation. However, it does not directly involve controlling web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.04522" target="_blank">Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.04522v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.04522v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, Yi Fang</p>
            <p><strong>Summary:</strong> arXiv:2404.04522v2 Announce Type: replace-cross 
Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized in Large Language Models (LLMs) to improve the down-streaming tasks without the cost of fine-tuing the whole LLMs. Recent studies have shown how to effectively use PEFT for fine-tuning LLMs in ranking tasks with convincing performance; there are some limitations, including the learned prompt being fixed for different documents, overfitting to specific tasks, and low adaptation ability. In this paper, we introduce a query-dependent parameter efficient fine-tuning (Q-PEFT) approach for text reranking to leak the information of the true queries to LLMs and then make the generation of true queries from input documents much easier. Specifically, we utilize the query to extract the top-$k$ tokens from concatenated documents, serving as contextual clues. We further augment Q-PEFT by substituting the retrieval mechanism with a multi-head attention layer to achieve end-to-end training and cover all the tokens in the documents, guiding the LLMs to generate more document-specific synthetic queries, thereby further improving the reranking performance. Extensive experiments are conducted on four public datasets, demonstrating the effectiveness of our proposed approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.04522">https://arxiv.org/abs/2404.04522</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it focuses on fine-tuning large language models to improve performance, specifically in text reranking, which could potentially be applied to control software systems and web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.06407" target="_blank">Rethinking How to Evaluate Language Model Jailbreak</a></h3>
            
            <p><strong>Authors:</strong> Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik</p>
            <p><strong>Summary:</strong> arXiv:2404.06407v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not. In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.06407">https://arxiv.org/abs/2404.06407</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses large language models, specifically focusing on their safety features and the processes of evaluating their vulnerabilities. Although the paper does not directly mention using models to control software or browsers, or computerautomation, it provides important insights into the design and implementation of safe language models, which is a critical aspect when developing agent-based applications.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08024" target="_blank">The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health</a></h3>
            <a href="https://arxiv.org/html/2404.08024v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08024v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> M. Jaleed Khan, Ioana Duta, Beth Albert, William Cooke, Manu Vatish, Gabriel Davis Jones</p>
            <p><strong>Summary:</strong> arXiv:2404.08024v1 Announce Type: new 
Abstract: The rapid advancement of Artificial Intelligence (AI) in healthcare presents a unique opportunity for advancements in obstetric care, particularly through the analysis of cardiotocography (CTG) for fetal monitoring. However, the effectiveness of such technologies depends upon the availability of large, high-quality datasets that are suitable for machine learning. This paper introduces the Oxford Maternity (OxMat) dataset, the world's largest curated dataset of CTGs, featuring raw time series CTG data and extensive clinical data for both mothers and babies, which is ideally placed for machine learning. The OxMat dataset addresses the critical gap in women's health data by providing over 177,211 unique CTG recordings from 51,036 pregnancies, carefully curated and reviewed since 1991. The dataset also comprises over 200 antepartum, intrapartum and postpartum clinical variables, ensuring near-complete data for crucial outcomes such as stillbirth and acidaemia. While this dataset also covers the intrapartum stage, around 94% of the constituent CTGS are antepartum. This allows for a unique focus on the underserved antepartum period, in which early detection of at-risk fetuses can significantly improve health outcomes. Our comprehensive review of existing datasets reveals the limitations of current datasets: primarily, their lack of sufficient volume, detailed clinical data and antepartum data. The OxMat dataset lays a foundation for future AI-driven prenatal care, offering a robust resource for developing and testing algorithms aimed at improving maternal and fetal health outcomes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08024">https://arxiv.org/abs/2404.08024</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'time-series' subtopic 'Datasets to train foundation models for time series'. It presents the OxMat dataset, a unique and large multimodal dataset consisting of time series data, with comprehensive clinical variables. It is an important resource for developing deep learning methods focused on time series applications in obstetric care.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08472" target="_blank">TSLANet: Rethinking Transformers for Time Series Representation Learning</a></h3>
            <a href="https://arxiv.org/html/2404.08472v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08472v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li</p>
            <p><strong>Summary:</strong> arXiv:2404.08472v1 Announce Type: new 
Abstract: Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal convolutional model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature representation and to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive Convolution Block and leverage self-supervised learning to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and anomaly detection, showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at \url{https://github.com/emadeldeen24/TSLANet}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08472">https://arxiv.org/abs/2404.08472</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it presents a new Transformer-based model, TSLANet, which is designed for handling time series data. While it doesn't specifically discuss multimodal model or foundation models, its novelty lies in the proposed new Adaptive Spectral Block and Interactive Convolution Block that enhance feature representation and robustness in time series tasks, which include forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08601" target="_blank">Generating Synthetic Time Series Data for Cyber-Physical Systems</a></h3>
            <a href="https://arxiv.org/html/2404.08601v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08601v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Sommers, Somayeh Bakhtiari Ramezani, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure</p>
            <p><strong>Summary:</strong> arXiv:2404.08601v1 Announce Type: new 
Abstract: Data augmentation is an important facilitator of deep learning applications in the time series domain. A gap is identified in the literature, demonstrating sparse exploration of the transformer, the dominant sequence model, for data augmentation in time series. A architecture hybridizing several successful priors is put forth and tested using a powerful time domain similarity metric. Results suggest the challenge of this domain, and several valuable directions for future work.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08601">https://arxiv.org/abs/2404.08601</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper explains a new method using transformer models for data augmentation in time series, which aligns with your interest in 'transformer-like models for time series' and 'new deep learning methods for time series'</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.12544" target="_blank">Neural Likelihood Approximation for Integer Valued Time Series Data</a></h3>
            <a href="https://arxiv.org/html/2310.12544v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.12544v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Luke O'Loughlin, John Maclean, Andrew Black</p>
            <p><strong>Summary:</strong> arXiv:2310.12544v2 Announce Type: replace-cross 
Abstract: Stochastic processes defined on integer valued state spaces are popular within the physical and biological sciences. These models are necessary for capturing the dynamics of small systems where the individual nature of the populations cannot be ignored and stochastic effects are important. The inference of the parameters of such models, from time series data, is challenging due to intractability of the likelihood. To work at all, current simulation based inference methods require the generation of realisations of the model conditional on the data, which can be both tricky to implement and computationally expensive. In this paper we instead construct a neural likelihood approximation that can be trained using unconditional simulation of the underlying model, which is much simpler. We demonstrate our method by performing inference on a number of ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving significant computational speed ups compared to current best methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12544">https://arxiv.org/abs/2310.12544</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Though it's not explicitly about deep learning, this paper proposes a new method using neural likelihood approximation for integer-valued time series data, which falls under new methods for time series. It discusses ecological and epidemiological models which may give you ideas for new datasets to train foundation models for time series.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2206.08952" target="_blank">The Impact of Variable Ordering on Bayesian Network Structure Learning</a></h3>
            <a href="https://arxiv.org/html/2206.08952v2/extracted/5533405/ord_mec_examples.png" target="_blank"><img src="https://arxiv.org/html/2206.08952v2/extracted/5533405/ord_mec_examples.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Neville K Kitson, Anthony C Constantinou</p>
            <p><strong>Summary:</strong> arXiv:2206.08952v2 Announce Type: replace 
Abstract: Causal Bayesian Networks provide an important tool for reasoning under uncertainty with potential application to many complex causal systems. Structure learning algorithms that can tell us something about the causal structure of these systems are becoming increasingly important. In the literature, the validity of these algorithms is often tested for sensitivity over varying sample sizes, hyper-parameters, and occasionally objective functions. In this paper, we show that the order in which the variables are read from data can have much greater impact on the accuracy of the algorithm than these factors. Because the variable ordering is arbitrary, any significant effect it has on learnt graph accuracy is concerning, and this raises questions about the validity of the results produced by algorithms that are sensitive to, but have not been assessed against, different variable orderings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2206.08952">https://arxiv.org/abs/2206.08952</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses Bayesian Networks, a structure for reasoning under uncertainty which has potential application in complex causal systems. It's therefore relevant for your interest in causality and machine learning, specifically for causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.11321" target="_blank">Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</a></h3>
            
            <p><strong>Authors:</strong> Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</p>
            <p><strong>Summary:</strong> arXiv:2311.11321v3 Announce Type: replace-cross 
Abstract: State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic refutation framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATE is non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose a neural refutation framework which performs partial identification of CATE or, equivalently, aims at estimating lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our refutation framework is of direct relevance in practice where the validity of CATE estimation is of importance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.11321">https://arxiv.org/abs/2311.11321</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper delves into causal representation learning and presents a new framework for estimating bounds on the representation-induced confounding bias in treatment effect estimation. It might be of interest given your focus on causal representation learning and new methods in causality.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 15, 2024 at 21:33:24</div></body></html>