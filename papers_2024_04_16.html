
            <html>
            <head>
                <title>Report Generated on April 16, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 16, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09249" target="_blank">Test Code Generation for Telecom Software Systems using Two-Stage Generative Model</a></h3>
            <a href="https://arxiv.org/html/2404.09249v1/extracted/5535559/sections/Figures/prompt_example.jpeg" target="_blank"><img src="https://arxiv.org/html/2404.09249v1/extracted/5535559/sections/Figures/prompt_example.jpeg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda</p>
            <p><strong>Summary:</strong> arXiv:2404.09249v1 Announce Type: cross 
Abstract: In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09249">https://arxiv.org/abs/2404.09249</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the application is in the field of telecommunication networks, the proposed framework features the use of a large language model for test script generation, which falls under the umbrella of using large language models to control software, a subtopic of llm-agents that you are interested in. Furthermore, the application involves a degree of automation, aligning with your interest in automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08707" target="_blank">Large Language Model Can Continue Evolving From Mistakes</a></h3>
            <a href="https://arxiv.org/html/2404.08707v1/extracted/5531216/acl-style-files-master/imgs/in.png" target="_blank"><img src="https://arxiv.org/html/2404.08707v1/extracted/5531216/acl-style-files-master/imgs/in.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haokun Zhao, Haixia Han, Jie Shi, Chengyu Du, Jiaqing Liang, Yanghua Xiao</p>
            <p><strong>Summary:</strong> arXiv:2404.08707v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive performance in various downstream tasks. However, they may still generate incorrect responses in certain scenarios due to the knowledge deficiencies and the flawed pre-training data. Continual Learning (CL) is a commonly used method to address this issue. Traditional CL is task-oriented, using novel or factually accurate data to retrain LLMs from scratch. However, this method requires more task-related training data and incurs expensive training costs. To address this challenge, we propose the Continue Evolving from Mistakes (CEM) method, inspired by the 'summarize mistakes' learning skill, to achieve iterative refinement of LLMs. Specifically, the incorrect responses of LLMs indicate knowledge deficiencies related to the questions. Therefore, we collect corpora with these knowledge from multiple data sources and follow it up with iterative supplementary training for continuous, targeted knowledge updating and supplementation. Meanwhile, we developed two strategies to construct supplementary training sets to enhance the LLM's understanding of the corpus and prevent catastrophic forgetting. We conducted extensive experiments to validate the effectiveness of this CL method. In the best case, our method resulted in a 17.00\% improvement in the accuracy of the LLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08707">https://arxiv.org/abs/2404.08707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in improving large language models for automation purposes. It proposes a new method for the continuous updating and enhancement of an LLM based on its mistakes. The continual learning method introduced here could be applicable in your scenario for controlling software and browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08755" target="_blank">Training a Vision Language Model as Smartphone Assistant</a></h3>
            <a href="https://arxiv.org/html/2404.08755v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08755v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nicolai Dorka, Janusz Marecki, Ammar Anwar</p>
            <p><strong>Summary:</strong> arXiv:2404.08755v1 Announce Type: new 
Abstract: Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08755">https://arxiv.org/abs/2404.08755</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses advancements in large language models, focusing on mobile device control which aligns with your interest in computer automation using large language models. While not explicitly about a new method, it presents a novel application of these models in a practical scenario.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08763" target="_blank">CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.08763v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08763v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia Mirhoseini</p>
            <p><strong>Summary:</strong> arXiv:2404.08763v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of LLMs by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base LLMs and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including Mistral-7B and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any fine-tuning and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when fine-tuning is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both Llama-7B and Mistral-7B.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08763">https://arxiv.org/abs/2404.08763</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though the primary focus of this paper isn't using large language models to control software or web browsers, it relates to your interest in large language model agents. Specifically, it discusses methods for reducing the inference costs of large language models, which could have implications for using these models in the control of software or browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08801" target="_blank">Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</a></h3>
            <a href="https://arxiv.org/html/2404.08801v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08801v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou</p>
            <p><strong>Summary:</strong> arXiv:2404.08801v1 Announce Type: new 
Abstract: The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: https://github.com/XuezheMax/megalodon</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08801">https://arxiv.org/abs/2404.08801</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not deal specifically with using large language models for controlling software or web browsers. It does discuss efficient sequence modeling with unlimited context length using Megalodon, a new neural architecture, which could be relevant for developing intelligent agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08819" target="_blank">The Illusion of State in State-Space Models</a></h3>
            <a href="https://arxiv.org/html/2404.08819v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08819v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William Merrill, Jackson Petty, Ashish Sabharwal</p>
            <p><strong>Summary:</strong> arXiv:2404.08819v1 Announce Type: new 
Abstract: State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill and Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08819">https://arxiv.org/abs/2404.08819</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly related to 'Agents based on large language models' as it discusses the strengths and limitations of large language models, especially about state tracking, which is tightly connected to controlling software and computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08846" target="_blank">Experimental Design for Active Transductive Inference in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.08846v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08846v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Subhojyoti Mukherjee, Ge Liu, Aniket Deshmukh, Anusha Lalitha, Yifei Ma, Branislav Kveton</p>
            <p><strong>Summary:</strong> arXiv:2404.08846v1 Announce Type: new 
Abstract: Transduction, the ability to include query-specific examples in the prompt at inference time, is one of the emergent abilities of large language models (LLMs). In this work, we propose a framework for adaptive prompt design called active transductive inference (ATI). We design the LLM prompt by adaptively choosing few-shot examples for a given inference query. The examples are initially unlabeled and we query the user to label the most informative ones, which maximally reduces the uncertainty in the LLM prediction. We propose two algorithms, GO and SAL, which differ in how the few-shot examples are chosen. We analyze these algorithms in linear models: first GO and then use its equivalence with SAL. We experiment with many different tasks and show that GO and SAL outperform other methods for choosing few-shot examples in the LLM prompt at inference time.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08846">https://arxiv.org/abs/2404.08846</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models (LLMs) for intelligent task handling and adaptive prompts, which can be seen as a method of controlling software using LLMs. However, it does not specifically mention controlling web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09022" target="_blank">Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies</a></h3>
            <a href="https://arxiv.org/html/2404.09022v1/extracted/5534679/figs/finetuning-process.png" target="_blank"><img src="https://arxiv.org/html/2404.09022v1/extracted/5534679/figs/finetuning-process.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Benjue Weng</p>
            <p><strong>Summary:</strong> arXiv:2404.09022v1 Announce Type: new 
Abstract: With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09022">https://arxiv.org/abs/2404.09022</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper delves into the intricacies of fine-tuning large language models, which is vital to your interest in controlling software and automating computer processes using these models. However, as it does not explicitly describe using these models for control of web browsers or software, a perfect score was not assigned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09173" target="_blank">TransformerFAM: Feedback attention is working memory</a></h3>
            <a href="https://arxiv.org/html/2404.09173v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09173v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, Pedro Moreno Mengibar</p>
            <p><strong>Summary:</strong> arXiv:2404.09173v1 Announce Type: new 
Abstract: While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09173">https://arxiv.org/abs/2404.09173</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses an innovative approach to enhancing the performance of Large Language Models (LLMs). While not directly linked to agents control, the proposed method would likely be useful for scenarios involving agents based on LLMs, such as processing extremely long sequences which can be crucial for web browsing or software control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09248" target="_blank">Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts</a></h3>
            <a href="https://arxiv.org/html/2404.09248v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09248v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, Yang Yu</p>
            <p><strong>Summary:</strong> arXiv:2404.09248v1 Announce Type: new 
Abstract: Reinforcement learning (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data. To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from large language models (LLMs). Despite previous studies combining LLMs with RL, seamless integration of the two components remains challenging due to their semantic gap. This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods. The primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to LLMs. To address this, KALM fine-tunes the LLM to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data. This grounding process enhances the LLM's comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors. KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods. Furthermore, KALM effectively enables the LLM to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of large language models and reinforcement learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09248">https://arxiv.org/abs/2404.09248</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it introduces KALM, a method that integrates reinforcement learning and large language models, which aligns with your interest in agents based on large-language models. However, it does not specifically mention control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09491" target="_blank">Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning</a></h3>
            <a href="https://arxiv.org/html/2404.09491v1/extracted/5536365/figures/model_figure2.jpg" target="_blank"><img src="https://arxiv.org/html/2404.09491v1/extracted/5536365/figures/model_figure2.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sungwon Han, Jinsung Yoon, Sercan O Arik, Tomas Pfister</p>
            <p><strong>Summary:</strong> arXiv:2404.09491v1 Announce Type: new 
Abstract: Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09491">https://arxiv.org/abs/2404.09491</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the use of large language models as feature engineers and how this can be optimally applied to tabular predictions. This method can be seen as a form of controlling software, one of your areas of interest in the field of agents based on large-language models. Even though this paper is not exactly about automation or web browsing, it offers an innovative example of how large language models can be used.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09529" target="_blank">Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.09529v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09529v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya Grover</p>
            <p><strong>Summary:</strong> arXiv:2404.09529v1 Announce Type: new 
Abstract: During inference for transformer-based large language models (LLM), prefilling is the computation of the key-value (KV) cache for input tokens in the prompt prior to autoregressive generation. For longer input prompt lengths, prefilling will incur a significant overhead on decoding time. In this work, we highlight the following pitfall of prefilling: for batches containing high-varying prompt lengths, significant computation is wasted by the standard practice of padding sequences to the maximum length. As LLMs increasingly support longer context lengths, potentially up to 10 million tokens, variations in prompt lengths within a batch become more pronounced. To address this, we propose Prepacking, a simple yet effective method to optimize prefilling computation. To avoid redundant computation on pad tokens, prepacking combines prompts of varying lengths into a sequence and packs multiple sequences into a compact batch using a bin-packing algorithm. It then modifies the attention mask and positional encoding to compute multiple prefilled KV-caches for multiple prompts within a single sequence. On standard curated dataset containing prompts with varying lengths, we obtain a significant speed and memory efficiency improvements as compared to the default padding-based prefilling computation within Huggingface across a range of base model configurations and inference serving scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09529">https://arxiv.org/abs/2404.09529</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not specifically address using large language models to control software or web browsers, it does discuss the implementation of large language models in a more efficient manner which could further aid in the process of automating tasks. Furthermore, the increased efficiency addressed in the paper could be especially important when it comes to usage for software control or web browsing.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09562" target="_blank">{\sigma}-GPTs: A New Approach to Autoregressive Models</a></h3>
            <a href="https://arxiv.org/html/2404.09562v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09562v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Arnaud Pannatier, Evann Courdier, Fran\c{c}ois Fleuret</p>
            <p><strong>Summary:</strong> arXiv:2404.09562v1 Announce Type: new 
Abstract: Autoregressive models, such as the GPT family, use a fixed order, usually left-to-right, to generate sequences. However, this is not a necessity. In this paper, we challenge this assumption and show that by simply adding a positional encoding for the output, this order can be modulated on-the-fly per-sample which offers key advantageous properties. It allows for the sampling of and conditioning on arbitrary subsets of tokens, and it also allows sampling in one shot multiple tokens dynamically according to a rejection strategy, leading to a sub-linear number of model evaluations. We evaluate our method across various domains, including language modeling, path-solving, and aircraft vertical rate prediction, decreasing the number of steps required for generation by an order of magnitude.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09562">https://arxiv.org/abs/2404.09562</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although not explicitly stated, this paper introduces a new approach to autoregressive models that can be beneficial for large language models controlling software applications and web browsing, which fall in line with your interests in large-language model-based agents. However, it does not explicitly address all your specific subtopics within this domain.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09606" target="_blank">A Self-feedback Knowledge Elicitation Approach for Chemical Reaction Predictions</a></h3>
            <a href="https://arxiv.org/html/2404.09606v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09606v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pengfei Liu, Jun Tao, Zhixiang Ren</p>
            <p><strong>Summary:</strong> arXiv:2404.09606v1 Announce Type: new 
Abstract: The task of chemical reaction predictions (CRPs) plays a pivotal role in advancing drug discovery and material science. However, its effectiveness is constrained by the vast and uncertain chemical reaction space and challenges in capturing reaction selectivity, particularly due to existing methods' limitations in exploiting the data's inherent knowledge. To address these challenges, we introduce a data-curated self-feedback knowledge elicitation approach. This method starts from iterative optimization of molecular representations and facilitates the extraction of knowledge on chemical reaction types (RTs). Then, we employ adaptive prompt learning to infuse the prior knowledge into the large language model (LLM). As a result, we achieve significant enhancements: a 14.2% increase in retrosynthesis prediction accuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the model's capability for handling multi-task chemical reactions. This research offers a novel paradigm for knowledge elicitation in scientific research and showcases the untapped potential of LLMs in CRPs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09606">https://arxiv.org/abs/2404.09606</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While not directly corresponding with your exact subtopics, this paper's innovative use of large language models for knowledge extraction and enhancement of chemical reaction predictions denotes a relevant application that could provide valuable insights for agent-based utilisation of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09656" target="_blank">Learn Your Reference Model for Real Good Alignment</a></h3>
            <a href="https://arxiv.org/html/2404.09656v1/extracted/5537057/images/shushing-face_1f92b.png" target="_blank"><img src="https://arxiv.org/html/2404.09656v1/extracted/5537057/images/shushing-face_1f92b.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov</p>
            <p><strong>Summary:</strong> arXiv:2404.09656v1 Announce Type: new 
Abstract: The complexity of the alignment problem stems from the fact that existing methods are unstable. Researchers continuously invent various tricks to address this shortcoming. For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized. This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM. The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy. In our paper, we argue that this implicit limitation in the DPO method leads to sub-optimal results. We propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training. With such a straightforward update, we demonstrate the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09656">https://arxiv.org/abs/2404.09656</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the alignment problem in large language models, a field of your interest. it proposes a new method, Trust Region DPO (TR-DPO), for the policy of large language models, but it doesn't exactly talk about controlling software or web browsers with these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09932" target="_blank">Foundational Challenges in Assuring Alignment and Safety of Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario G\"unther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se\'an \'O h\'Eigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, David Krueger</p>
            <p><strong>Summary:</strong> arXiv:2404.09932v1 Announce Type: new 
Abstract: This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09932">https://arxiv.org/abs/2404.09932</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant since it focuses on ensuring the alignment and safety of large language models (LLMs), which is crucial when these models are used to control software or for computer automation. However, it doesn't directly address the application of LLMs in controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08676" target="_blank">ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming</a></h3>
            <a href="https://arxiv.org/html/2404.08676v1/extracted/5516588/img/alert2.png" target="_blank"><img src="https://arxiv.org/html/2404.08676v1/extracted/5516588/img/alert2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li</p>
            <p><strong>Summary:</strong> arXiv:2404.08676v1 Announce Type: cross 
Abstract: When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08676">https://arxiv.org/abs/2404.08676</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it doesn't directly discuss using large language models to control software or browsers, the paper is relevant as it discusses safety aspects of large language models. This safety concern is an important consideration when using large language models for controlling software and automation. It can provide insight on potential risks and necessary precautions.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08679" target="_blank">Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector</a></h3>
            <a href="https://arxiv.org/html/2404.08679v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08679v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andi Zhang, Tim Z. Xiao, Weiyang Liu, Robert Bamler, Damon Wischik</p>
            <p><strong>Summary:</strong> arXiv:2404.08679v1 Announce Type: cross 
Abstract: We revisit the likelihood ratio between a pretrained large language model (LLM) and its finetuned variant as a criterion for out-of-distribution (OOD) detection. The intuition behind such a criterion is that, the pretrained LLM has the prior knowledge about OOD data due to its large amount of training data, and once finetuned with the in-distribution data, the LLM has sufficient knowledge to distinguish their difference. Leveraging the power of LLMs, we show that, for the first time, the likelihood ratio can serve as an effective OOD detector. Moreover, we apply the proposed LLM-based likelihood ratio to detect OOD questions in question-answering (QA) systems, which can be used to improve the performance of specialized LLMs for general questions. Given that likelihood can be easily obtained by the loss functions within contemporary neural network frameworks, it is straightforward to implement this approach in practice. Since both the pretrained LLMs and its various finetuned models are available, our proposed criterion can be effortlessly incorporated for OOD detection without the need for further training. We conduct comprehensive evaluation across on multiple settings, including far OOD, near OOD, spam detection, and QA scenarios, to demonstrate the effectiveness of the method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08679">https://arxiv.org/abs/2404.08679</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not directly discuss controlling software or web browsers, it talks about leveraging large language models for Out-of-distribution detection which can be useful in the context of agent capabilities. Plus, it discusses the potential use of LLM in question-answering systems, a form of software control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08698" target="_blank">Lossless Acceleration of Large Language Model via Adaptive N-gram Parallel Decoding</a></h3>
            <a href="https://arxiv.org/html/2404.08698v1/extracted/5528821/tokenswords2.png" target="_blank"><img src="https://arxiv.org/html/2404.08698v1/extracted/5528821/tokenswords2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jie Ou, Yueming Chen, Wenhong Tian</p>
            <p><strong>Summary:</strong> arXiv:2404.08698v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have shown remarkable abilities, they are hindered by significant resource consumption and considerable latency due to autoregressive processing. In this study, we introduce Adaptive N-gram Parallel Decoding (ANPD), an innovative and lossless approach that accelerates inference by allowing the simultaneous generation of multiple tokens. ANPD incorporates a two-stage approach: it begins with a rapid drafting phase that employs an N-gram module, which adapts based on the current interactive context, followed by a verification phase, during which the original LLM assesses and confirms the proposed tokens. Consequently, ANPD preserves the integrity of the LLM's original output while enhancing processing speed. We further leverage a multi-level architecture for the N-gram module to enhance the precision of the initial draft, consequently reducing inference latency. ANPD eliminates the need for retraining or extra GPU memory, making it an efficient and plug-and-play enhancement. In our experiments, models such as LLaMA and its fine-tuned variants have shown speed improvements up to 3.67x, validating the effectiveness of our proposed ANPD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08698">https://arxiv.org/abs/2404.08698</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it explores advancements in the use of large language models (LLMs), specifically focusing on accelerating the speed of LLMs. Although it doesn't directly discuss LLM agent control of software or browsers, the increased speed of LLMs through their suggested methodology of Adaptive N-gram Parallel Decoding could contribute to better performance in those domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08767" target="_blank">LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning</a></h3>
            <a href="https://arxiv.org/html/2404.08767v1/extracted/5528285/images/teaser_final_small.drawio.png" target="_blank"><img src="https://arxiv.org/html/2404.08767v1/extracted/5528285/images/teaser_final_small.drawio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Junchi Wang, Lei Ke</p>
            <p><strong>Summary:</strong> arXiv:2404.08767v1 Announce Type: cross 
Abstract: Understanding human instructions to identify the target objects is vital for perception systems. In recent years, the advancements of Large Language Models (LLMs) have introduced new possibilities for image segmentation. In this work, we delve into reasoning segmentation, a novel task that enables segmentation system to reason and interpret implicit user intention via large language model reasoning and then segment the corresponding target. Our work on reasoning segmentation contributes on both the methodological design and dataset labeling. For the model, we propose a new framework named LLM-Seg. LLM-Seg effectively connects the current foundational Segmentation Anything Model and the LLM by mask proposals selection. For the dataset, we propose an automatic data generation pipeline and construct a new reasoning segmentation dataset named LLM-Seg40K. Experiments demonstrate that our LLM-Seg exhibits competitive performance compared with existing methods. Furthermore, our proposed pipeline can efficiently produce high-quality reasoning segmentation datasets. The LLM-Seg40K dataset, developed through this pipeline, serves as a new benchmark for training and evaluating various reasoning segmentation approaches. Our code, models and dataset are at https://github.com/wangjunchi/LLMSeg.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08767">https://arxiv.org/abs/2404.08767</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it does not directly relate to controlling software or web browsers, the paper discusses a new methodology that connects image segmentation models with Large Language Models. It proposes a novel task of 'reasoning segmentation' where an LLM interprets implicit user intentions. This could potentially be operated as part of an agent system based on Large Language Models in the future, hence its relevance.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08856" target="_blank">On Speculative Decoding for Multimodal Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.08856v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08856v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott</p>
            <p><strong>Summary:</strong> arXiv:2404.08856v1 Announce Type: cross 
Abstract: Inference with Multimodal Large Language Models (MLLMs) is slow due to their large-language-model backbone which suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37$\times$ using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08856">https://arxiv.org/abs/2404.08856</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models. Specifically, it uses a large language model (LLaVA 7B) and explores its application in multimodal settings. Although it does not explicitly mention software or web browser control, it might still contain useful ideas for improving the efficiency of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08860" target="_blank">Improving Technical "How-to" Query Accuracy with Automated Search Results Verification and Reranking</a></h3>
            <a href="https://arxiv.org/html/2404.08860v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08860v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lei Ding, Jeshwanth Bheemanpally, Yi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.08860v1 Announce Type: cross 
Abstract: Many people use search engines to find online guidance to solve computer or mobile device problems. Users frequently encounter challenges in identifying effective solutions from search results, often wasting time trying ineffective solutions that seem relevant yet fail to solve the real problems. This paper introduces a novel approach to improving the accuracy and relevance of online technical support search results through automated search results verification and reranking. Taking "How-to" queries specific to on-device execution as a starting point, we first developed a solution that allows an AI agent to interpret and execute step-by-step instructions in the search results in a controlled Android environment. We further integrated the agent's findings into a reranking mechanism that orders search results based on the success indicators of the tested solutions.
  The paper details the architecture of our solution and a comprehensive evaluation of the system through a series of tests across various application domains. The results demonstrate a significant improvement in the quality and reliability of the top-ranked results. Our findings suggest a paradigm shift in how search engine ranking for online technical support help can be optimized, offering a scalable and automated solution to the pervasive challenge of finding effective and reliable online help.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08860">https://arxiv.org/abs/2404.08860</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not directly discuss new methods proposed in controlling software or web browsers with large language models, it introduces an AI agent that uses step-by-step instructions from search results to verify and rerank these results. This is highly relevant to your interest in automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08865" target="_blank">LLM In-Context Recall is Prompt Dependent</a></h3>
            <a href="https://arxiv.org/html/2404.08865v1/extracted/5533812/san_fancisco/gpt-4-0125-preview_sf_inline.png" target="_blank"><img src="https://arxiv.org/html/2404.08865v1/extracted/5533812/san_fancisco/gpt-4-0125-preview_sf_inline.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daniel Machlab, Rick Battle</p>
            <p><strong>Summary:</strong> arXiv:2404.08865v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given prompt. A model's ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications.
  Our research analyzes the in-context recall performance of various LLMs using the needle-in-a-haystack method. In this approach, a factoid (the "needle") is embedded within a block of filler text (the "haystack"), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an LLM's recall capability is not only contingent upon the prompt's content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or fine-tuning can improve performance. Our analysis provides insight into LLM behavior, offering direction for the development of more effective applications of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08865">https://arxiv.org/abs/2404.08865</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is about Large Language Models (LLMs) and their recall capabilities, which is important for controlling software using language models. However, it doesn't explicitly mention software or browser control, hence the score is not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08877" target="_blank">Aligning LLMs for FL-free Program Repair</a></h3>
            <a href="https://arxiv.org/html/2404.08877v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08877v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He</p>
            <p><strong>Summary:</strong> arXiv:2404.08877v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs are capable of locating and repairing bugs end-to-end when using the related artifacts (e.g., test cases) as input, existing methods regard them as separate tasks and ask LLMs to generate patches at fixed locations. This restriction hinders LLMs from exploring potential patches beyond the given locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first performing fault localization. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08877">https://arxiv.org/abs/2404.08877</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it doesn't explicitly discuss utilization of Large Language Models (LLMs) to control software or web browsers, it delves into the topic of improving LLM's performance in program repair, which can be seen as a form of software control. New methods are proposed, aligning with the interest for novel methodologies.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08878" target="_blank">Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision</a></h3>
            <a href="https://arxiv.org/html/2404.08878v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08878v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhe Wang, Jiayi Zhang, Hongyang Du, Ruichen Zhang, Dusit Niyato, Bo Ai, Khaled B. Letaief</p>
            <p><strong>Summary:</strong> arXiv:2404.08878v1 Announce Type: cross 
Abstract: Next-generation multiple input multiple output (MIMO) is expected to be intelligent and scalable. In this paper, we study generative artificial intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the generative AI agent, which is capable of generating tailored and specialized contents with the aid of large language model (LLM) and retrieval augmented generation (RAG). Next, we comprehensively discuss the features and advantages of the generative AI agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss generative AI agent-enabled next-generation MIMO design, from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the generative AI agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of generative AI agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08878">https://arxiv.org/abs/2404.08878</a></p>
            <p><strong>Category:</strong> cs.NI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses using a large language model (LLM) in the context of designing next-generation multiple input multiple output (MIMO) systems. Although it focuses more on performance analysis than automation, it provides useful insights regarding how LLM can be used for designing complex systems and tailoring specialized contents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08885" target="_blank">Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension</a></h3>
            <a href="https://arxiv.org/html/2404.08885v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08885v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mengnan Qi, Yufan Huang, Yongqiang Yao, Maoquan Wang, Bin Gu, Neel Sundaresan</p>
            <p><strong>Summary:</strong> arXiv:2404.08885v1 Announce Type: cross 
Abstract: Large language models (LLMs) has experienced exponential growth, they demonstrate remarkable performance across various tasks. Notwithstanding, contemporary research primarily centers on enhancing the size and quality of pretraining data, still utilizing the next token prediction task on autoregressive transformer model structure. The efficacy of this task in truly facilitating the model's comprehension of code logic remains questionable, we speculate that it still interprets code as mere text, while human emphasizes the underlying logical knowledge. In order to prove it, we introduce a new task, "Logically Equivalent Code Selection," which necessitates the selection of logically equivalent code from a candidate set, given a query code. Our experimental findings indicate that current LLMs underperform in this task, since they understand code by unordered bag of keywords. To ameliorate their performance, we propose an advanced pretraining task, "Next Token Prediction+". This task aims to modify the sentence embedding distribution of the LLM without sacrificing its generative capabilities. Our experimental results reveal that following this pretraining, both Code Llama and StarCoder, the prevalent code domain pretraining models, display significant improvements on our logically equivalent code selection task and the code completion task.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08885">https://arxiv.org/abs/2404.08885</a></p>
            <p><strong>Category:</strong> cs.PL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper neither directly discusses controlling software or web browsers with large language models nor refers to computer automation with these models, it does imply the limitations and possible improvement strategies for large language models, especially in understanding code logic. This could be related to and beneficial for making automation tasks more efficient and reliable when using language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08886" target="_blank">EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM</a></h3>
            <a href="https://arxiv.org/html/2404.08886v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08886v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Henry Peng Zou, Gavin Heqing Yu, Ziwei Fan, Dan Bu, Han Liu, Peng Dai, Dongmei Jia, Cornelia Caragea</p>
            <p><strong>Summary:</strong> arXiv:2404.08886v1 Announce Type: cross 
Abstract: In e-commerce, accurately extracting product attribute values from multimodal data is crucial for improving user experience and operational efficiency of retailers. However, previous approaches to multimodal attribute value extraction often struggle with implicit attribute values embedded in images or text, rely heavily on extensive labeled data, and can easily confuse similar attribute values. To address these issues, we introduce EIVEN, a data- and parameter-efficient generative framework that pioneers the use of multimodal LLM for implicit attribute value extraction. EIVEN leverages the rich inherent knowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled data. We also introduce a novel Learning-by-Comparison technique to reduce model confusion by enforcing attribute value comparison and difference identification. Additionally, we construct initial open-source datasets for multimodal implicit attribute value extraction. Our extensive experiments reveal that EIVEN significantly outperforms existing methods in extracting implicit attribute values while requiring less labeled data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08886">https://arxiv.org/abs/2404.08886</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduced 'EIVEN', a framework that leverages large language models (LLMs) and visual encoders for attribute value extraction from multimodal data. This directly ties into your interest in agents based on large language models. It's rated 4 and not 5 because the paper focuses mostly on e-commerce instead of controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08940" target="_blank">Introducing Super RAGs in Mistral 8x7B-v1</a></h3>
            <a href="https://arxiv.org/html/2404.08940v1/extracted/5534386/working.png" target="_blank"><img src="https://arxiv.org/html/2404.08940v1/extracted/5534386/working.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ayush Thakur, Raghav Gupta</p>
            <p><strong>Summary:</strong> arXiv:2404.08940v1 Announce Type: cross 
Abstract: The relentless pursuit of enhancing Large Language Models (LLMs) has led to the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel approach designed to elevate the performance of LLMs by integrating external knowledge sources with minimal structural modifications. This paper presents the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM, and examines the resultant improvements in accuracy, speed, and user satisfaction. Our methodology uses a fine-tuned instruct model setup and a cache tuning fork system, ensuring efficient and relevant data retrieval. The evaluation, conducted over several epochs, demonstrates significant enhancements across all metrics. The findings suggest that Super RAGs can effectively augment LLMs, paving the way for more sophisticated and reliable AI systems. This research contributes to the field by providing empirical evidence of the benefits of Super RAGs and offering insights into their potential applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08940">https://arxiv.org/abs/2404.08940</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the integration of Super Retrieval-Augmented Generation (Super RAGs) into large language models, which can be categorized under agent-based LLMs. Although it doesn't specifically discuss controlling software or web browsers, it contributes to the understanding of enhancing the performance of LLMs that might lead to better automation applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09066" target="_blank">CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants</a></h3>
            <a href="https://arxiv.org/html/2404.09066v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09066v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Amit Finkman, Eden Bar-Kochva, Avishag Shapira, Dudu Mimran, Yuval Elovici, Asaf Shabtai</p>
            <p><strong>Summary:</strong> arXiv:2404.09066v1 Announce Type: cross 
Abstract: LLM-based code assistants are becoming increasingly popular among developers. These tools help developers improve their coding efficiency and reduce errors by providing real-time suggestions based on the developer's codebase. While beneficial, these tools might inadvertently expose the developer's proprietary code to the code assistant service provider during the development process. In this work, we propose two complementary methods to mitigate the risk of code leakage when using LLM-based code assistants. The first is a technique for reconstructing a developer's original codebase from code segments sent to the code assistant service (i.e., prompts) during the development process, enabling assessment and evaluation of the extent of code leakage to third parties (or adversaries). The second is CodeCloak, a novel deep reinforcement learning agent that manipulates the prompts before sending them to the code assistant service. CodeCloak aims to achieve the following two contradictory goals: (i) minimizing code leakage, while (ii) preserving relevant and useful suggestions for the developer. Our evaluation, employing GitHub Copilot, StarCoder, and CodeLlama LLM-based code assistants models, demonstrates the effectiveness of our CodeCloak approach on a diverse set of code repositories of varying sizes, as well as its transferability across different models. In addition, we generate a realistic simulated coding environment to thoroughly analyze code leakage risks and evaluate the effectiveness of our proposed mitigation techniques under practical development scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09066">https://arxiv.org/abs/2404.09066</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While not directly fitting into the subtopics you mentioned, this paper aligns with your interest in large-language model agents. It specifically discusses a deep reinforcement learning agent, CodeCloak, designed to manipulate prompts sent to LLM-based code assistants - a relevant concept when considering automation and control using such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09077" target="_blank">CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting</a></h3>
            <a href="https://arxiv.org/html/2404.09077v1/extracted/5534945/images/questions.png" target="_blank"><img src="https://arxiv.org/html/2404.09077v1/extracted/5534945/images/questions.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zukang Yang, Zixuan Zhu</p>
            <p><strong>Summary:</strong> arXiv:2404.09077v1 Announce Type: cross 
Abstract: In the field of Question Answering (QA), unifying large language models (LLMs) with external databases has shown great success. However, these methods often fall short in providing the advanced reasoning needed for complex QA tasks. To address these issues, we improve over a novel approach called Knowledge Graph Prompting (KGP), which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy. Nevertheless, the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination. Therefore, we propose a reasoning-infused LLM agent to enhance this framework. This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search. This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the QA domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09077">https://arxiv.org/abs/2404.09077</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper doesn't implicitly discuss using large language models to control software or web browsers, it does explore the concept of enhancing large language models with external databases for improved performance in complex tasks such as QA. This concept might be relevant and beneficial for developing tools to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09134" target="_blank">Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission</a></h3>
            <a href="https://arxiv.org/html/2404.09134v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09134v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim</p>
            <p><strong>Summary:</strong> arXiv:2404.09134v1 Announce Type: cross 
Abstract: In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09134">https://arxiv.org/abs/2404.09134</a></p>
            <p><strong>Category:</strong> cs.NI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is fairly relevant to your interest in agents based on large language models, as it discusses employing large language models for modeling and designed transmission strategies for satellite communication networks. Furthermore, it is mostly focused on proposing new methods (via a mixture of experts), which aligns with your preference.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09339" target="_blank">Towards Practical Tool Usage for Continually Learning LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.09339v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09339v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Sarath Chandar</p>
            <p><strong>Summary:</strong> arXiv:2404.09339v1 Announce Type: cross 
Abstract: Large language models (LLMs) show an innate skill for solving language based tasks. But insights have suggested an inability to adjust for information or task-solving skills becoming outdated, as their knowledge, stored directly within their parameters, remains static in time. Tool use helps by offloading work to systems that the LLM can access through an interface, but LLMs that use them still must adapt to nonstationary environments for prolonged use, as new tools can emerge and existing tools can change. Nevertheless, tools require less specialized knowledge, therefore we hypothesize they are better suited for continual learning (CL) as they rely less on parametric memory for solving tasks and instead focus on learning when to apply pre-defined tools. To verify this, we develop a synthetic benchmark and follow this by aggregating existing NLP tasks to form a more realistic testing scenario. While we demonstrate scaling model size is not a solution, regardless of tool usage, continual learning techniques can enable tool LLMs to both adapt faster while forgetting less, highlighting their potential as continual learners.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09339">https://arxiv.org/abs/2404.09339</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to your interest in agents based on large-language models. It discusses the utilization of large language models for tasks and tool usage, which can be relevant for computer automation and controlling software. However, it does not explicitly mention controlling web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09384" target="_blank">Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches</a></h3>
            
            <p><strong>Authors:</strong> V\'ictor A. Braberman, Flavia Bonomo-Braberman, Yiannis Charalambous, Juan G. Colonna, Lucas C. Cordeiro, Rosiane de Freitas</p>
            <p><strong>Summary:</strong> arXiv:2404.09384v1 Announce Type: cross 
Abstract: Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and practitioners have been playing with prompts to see how to make the most of LLMs. By homogeneously dissecting 80 papers, we investigate in deep how software testing and verification research communities have been abstractly architecting their LLM-enabled solutions. More precisely, first, we want to validate whether downstream tasks are an adequate concept to convey the blueprint of prompt-based solutions. We also aim at identifying number and nature of such tasks in solutions. For such goal, we develop a novel downstream task taxonomy that enables pinpointing some engineering patterns in a rather varied spectrum of Software Engineering problems that encompasses testing, fuzzing, debugging, vulnerability detection, static analysis and program verification approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09384">https://arxiv.org/abs/2404.09384</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses how Large Language Models are being used for complex tasks in software testing and verification. Although it doesn't explicitly mention control of software or web browsers, the discussion of various tasks could possibly include these.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09526" target="_blank">LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism</a></h3>
            <a href="https://arxiv.org/html/2404.09526v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09526v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin</p>
            <p><strong>Summary:</strong> arXiv:2404.09526v1 Announce Type: cross 
Abstract: The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\times$ compared to the chunked prefill and 5.81$\times$ compared to the prefill-decoding disaggregation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09526">https://arxiv.org/abs/2404.09526</a></p>
            <p><strong>Category:</strong> cs.DC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to your interests since it tackles the efficiency of Large Language Models (LLMs), which relates to using such models for controlling software and automation. However, it doesn't introduce a new method for LLMs controlling software or web browsers specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09717" target="_blank">Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model</a></h3>
            
            <p><strong>Authors:</strong> Hyunsoo Cho</p>
            <p><strong>Summary:</strong> arXiv:2404.09717v1 Announce Type: cross 
Abstract: Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with erroneous responses, and flawed reasoning. Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact. To this end, this paper explores the correlation between the degree of noise and its impact on language models through instruction tuning. We first introduce the Falsity-Controllable (FACO) dataset, which comprises pairs of true answers with corresponding reasoning, as well as false pairs to manually control the falsity ratio of the dataset.Through our extensive experiments, we found multiple intriguing findings of the correlation between the factuality of the dataset and instruction tuning: Specifically, we verified falsity of the instruction is highly relevant to various benchmark scores. Moreover, when LLMs are trained with false instructions, they learn to lie and generate fake unfaithful answers, even though they know the correct answer for the user request. Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance is possible, but it failed to reach full performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09717">https://arxiv.org/abs/2404.09717</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores imitation learning in large language models which is aligned with your interest in agents based on large-language models. It provides insights into how LLMs generate responses based on the quality of their training data. Though not specifically about controlling software applications or web browsers, the findings might offer indirect implications on these aspects.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2303.03751" target="_blank">Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles</a></h3>
            <a href="https://arxiv.org/html/2303.03751v3/extracted/5534372/figure/overview.png" target="_blank"><img src="https://arxiv.org/html/2303.03751v3/extracted/5534372/figure/overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang</p>
            <p><strong>Summary:</strong> arXiv:2303.03751v3 Announce Type: replace 
Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are available. Last but not least, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers a new and effective approach for aligning Artificial Intelligence (AI) with human intentions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2303.03751">https://arxiv.org/abs/2303.03751</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper describes a method to improve the performance of Large Language Models using human feedback, which aligns well with your interest in agents based on large language models, particularly regarding using them for tasks in interactive applications like handling software or web browsers. The study, though does not directly discuss the specific use cases you are interested in, still provides valuable insights related to the general topic of using LLMs in practical applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.04891" target="_blank">In-Context Learning through the Bayesian Prism</a></h3>
            
            <p><strong>Authors:</strong> Madhur Panwar, Kabir Ahuja, Navin Goyal</p>
            <p><strong>Summary:</strong> arXiv:2306.04891v2 Announce Type: replace 
Abstract: In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.04891">https://arxiv.org/abs/2306.04891</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses in-context learning which is a key part of large language models. It does not directly focus on software or web browser control, but the insights into how transformers can learn new tasks and generalize from their training could be applicable to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.03409" target="_blank">Large Language Models as Optimizers</a></h3>
            <a href="https://arxiv.org/html/2309.03409v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.03409v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen</p>
            <p><strong>Summary:</strong> arXiv:2309.03409v3 Announce Type: replace 
Abstract: Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.03409">https://arxiv.org/abs/2309.03409</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your third theme of using large language models for automation. It discusses how to leverage large language models as optimizers, a process which may be applicable to controlling software or web browsers. However, the paper does not touch upon these specific applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.03302" target="_blank">MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</a></h3>
            <a href="https://arxiv.org/html/2310.03302v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.03302v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qian Huang, Jian Vora, Percy Liang, Jure Leskovec</p>
            <p><strong>Summary:</strong> arXiv:2310.03302v2 Announce Type: replace 
Abstract: A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.03302">https://arxiv.org/abs/2310.03302</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents MLAgentBench, a suite for benchmarking large language model (LLM)-based agents in machine learning experimentation tasks. This work directly relates to your interest in using large language models for task automation and software control. However, it doesn't discuss using LLMs for controlling web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12038" target="_blank">Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations</a></h3>
            <a href="https://arxiv.org/html/2402.12038v2/extracted/5537660/image/example_self_amplify.png" target="_blank"><img src="https://arxiv.org/html/2402.12038v2/extracted/5537660/image/example_self_amplify.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot</p>
            <p><strong>Summary:</strong> arXiv:2402.12038v2 Announce Type: replace 
Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12038">https://arxiv.org/abs/2402.12038</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents an automated method for improving Small Language Models, which can be potentially expanded to LLM. Although it doesn't directly talk about software or browser control, the automatic generation of high-quality rationales seems valuable to create sophisticated LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14361" target="_blank">OpenTab: Advancing Large Language Models as Open-domain Table Reasoners</a></h3>
            <a href="https://arxiv.org/html/2402.14361v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14361v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis</p>
            <p><strong>Summary:</strong> arXiv:2402.14361v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14361">https://arxiv.org/abs/2402.14361</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It discusses the use of a Large Language Model in conjunction with a retriever to process structured data in an open-domain setting which can be related to using large language models for controlling software or automating certain processes. However, it doesn't directly address web browser control or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02948" target="_blank">PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02948v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02948v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fanxu Meng, Zhaohui Wang, Muhan Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.02948v2 Announce Type: replace 
Abstract: As the parameters of LLMs expand, the computational cost of fine-tuning the entire model becomes prohibitive. To address this challenge, we introduce a PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA), which optimizes a significantly reduced parameter space while achieving or surpassing the performance of full-parameter fine-tuning. PiSSA is inspired by Intrinsic SAID, which suggests that pre-trained, over-parametrized models inhabit a space of low intrinsic dimension. Consequently, PiSSA represents a matrix W within the model by the product of two trainable matrices A and B, plus a residual matrix $W^{res}$ for error correction. SVD is employed to factorize W, and the principal singular values and vectors of W are utilized to initialize A and B. The residual singular values and vectors initialize the residual matrix $W^{res}$, which keeps frozen during fine-tuning. Notably, PiSSA shares the same architecture with LoRA. However, LoRA approximates Delta W through the product of two matrices, A, initialized with Gaussian noise, and B, initialized with zeros, while PiSSA initializes A and B with principal singular values and vectors of the original matrix W. PiSSA can better approximate the outcomes of full-parameter fine-tuning at the beginning by changing the essential parts while freezing the "noisy" parts. In comparison, LoRA freezes the original matrix and updates the "noise". This distinction enables PiSSA to convergence much faster than LoRA and also achieve better performance in the end. Due to the same architecture, PiSSA inherits many of LoRA's advantages, such as parameter efficiency and compatibility with quantization. Leveraging a fast SVD method, the initialization of PiSSA takes only a few seconds, inducing negligible cost of switching LoRA to PiSSA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02948">https://arxiv.org/abs/2404.02948</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a method (PiSSA) for optimizing language models, implying potential application in the control of software or other automated tasks by more efficient language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08495" target="_blank">Dataset Reset Policy Optimization for RLHF</a></h3>
            <a href="https://arxiv.org/html/2404.08495v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08495v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kiant\'e Brantley, Dipendra Misra, Jason D. Lee, Wen Sun</p>
            <p><strong>Summary:</strong> arXiv:2404.08495v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08495">https://arxiv.org/abs/2404.08495</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Even though this paper doesn't refer explicitly to the large language models, it details a reinforcement learning paradigm which is related to managing agents and it could be potentially applied to such models. The paper introduces a new method for optimizing policy training procedure which could have implications on using LLMs to control software or perform automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.10105" target="_blank">Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></h3>
            <a href="https://arxiv.org/html/2309.10105v2/extracted/5535080/assets/fig1-arxiv.png" target="_blank"><img src="https://arxiv.org/html/2309.10105v2/extracted/5535080/assets/fig1-arxiv.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan</p>
            <p><strong>Summary:</strong> arXiv:2309.10105v2 Announce Type: replace-cross 
Abstract: We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.10105">https://arxiv.org/abs/2309.10105</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interests as it deals with the behavior and abilities of large language models when fine-tuned to specific tasks, which relates to your interest in using large language models for tasks such as controlling software and web browsers. However, it does not directly propose new methods for these tasks, which is why it is not given a full score of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.09188" target="_blank">Towards Verifiable Text Generation with Symbolic References</a></h3>
            
            <p><strong>Authors:</strong> Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</p>
            <p><strong>Summary:</strong> arXiv:2311.09188v2 Announce Type: replace-cross 
Abstract: LLMs are vulnerable to hallucinations, and thus their outputs generally require laborious human verification for high-stakes applications. To this end, we propose symbolically grounded generation (SymGen) as a simple approach for enabling easier manual validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across a range of data-to-text and question-answering experiments, we find that LLMs are able to directly output text that makes use of accurate symbolic references while maintaining fluency and factuality. In a human study we further find that such annotations can streamline human verification of machine-generated text. Our code will be available at http://symgen.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.09188">https://arxiv.org/abs/2311.09188</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the use of LLMs and their symbolic grounding which can help in manual validation of LLM's output, and so it pertains to the use of large language models in controlling software and enhancing computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.04552" target="_blank">Generating Illustrated Instructions</a></h3>
            
            <p><strong>Authors:</strong> Sachit Menon, Ishan Misra, Rohit Girdhar</p>
            <p><strong>Summary:</strong> arXiv:2312.04552v2 Announce Type: replace-cross 
Abstract: We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.04552">https://arxiv.org/abs/2312.04552</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper addresses the application of large language models for generating illustrated instructions. Although it does not strictly focus on controlling software or web browsers, it can still be of interest as it relates to general automation and customisation tasks handled by large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.05356" target="_blank">Neuron-level LLM Patching for Code Generation</a></h3>
            
            <p><strong>Authors:</strong> Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang</p>
            <p><strong>Summary:</strong> arXiv:2312.05356v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have found widespread adoption in software engineering, particularly in code generation tasks. However, updating these models with new knowledge can be prohibitively expensive, yet it is essential for maximizing their utility. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction. The experimental results show that the proposed approach outperforms the state of the arts by a significant margin in both effectiveness and efficiency measures. In addition, we demonstrate the usages of \textsc{MENT} for LLM reasoning in software engineering. By editing LLM knowledge, the directly or indirectly dependent behaviors of API invocation in the chain-of-thought will change accordingly. It explained the significance of repairing LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.05356">https://arxiv.org/abs/2312.05356</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a novel approach to patching Large Language Models (LLMs) for code generation tasks, which falls under your interest in using LLMs for software control. However, while the technique is relevant, it's more focused on code generation than specifically automating control of software or web browsers, thus a score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.16818" target="_blank">H2O-Danube-1.8B Technical Report</a></h3>
            <a href="https://arxiv.org/html/2401.16818v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.16818v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, Sri Satish Ambati</p>
            <p><strong>Summary:</strong> arXiv:2401.16818v2 Announce Type: replace-cross 
Abstract: We present H2O-Danube, a series of small 1.8B language models consisting of H2O-Danube-1.8B, trained on 1T tokens, and the incremental improved H2O-Danube2-1.8B trained on an additional 2T tokens. Our models exhibit highly competitive metrics across a multitude of benchmarks and, as of the time of this writing, H2O-Danube2-1.8B achieves the top ranking on Open LLM Leaderboard for all models below the 2B parameter range. The models follow core principles of LLama 2 and Mistral, and we leverage and refine various techniques for pre-training large language models. We additionally release chat models trained with supervised fine-tuning followed by direct preference optimization. We make all models openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.16818">https://arxiv.org/abs/2401.16818</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not explicitly mention software or browser control, it discusses the training of large language models (LLMs), which are key in creating intelligent agents. It also discusses the success of the models on an open LLM leaderboard. However, as it doesn't directly focus on software or browser control, it's relevance might be slightly lower.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01643" target="_blank">L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.01643v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.01643v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Md. Kowsher, Md. Shohanur Islam Sobuj, Asif Mahmud, Nusrat Jahan Prottasha, Prakash Bhat</p>
            <p><strong>Summary:</strong> arXiv:2402.01643v2 Announce Type: replace-cross 
Abstract: Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with L-Tuning compared to traditional approaches, marking a promising advancement in fine-tuning LLMs for complex language tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01643">https://arxiv.org/abs/2402.01643</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper appears relevant to your interest in 'Agents based on large language models'. It explores L-Tuning, a new method for efficiently fine-tuning Large Language Models, which could be useful for tasks like controlling software or web browsers. Although it doesn't specifically mention these applications, the implications of the fine-tuning technique they propose should apply to these cases.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.02181" target="_blank">Not all Layers of LLMs are Necessary during Inference</a></h3>
            <a href="https://arxiv.org/html/2403.02181v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.02181v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, Zhongyuan Wang</p>
            <p><strong>Summary:</strong> arXiv:2403.02181v2 Announce Type: replace-cross 
Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance. Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.02181">https://arxiv.org/abs/2403.02181</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the efficiency of large language models during inference phase. It could provide useful insights into controlling software or automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14472" target="_blank">Detoxifying Large Language Models via Knowledge Editing</a></h3>
            
            <p><strong>Authors:</strong> Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2403.14472v3 Announce Type: replace-cross 
Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14472">https://arxiv.org/abs/2403.14472</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models, your third interest. More specifically, it discusses an important aspect of handling large language models, intervening with toxic behavior and keeping the model safe. While it may not directly involve controlling software or web browsers, the detoxification aspect discussed could be crucial in those applications as well.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.07009" target="_blank">A Mathematical Theory for Learning Semantic Languages by Abstract Learners</a></h3>
            <a href="https://arxiv.org/html/2404.07009v2/extracted/5534199/skilltextequiv.png" target="_blank"><img src="https://arxiv.org/html/2404.07009v2/extracted/5534199/skilltextequiv.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kuo-Yu Liao, Cheng-Shang Chang, Y. -W. Peter Hong</p>
            <p><strong>Summary:</strong> arXiv:2404.07009v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated the emergence of capabilities (learned skills) when the number of system parameters and the size of training data surpass certain thresholds. The exact mechanisms behind such phenomena are not fully understood and remain a topic of active research. Inspired by the skill-text bipartite graph model presented in [1] for modeling semantic language, we develop a mathematical theory to explain the emergence of learned skills, taking the learning (or training) process into account. Our approach models the learning process for skills in the skill-text bipartite graph as an iterative decoding process in Low-Density Parity Check (LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density evolution analysis, we demonstrate the emergence of learned skills when the ratio of the size of training texts to the number of skills exceeds a certain threshold. Our analysis also yields a scaling law for testing errors relative to the size of training texts. Upon completion of the training, we propose a method for semantic compression and discuss its application in semantic communication.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.07009">https://arxiv.org/abs/2404.07009</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the concepts and methodology related to large language models (LLMs), including the emergence of learned skills in these models, which might be relevant to the control and automation aspects within your topic of interest.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12767" target="_blank">When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2402.12767v2/extracted/5534918/figures/ablation1.png" target="_blank"><img src="https://arxiv.org/html/2402.12767v2/extracted/5534918/figures/ablation1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2402.12767v2 Announce Type: replace 
Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated stationary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov model to estimate latent environments and modular prior networks to identify latent states. The IDEA model outperforms several latest nonstationary forecasting methods on various benchmark datasets, highlighting its advantages in real-world scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12767">https://arxiv.org/abs/2402.12767</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it innovatively proposes a new method, the IDEA model, to tackle time series forecasting. The model considers nonstationary and stationary dependencies which fits with your interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09140" target="_blank">RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion</a></h3>
            <a href="https://arxiv.org/html/2404.09140v1/extracted/5535193/pdf/00-wifi-gt.png" target="_blank"><img src="https://arxiv.org/html/2404.09140v1/extracted/5535193/pdf/00-wifi-gt.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Guoxuan Chi, Zheng Yang, Chenshu Wu, Jingao Xu, Yuchong Gao, Yunhao Liu, Tony Xiao Han</p>
            <p><strong>Summary:</strong> arXiv:2404.09140v1 Announce Type: new 
Abstract: Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years. Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities. In this work, inspired by the stellar achievements of the diffusion model in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion. To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency Diffusion theory to enhance the original diffusion model, enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals. On this basis, we propose a Hierarchical Diffusion Transformer to translate the theory into a practical generative DNN through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data. Performance comparison with three prevalent generative models demonstrates the RF-Diffusion's superior performance in synthesizing Wi-Fi and FMCW signals. We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09140">https://arxiv.org/abs/2404.09140</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper focuses on a new method for generating time-series RF data with an adapted diffusion model. This might be relevant to your interest in new deep learning methods for time-series, and in specific, the novel Time-Frequency Diffusion theory might be viewed as a new foundation model for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09240" target="_blank">Fault Detection in Mobile Networks Using Diffusion Models</a></h3>
            <a href="https://arxiv.org/html/2404.09240v1/extracted/5535357/Figures/Methods/figure1.png" target="_blank"><img src="https://arxiv.org/html/2404.09240v1/extracted/5535357/Figures/Methods/figure1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda</p>
            <p><strong>Summary:</strong> arXiv:2404.09240v1 Announce Type: new 
Abstract: In today's hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial. Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities. To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts. These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks. In this paper, we present a system to detect anomalies in telecom networks using a generative AI model. We evaluate several strategies using diffusion models to train the model for anomaly detection using multivariate time-series data. The contributions of this paper are threefold: (i) A proposal of a framework for utilizing diffusion models for time-series anomaly detection in telecom networks, (ii) A proposal of a particular Diffusion model architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09240">https://arxiv.org/abs/2404.09240</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to your interests as it introduces a new application of deep learning, specifically diffusion models, for time series anomaly detection in telecom networks. Although it doesn't explicitly propose a new multimodal or transformer-like model for time series, it could be valuable for understanding new methods in deep learning applied to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09256" target="_blank">Foundational GPT Model for MEG</a></h3>
            <a href="https://arxiv.org/html/2404.09256v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09256v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Richard Csaky, Mats W. J. van Es, Oiwi Parker Jones, Mark Woolrich</p>
            <p><strong>Summary:</strong> arXiv:2404.09256v1 Announce Type: new 
Abstract: Deep learning techniques can be used to first training unsupervised models on large amounts of unlabelled data, before fine-tuning the models on specific tasks. This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data). However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG). Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG. First, we consider a modified Wavenet; and second, we consider a modified Transformer-based (GPT2) model. The modified GPT2 includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data. We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data. We compare the performance of these deep learning models with standard linear autoregressive (AR) modelling on MEG data. This shows that GPT2-based models provide better modelling capabilities than Wavenet and linear AR models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data. We show how the GPT2 model scales well to multiple subjects, while adapting its model to each subject through subject embedding. Finally, we show how such a model can be useful in downstream decoding tasks through data simulation. All code is available on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09256">https://arxiv.org/abs/2404.09256</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper explores two new deep learning foundational models for time series data coming from Magneto-/electroencephalography (M/EEG). It includes the adaptation of a transformer-based GPT2 model for time series forecasting, which aligns with your interest in new transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09402" target="_blank">Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion Processes</a></h3>
            <a href="https://arxiv.org/html/2404.09402v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.09402v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoming Yang, Ali Hasan, Yuting Ng, Vahid Tarokh</p>
            <p><strong>Summary:</strong> arXiv:2404.09402v1 Announce Type: new 
Abstract: McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density. As such, we study the influence of explicitly including distributional information in the parameterization of the SDE. We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE. We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems. We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling. The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard It\^o-SDEs due to the richer class of probability flows associated with MV-SDEs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09402">https://arxiv.org/abs/2404.09402</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper can be relevant as it is focused on MVC-SDEs which are used for modeling temporal data and adopts a novel approach using distributional dependence in the parameterization. Although not directly focused on DL methods, it can enhance your understanding of new methods used in time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09516" target="_blank">State Space Model for New-Generation Network Alternative to Transformers: A Survey</a></h3>
            <a href="https://arxiv.org/html/2404.09516v1/extracted/5535382/mamba_park.jpg" target="_blank"><img src="https://arxiv.org/html/2404.09516v1/extracted/5535382/mamba_park.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bo Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, Jin Tang</p>
            <p><strong>Summary:</strong> arXiv:2404.09516v1 Announce Type: new 
Abstract: In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM. More related works will be continuously updated on the following GitHub: https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09516">https://arxiv.org/abs/2404.09516</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper reviews and discusses State Space Model (SSM), a potential alternative to Transformer models, with applications to various domains including time-series data. Despite not focusing solely on forecasting, it explores new efficient model architectures which might be of interest to you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.17548" target="_blank">Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators</a></h3>
            <a href="https://arxiv.org/html/2401.17548v5/training_sample.pdf" target="_blank"><img src="https://arxiv.org/html/2401.17548v5/training_sample.pdf" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lifan Zhao, Yanyan Shen</p>
            <p><strong>Summary:</strong> arXiv:2401.17548v5 Announce Type: replace 
Abstract: Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments on six real-world datasets demonstrate that LIFT improves the state-of-the-art methods by 5.5% in average forecasting performance. Our code is available at https://github.com/SJTU-Quant/LIFT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.17548">https://arxiv.org/abs/2401.17548</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new method for multivariate time series forecasting, which makes it highly relevant to your interest in deep learning techniques for time series forecasting. There is no explicit mention of Deep Learning or multimodal models, therefore, the score is 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.01133" target="_blank">Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data</a></h3>
            <a href="https://arxiv.org/html/2403.01133v2/extracted/5535457/figures/human_annot.png" target="_blank"><img src="https://arxiv.org/html/2403.01133v2/extracted/5535457/figures/human_annot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aritra Hota, Soumyajit Chatterjee, Sandip Chakraborty</p>
            <p><strong>Summary:</strong> arXiv:2403.01133v2 Announce Type: replace 
Abstract: Traditional human-in-the-loop-based annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent large language models (LLMs) are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore LLMs as virtual annotators where the LLMs will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems of the traditional human-in-the-loop approach. Motivated by this observation, we perform a detailed study in this paper to assess whether the state-of-the-art (SOTA) LLMs can be used as virtual annotators for labeling time-series physical sensing data. To perform this in a principled manner, we segregate the study into two major phases. In the first phase, we investigate the challenges an LLM like GPT-4 faces in comprehending raw sensor data. Considering the observations from phase 1, in the next phase, we investigate the possibility of encoding the raw sensor data using SOTA SSL approaches and utilizing the projected time-series data to get annotations from the LLM. Detailed evaluation with four benchmark HAR datasets shows that SSL-based encoding and metric-based guidance allow the LLM to make more reasonable decisions and provide accurate annotations without requiring computationally expensive fine-tuning or sophisticated prompt engineering.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.01133">https://arxiv.org/abs/2403.01133</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper explores large language models as virtual annotators for time-series physical sensing data, thus aligning with your interests in 'New foundation models for time series' and 'New multimodal deep learning models for time series'. However, it's more about a novel application of LLMs to time series rather than focusing on proposing new foundational models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.11654" target="_blank">Large Transformers are Better EEG Learners</a></h3>
            <a href="https://arxiv.org/html/2308.11654v2/extracted/5534081/content/1.png" target="_blank"><img src="https://arxiv.org/html/2308.11654v2/extracted/5534081/content/1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bingxin Wang, Xiaowen Fu, Yuan Lan, Luchan Zhang, Wei Zheng, Yang Xiang</p>
            <p><strong>Summary:</strong> arXiv:2308.11654v2 Announce Type: replace-cross 
Abstract: Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. However, the limited availability of public electroencephalogram (EEG) data presents a unique challenge for extending the success of these models to EEG-based tasks. To address this gap, we propose AdaCT, plug-and-play Adapters designed for Converting Time series data into spatio-temporal 2D pseudo-images or text forms. Essentially, AdaCT-I transforms multi-channel or lengthy single-channel time series data into spatio-temporal 2D pseudo-images for fine-tuning pre-trained vision transformers, while AdaCT-T converts short single-channel data into text for fine-tuning pre-trained language transformers. The proposed approach allows for seamless integration of pre-trained vision models and language models in time series decoding tasks, particularly in EEG data analysis. Experimental results on diverse benchmark datasets, including Epileptic Seizure Recognition, Sleep-EDF, and UCI HAR, demonstrate the superiority of AdaCT over baseline methods. Overall, we provide a promising transfer learning framework for leveraging the capabilities of pre-trained vision and language models in EEG-based tasks, thereby advancing the field of time series decoding and enhancing interpretability in EEG data analysis. Our code will be available at https://github.com/wangbxj1234/AdaCE.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.11654">https://arxiv.org/abs/2308.11654</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper discusses AdaCT, a new method to convert time series data into forms interpretable by pre-trained language and vision Transformer models, which aligns with your interest in new deep learning methods and Transformer-like models for time series. However, its main application is EEG data rather than general time series forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09871" target="_blank">Explainable Online Unsupervised Anomaly Detection for Cyber-Physical Systems via Causal Discovery from Time Series</a></h3>
            <a href="https://arxiv.org/html/2404.09871v1/extracted/5537849/images/Actual-Photograph-of-SWaT-testbed.png" target="_blank"><img src="https://arxiv.org/html/2404.09871v1/extracted/5537849/images/Actual-Photograph-of-SWaT-testbed.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daniele Meli</p>
            <p><strong>Summary:</strong> arXiv:2404.09871v1 Announce Type: new 
Abstract: Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them. State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series. However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance. In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies. On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of $>10$ different anomalies. The code for experimental replication is at http://tinyurl.com/case24causal.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09871">https://arxiv.org/abs/2404.09871</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'Causality and Machine Learning', specifically the subtopic 'Causal Discovery'. The paper's primary focus is on using causal discovery to learn a system's normal causal graph for the purpose of anomaly detection. While not strictly proposing a new machine learning method, it applies existing methods in a new and novel context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08839" target="_blank">Multiply-Robust Causal Change Attribution</a></h3>
            <a href="https://arxiv.org/html/2404.08839v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08839v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</p>
            <p><strong>Summary:</strong> arXiv:2404.08839v1 Announce Type: cross 
Abstract: Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08839">https://arxiv.org/abs/2404.08839</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to align with your stated interests in causality and machine learning, particularly the subtopic of causal discovery. It discusses a new methodology for causal attribution which could provide valuable insights.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.11281" target="_blank">Towards Characterizing Domain Counterfactuals For Invertible Latent Causal Models</a></h3>
            <a href="https://arxiv.org/html/2306.11281v3/extracted/5534988/figures/simulated/intpos_ndomain3.png" target="_blank"><img src="https://arxiv.org/html/2306.11281v3/extracted/5534988/figures/simulated/intpos_ndomain3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zeyu Zhou, Ruqi Bai, Sean Kulinski, Murat Kocaoglu, David I. Inouye</p>
            <p><strong>Summary:</strong> arXiv:2306.11281v3 Announce Type: replace 
Abstract: Answering counterfactual queries has important applications such as explainability, robustness, and fairness but is challenging when the causal variables are unobserved and the observations are non-linear mixtures of these latent variables, such as pixels in images. One approach is to recover the latent Structural Causal Model (SCM), which may be infeasible in practice due to requiring strong assumptions, e.g., linearity of the causal mechanisms or perfect atomic interventions. Meanwhile, more practical ML-based approaches using naive domain translation models to generate counterfactual samples lack theoretical grounding and may construct invalid counterfactuals. In this work, we strive to strike a balance between practicality and theoretical guarantees by analyzing a specific type of causal query called domain counterfactuals, which hypothesizes what a sample would have looked like if it had been generated in a different domain (or environment). We show that recovering the latent SCM is unnecessary for estimating domain counterfactuals, thereby sidestepping some of the theoretic challenges. By assuming invertibility and sparsity of intervention, we prove domain counterfactual estimation error can be bounded by a data fit term and intervention sparsity term. Building upon our theoretical results, we develop a theoretically grounded practical algorithm that simplifies the modeling process to generative model estimation under autoregressive and shared parameter constraints that enforce intervention sparsity. Finally, we show an improvement in counterfactual estimation over baseline methods through extensive simulated and image-based experiments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.11281">https://arxiv.org/abs/2306.11281</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper appears to present a study on counterfactuals within the field of causal discovery, discussing both theoretical and practical aspects of causal modeling and therefore could be relevant to your interest in causal discovery and causal representation learning topics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.06969" target="_blank">FiP: a Fixed-Point Approach for Causal Generative Modeling</a></h3>
            <a href="https://arxiv.org/html/2404.06969v2/extracted/5535706/figures/effect_subsampling_out.png" target="_blank"><img src="https://arxiv.org/html/2404.06969v2/extracted/5535706/figures/effect_subsampling_out.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Meyer Scetbon, Joel Jennings, Agrin Hilmkil, Cheng Zhang, Chao Ma</p>
            <p><strong>Summary:</strong> arXiv:2404.06969v2 Announce Type: replace 
Abstract: Modeling true world data-generating processes lies at the heart of empirical science. Structural Causal Models (SCMs) and their associated Directed Acyclic Graphs (DAGs) provide an increasingly popular answer to such problems by defining the causal generative process that transforms random noise into observations. However, learning them from observational data poses an ill-posed and NP-hard inverse problem in general. In this work, we propose a new and equivalent formalism that does not require DAGs to describe them, viewed as fixed-point problems on the causally ordered variables, and we show three important cases where they can be uniquely recovered given the topological ordering (TO). To the best of our knowledge, we obtain the weakest conditions for their recovery when TO is known. Based on this, we design a two-stage causal generative model that first infers the causal order from observations in a zero-shot manner, thus by-passing the search, and then learns the generative fixed-point SCM on the ordered variables. To infer TOs from observations, we propose to amortize the learning of TOs on generated datasets by sequentially predicting the leaves of graphs seen during training. To learn fixed-point SCMs, we design a transformer-based architecture that exploits a new attention mechanism enabling the modeling of causal structures, and show that this parameterization is consistent with our formalism. Finally, we conduct an extensive evaluation of each method individually, and show that when combined, our model outperforms various baselines on generated out-of-distribution problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.06969">https://arxiv.org/abs/2404.06969</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is a valuable resource as it delves into causal generative modeling, providing new methods and insights into the recovery of topological ordering. Although it doesn't address large language models in the context of causal discovery, it introduces new techniques relevant to your interest in causal discovery and causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2212.05256" target="_blank">Causality-Aware Local Interpretable Model-Agnostic Explanations</a></h3>
            <a href="https://arxiv.org/html/2212.05256v3/extracted/5537162/fig/calime_workflow.png" target="_blank"><img src="https://arxiv.org/html/2212.05256v3/extracted/5537162/fig/calime_workflow.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Martina Cinquini, Riccardo Guidotti</p>
            <p><strong>Summary:</strong> arXiv:2212.05256v3 Announce Type: replace-cross 
Abstract: A main drawback of eXplainable Artificial Intelligence (XAI) approaches is the feature independence assumption, hindering the study of potential variable dependencies. This leads to approximating black box behaviors by analyzing the effects on randomly generated feature values that may rarely occur in the original samples. This paper addresses this issue by integrating causal knowledge in an XAI method to enhance transparency and enable users to assess the quality of the generated explanations. Specifically, we propose a novel extension to a widely used local and model-agnostic explainer, which encodes explicit causal relationships within the data surrounding the instance being explained. Extensive experiments show that our approach overcomes the original method in terms of faithfully replicating the black-box model's mechanism and the consistency and reliability of the generated explanations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2212.05256">https://arxiv.org/abs/2212.05256</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper focuses on the topic of causal understanding which you are interested in, specifically, its relevance to explainable AI. However, it does not explicitly mention large language models in the context of causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11652" target="_blank">Doubly Robust Inference in Causal Latent Factor Models</a></h3>
            
            <p><strong>Authors:</strong> Alberto Abadie, Anish Agarwal, Raaz Dwivedi, Abhin Shah</p>
            <p><strong>Summary:</strong> arXiv:2402.11652v2 Announce Type: replace-cross 
Abstract: This article introduces a new estimator of average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11652">https://arxiv.org/abs/2402.11652</a></p>
            <p><strong>Category:</strong> econ.EM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interests in new approaches to causal discovery and causal representation learning. Although it is not explicitly about large language models, it presents a novel estimator for average treatment effects under unobserved confounding, which could be relevant to your area of interest.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 16, 2024 at 21:49:05</div></body></html>