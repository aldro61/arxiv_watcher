
            <html>
            <head>
                <title>Report Generated on April 17, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 17, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10209" target="_blank">Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.10209v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10209v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siqiao Xue, Danrui Qi, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang Zhao, Fan Zhou, Hong Yi, Shaodong Liu, Hongjun Yang, Faqiang Chen</p>
            <p><strong>Summary:</strong> arXiv:2404.10209v1 Announce Type: cross 
Abstract: The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. The technologies of interacting with data particularly have an important entanglement with LLMs as efficient and intuitive data interactions are paramount. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github(https://github.com/eosphoros-ai/DB-GPT) which already has over 10.7k stars.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10209">https://arxiv.org/abs/2404.10209</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper 'Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models' is quite relevant to your interests in 'Agents based on large-language models'. It discusses how LLMs are integrated into software for enhanced data interaction tasks, which aligns with your subtopics of using LLMs for software control and computer automation. However, it does not specifically mention the use of LLMs for web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10308" target="_blank">Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.10308v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10308v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo Shin</p>
            <p><strong>Summary:</strong> arXiv:2404.10308v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. Previous works have explored architectural changes and modifications in positional encoding to relax the constraint, but they often require expensive training or do not address the computational demands of self-attention. In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable chunks. Each chunk is then processed collectively, employing a hierarchical strategy that merges adjacent chunks at progressive transformer layers. A token reduction technique precedes each merging, ensuring memory usage efficiency. We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions. Our experiments demonstrate the proposed method's superior performance and memory efficiency, enabling the broader use of LLMs in contexts requiring extended context. Code is available at https://github.com/alinlab/HOMER.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10308">https://arxiv.org/abs/2404.10308</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is pertinent to your interest in 'Large Language Models (LLMs)' as it proposes 'Hierarchical cOntext MERging (HOMER)', a new method to overcome the context limit constraints of LLMs. It could potentially enhance the application of Large Language Models for controlling software or web browsing.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10019" target="_blank">Can AI Understand Our Universe? Test of Fine-Tuning GPT by Astrophysical Data</a></h3>
            <a href="https://arxiv.org/html/2404.10019v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10019v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yu Wang, Shu-Rui Zhang, Aidin Momtaz, Rahim Moradi, Fatemeh Rastegarnia, Narek Sahakyan, Soroush Shakeri, Liang Li</p>
            <p><strong>Summary:</strong> arXiv:2404.10019v1 Announce Type: cross 
Abstract: ChatGPT has been the most talked-about concept in recent months, captivating both professionals and the general public alike, and has sparked discussions about the changes that artificial intelligence (AI) will bring to the world. As physicists and astrophysicists, we are curious about if scientific data can be correctly analyzed by large language models (LLMs) and yield accurate physics. In this article, we fine-tune the generative pre-trained transformer (GPT) model by the astronomical data from the observations of galaxies, quasars, stars, gamma-ray bursts (GRBs), and the simulations of black holes (BHs), the fine-tuned model demonstrates its capability to classify astrophysical phenomena, distinguish between two types of GRBs, deduce the redshift of quasars, and estimate BH parameters. We regard this as a successful test, marking the LLM's proven efficacy in scientific research. With the ever-growing volume of multidisciplinary data and the advancement of AI technology, we look forward to the emergence of a more fundamental and comprehensive understanding of our universe. This article also shares some interesting thoughts on data collection and AI design. Using the approach of understanding the universe - looking outward at data and inward for fundamental building blocks - as a guideline, we propose a method of series expansion for AI, suggesting ways to train and control AI that is smarter than humans.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10019">https://arxiv.org/abs/2404.10019</a></p>
            <p><strong>Category:</strong> astro-ph.IM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a test of large language model usage in astrophysics simulations and data analysis, showcasing the potential capability of AI in scientific fields. While not directly related to software or web browser control by LLMs, it touches on the potential for AI smarter than humans, which is connected to automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10155" target="_blank">Quality Assessment of Prompts Used in Code Generation</a></h3>
            
            <p><strong>Authors:</strong> Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos</p>
            <p><strong>Summary:</strong> arXiv:2404.10155v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code-generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models possibly have data contamination issues.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10155">https://arxiv.org/abs/2404.10155</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses large language models (LLMs), code generation, and the quality of prompt which is closely related to software control and automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10179" target="_blank">Scaling Instructable Agents Across Many Simulated Worlds</a></h3>
            <a href="https://arxiv.org/html/2404.10179v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10179v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong>  SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, Nick Young</p>
            <p><strong>Summary:</strong> arXiv:2404.10179v1 Announce Type: cross 
Abstract: Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10179">https://arxiv.org/abs/2404.10179</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your third category as it discusses the use of language instructions for AI agency in virtual 3D environments. It involves the application of large language models in controlling simulation, closely aligning with your interest in 'Using large language models to control software'. It however does not fully meet the high relevancy score of 5, since it doesn't specifically discuss web browser control or other computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10220" target="_blank">Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V</a></h3>
            <a href="https://arxiv.org/html/2404.10220v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10220v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peiyuan Zhi, Zhiyuan Zhang, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang</p>
            <p><strong>Summary:</strong> arXiv:2404.10220v1 Announce Type: cross 
Abstract: Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10220">https://arxiv.org/abs/2404.10220</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls within your interests in large language models controlling systems, in this case a robot. GPT-4V, the 'brain' in the closed-loop framework, is a language foundation model used for reasoning and adaptive planning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10259" target="_blank">Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy</a></h3>
            <a href="https://arxiv.org/html/2404.10259v1/extracted/5538829/llm_tp.png" target="_blank"><img src="https://arxiv.org/html/2404.10259v1/extracted/5538829/llm_tp.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tunazzina Islam, Dan Goldwasser</p>
            <p><strong>Summary:</strong> arXiv:2404.10259v1 Announce Type: cross 
Abstract: The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10259">https://arxiv.org/abs/2404.10259</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It explicitly discusses using larged Language Models (LLMs) in a loop strategy to extract information from social media data. However, it does not directly address controlling software or web browsers with LLMs, which are your specific subtopics of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10304" target="_blank">LLM-Powered Test Case Generation for Detecting Tricky Bugs</a></h3>
            <a href="https://arxiv.org/html/2404.10304v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10304v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kaibo Liu, Yiyang Liu, Zhenpeng Chen, Jie M. Zhang, Yudong Han, Yun Ma, Ge Li, Gang Huang</p>
            <p><strong>Summary:</strong> arXiv:2404.10304v1 Announce Type: cross 
Abstract: Conventional automated test generation tools struggle to generate test oracles and tricky bug-revealing test inputs. Large Language Models (LLMs) can be prompted to produce test inputs and oracles for a program directly, but the precision of the tests can be very low for complex scenarios (only 6.3% based on our experiments). To fill this gap, this paper proposes AID, which combines LLMs with differential testing to generate fault-revealing test inputs and oracles targeting plausibly correct programs (i.e., programs that have passed all the existing tests). In particular, AID selects test inputs that yield diverse outputs on a set of program variants generated by LLMs, then constructs the test oracle based on the outputs. We evaluate AID on two large-scale datasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three state-of-the-art baselines. The evaluation results show that the recall, precision, and F1 score of AID outperform the state-of-the-art by up to 1.80x, 2.65x, and 1.66x, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10304">https://arxiv.org/abs/2404.10304</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is relevant to your interest in 'Agents based on large-language models'. It discusses the use of Large Language Models for detecting bugs in programs, which is a form of software control. However, it doesn't touch specifically on controlling web browsers or automating computers, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10513" target="_blank">CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity</a></h3>
            <a href="https://arxiv.org/html/2404.10513v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10513v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, Peter Izsak</p>
            <p><strong>Summary:</strong> arXiv:2404.10513v1 Announce Type: cross 
Abstract: State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses. One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output. However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems. We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions. This approach focuses the reasoning process on generating an attribution-centric output. Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions. In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10513">https://arxiv.org/abs/2404.10513</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents an attribution-oriented reasoning method for Large Language Models which might be interesting for you. Though it doesn't explicitly focus on software control or automation, it shows potential for improving large language models, like GPT-4, which may have implications in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10630" target="_blank">HLAT: High-quality Large Language Model Pre-trained on AWS Trainium</a></h3>
            <a href="https://arxiv.org/html/2404.10630v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10630v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haozheng Fan, Hao Zhou, Guangtai Huang, Parameswaran Raman, Xinwei Fu, Gaurav Gupta, Dhananjay Ram, Yida Wang, Jun Huan</p>
            <p><strong>Summary:</strong> arXiv:2404.10630v1 Announce Type: cross 
Abstract: Getting large language models (LLMs) to perform well on the downstream tasks requires pre-training over trillions of tokens. This typically demands a large number of powerful computational devices in addition to a stable distributed training framework to accelerate the training. The growing number of applications leveraging AI/ML had led to a scarcity of the expensive conventional accelerators (such as GPUs), which begs the need for the alternative specialized-accelerators that are scalable and cost-efficient. AWS Trainium is the second-generation machine learning accelerator that has been purposely built for training large deep learning models. Its corresponding instance, Amazon EC2 trn1, is an alternative to GPU instances for LLM training. However, training LLMs with billions of parameters on trn1 is challenging due to its relatively nascent software ecosystem. In this paper, we showcase HLAT: a 7 billion parameter decoder-only LLM pre-trained using trn1 instances over 1.8 trillion tokens. The performance of HLAT is benchmarked against popular open source baseline models including LLaMA and OpenLLaMA, which have been trained on NVIDIA GPUs and Google TPUs, respectively. On various evaluation tasks, we show that HLAT achieves model quality on par with the baselines. We also share the best practice of using the Neuron Distributed Training Library (NDTL), a customized distributed training library for AWS Trainium to achieve efficient training. Our work demonstrates that AWS Trainium powered by the NDTL is able to successfully pre-train state-of-the-art LLM models with high performance and cost-effectiveness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10630">https://arxiv.org/abs/2404.10630</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is of interest to you because it describes the training of a Large Language Model (LLM) which could potentially be adapted for controlling software and web browsers. However, it doesn't delve specifically into usage of LLMs for controlling software or web browsers hence the 4 rating.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10642" target="_blank">Self-playing Adversarial Language Game Enhances LLM Reasoning</a></h3>
            <a href="https://arxiv.org/html/2404.10642v1/extracted/5540296/figs/spag-reasoning-plot.png" target="_blank"><img src="https://arxiv.org/html/2404.10642v1/extracted/5540296/figs/spag-reasoning-plot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Nan Du</p>
            <p><strong>Summary:</strong> arXiv:2404.10642v1 Announce Type: cross 
Abstract: We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the attacker. The attacker aims to induce the defender to utter the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players should have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Play in this Adversarial language Game (SPAG). With this goal, we let LLMs act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performance uniformly improves on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLM's reasoning ability. The code is at https://github.com/Linear95/SPAG.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10642">https://arxiv.org/abs/2404.10642</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses self-play training procedure of large language models and how it enhances the reasoning ability. Although it doesn't specifically target software or web browsers control, it provides insights about improving the reasoning ability of LLMs which could be applied in those areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10678" target="_blank">Automating REST API Postman Test Cases Using LLM</a></h3>
            
            <p><strong>Authors:</strong> S Deepika Sri, Mohammed Aadil S, Sanjjushri Varshini R, Raja CSP Raman, Gopinath Rajagopal, S Taranath Chan</p>
            <p><strong>Summary:</strong> arXiv:2404.10678v1 Announce Type: cross 
Abstract: In the contemporary landscape of technological advancements, the automation of manual processes is crucial, compelling the demand for huge datasets to effectively train and test machines. This research paper is dedicated to the exploration and implementation of an automated approach to generate test cases specifically using Large Language Models. The methodology integrates the use of Open AI to enhance the efficiency and effectiveness of test case generation for training and evaluating Large Language Models. This formalized approach with LLMs simplifies the testing process, making it more efficient and comprehensive. Leveraging natural language understanding, LLMs can intelligently formulate test cases that cover a broad range of REST API properties, ensuring comprehensive testing. The model that is developed during the research is trained using manually collected postman test cases or instances for various Rest APIs. LLMs enhance the creation of Postman test cases by automating the generation of varied and intricate test scenarios. Postman test cases offer streamlined automation, collaboration, and dynamic data handling, providing a user-friendly and efficient approach to API testing compared to traditional test cases. Thus, the model developed not only conforms to current technological standards but also holds the promise of evolving into an idea of substantial importance in future technological advancements.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10678">https://arxiv.org/abs/2404.10678</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models in automating the generation of test cases for REST API, which relates to your interest in using large language models for computer automation. However, it doesn't specifically deal with control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17722" target="_blank">Large Language Models as Generalizable Policies for Embodied Tasks</a></h3>
            <a href="https://arxiv.org/html/2310.17722v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.17722v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev</p>
            <p><strong>Summary:</strong> arXiv:2310.17722v2 Announce Type: replace 
Abstract: We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17722">https://arxiv.org/abs/2310.17722</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models' specifically in the aspect of 'Using large language models to control software'. It discusses the application of Large Language Models (LLMs) in performing embodied visual tasks, which requires controlling actions directly within the environment.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14866" target="_blank">APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.14866v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14866v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao Yu</p>
            <p><strong>Summary:</strong> arXiv:2402.14866v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its effectiveness to produce high-quality quantized LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14866">https://arxiv.org/abs/2402.14866</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method to facilitate deployment of Large Language Models (LLMs) on edge devices, relevant to your interest in optimizing and using LLMs for controlling software and in automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.18742" target="_blank">Understanding the Learning Dynamics of Alignment with Human Feedback</a></h3>
            <a href="https://arxiv.org/html/2403.18742v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.18742v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shawn Im, Yixuan Li</p>
            <p><strong>Summary:</strong> arXiv:2403.18742v4 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.18742">https://arxiv.org/abs/2403.18742</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Even though it doesn't provide a new method, this paper is related to the alignment of large language models in controlling systems which aligns closely with your interest in exploring large language model based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08495" target="_blank">Dataset Reset Policy Optimization for RLHF</a></h3>
            <a href="https://arxiv.org/html/2404.08495v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08495v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kiant\'e Brantley, Dipendra Misra, Jason D. Lee, Wen Sun</p>
            <p><strong>Summary:</strong> arXiv:2404.08495v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08495">https://arxiv.org/abs/2404.08495</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses Reinforcement Learning from Human Preference-based feedback which is often used for fine-tuning generative models. While it doesn't directly mention using large language models to control software or web browsers, it is still related to agents based on large-language models and presents a new algorithm (DR-PO) which brings improvements over existing online policy training procedures. However, it doesn't propose a new method with large language models, hence a score of 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08555" target="_blank">RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs</a></h3>
            
            <p><strong>Authors:</strong> Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva</p>
            <p><strong>Summary:</strong> arXiv:2404.08555v2 Announce Type: replace 
Abstract: State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08555">https://arxiv.org/abs/2404.08555</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper delves into the use of Large Language Models (LLMs) and how they can be improved to serve as effective assistants. The focus is on the Reinforcement Learning from Human Feedback (RLHF) method, which could be essential in the control and automation of software. Although the specific application to controlling web browsers is not mentioned, the principles discussed may be broadly applicable to this topic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08801" target="_blank">Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</a></h3>
            
            <p><strong>Authors:</strong> Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou</p>
            <p><strong>Summary:</strong> arXiv:2404.08801v2 Announce Type: replace 
Abstract: The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: https://github.com/XuezheMax/megalodon</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08801">https://arxiv.org/abs/2404.08801</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large-language model agents. It introduces 'Megalodon', a new architecture for efficient sequence modeling which could be used to control software or automate tasks, as it works with unlimited context length. However, direct applications to controlling software or web browsers are not mentioned, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.14502" target="_blank">RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2305.14502v2/extracted/5540628/arch_new.png" target="_blank"><img src="https://arxiv.org/html/2305.14502v2/extracted/5540628/arch_new.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Scarlatos, Andrew Lan</p>
            <p><strong>Summary:</strong> arXiv:2305.14502v2 Announce Type: replace-cross 
Abstract: Recent developments in large pre-trained language models have enabled unprecedented performance on a variety of downstream tasks. Achieving best performance with these models often leverages in-context learning, where a model performs a (possibly new) task given one or more examples. However, recent work has shown that the choice of examples can have a large impact on task performance and that finding an optimal set of examples is non-trivial. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process and train an example retriever using reinforcement learning. We evaluate RetICL on math word problem solving and scientific question answering tasks and show that it consistently outperforms or matches heuristic and learnable baselines. We also use case studies to show that RetICL implicitly learns representations of problem solving strategies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.14502">https://arxiv.org/abs/2305.14502</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models and how they can enhance task performance. It introduces a new method for optimally selecting examples for in-context learning that could be beneficial for improving computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.13854" target="_blank">WebArena: A Realistic Web Environment for Building Autonomous Agents</a></h3>
            <a href="https://arxiv.org/html/2307.13854v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.13854v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig</p>
            <p><strong>Summary:</strong> arXiv:2307.13854v4 Announce Type: replace-cross 
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.13854">https://arxiv.org/abs/2307.13854</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the use of large language models for controlling web-based tasks, falling under your interests with regard to autonomous agents based on large language models. However, it does not explicitly propose new methods which lowers the score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07023" target="_blank">Automatic Macro Mining from Interaction Traces at Scale</a></h3>
            <a href="https://arxiv.org/html/2310.07023v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.07023v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Forrest Huang, Gang Li, Tao Li, Yang Li</p>
            <p><strong>Summary:</strong> arXiv:2310.07023v2 Announce Type: replace-cross 
Abstract: Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of mobile apps. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. We conduct multiple studies to validate the quality of extracted macros, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various downstream applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07023">https://arxiv.org/abs/2310.07023</a></p>
            <p><strong>Category:</strong> cs.HC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest of using large language models for computer automation. It introduces a novel approach using Large Language Models for task automation in mobile apps, which aligns with your interest in how large language models can be used for controlling software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07641" target="_blank">Evaluating Large Language Models at Evaluating Instruction Following</a></h3>
            <a href="https://arxiv.org/html/2310.07641v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.07641v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen</p>
            <p><strong>Summary:</strong> arXiv:2310.07641v2 Announce Type: replace-cross 
Abstract: As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07641">https://arxiv.org/abs/2310.07641</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to using large language models for instruction following, which is an aspect of controlling software and automating tasks with language models. It doesn't propose a new method, but discusses methods for evaluating such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17639" target="_blank">In-Context Learning Dynamics with Random Binary Sequences</a></h3>
            <a href="https://arxiv.org/html/2310.17639v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.17639v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman</p>
            <p><strong>Summary:</strong> arXiv:2310.17639v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) trained on huge corpora of text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs' behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17639">https://arxiv.org/abs/2310.17639</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the nuanced understanding of Large Language Model's (LLM's) capabilities and their behavioural patterns through in-context learning dynamics. Although it does not directly mention controlling software or web browsers, the understanding of in-context learning dynamics of LLMs could be a crucial aspect in designing agents for such tasks. It's an insightful read to understand the learning dynamics of LLMs and how different prompts can bring out various capabilities, something crucial when designing automation systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.15685" target="_blank">What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</a></h3>
            <a href="https://arxiv.org/html/2312.15685v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.15685v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He</p>
            <p><strong>Summary:</strong> arXiv:2312.15685v2 Announce Type: replace-cross 
Abstract: Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.15685">https://arxiv.org/abs/2312.15685</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in large language models as it discusses an effective, data-efficient method called deita for aligning models including LLaMA and Mistral, which could potentially be applied to control software or web browsers. However, it doesn't specifically mention controlling agents with these models, hence the score 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.07938" target="_blank">Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.07938v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.07938v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Syed Mekael Wasti, Ken Q. Pu, Ali Neshati</p>
            <p><strong>Summary:</strong> arXiv:2402.07938v2 Announce Type: replace-cross 
Abstract: The evolution of Large Language Models (LLMs) has showcased remarkable capacities for logical reasoning and natural language comprehension. These capabilities can be leveraged in solutions that semantically and textually model complex problems. In this paper, we present our efforts toward constructing a framework that can serve as an intermediary between a user and their user interface (UI), enabling dynamic and real-time interactions. We employ a system that stands upon textual semantic mappings of UI components, in the form of annotations. These mappings are stored, parsed, and scaled in a custom data structure, supplementary to an agent-based prompting backend engine. Employing textual semantic mappings allows each component to not only explain its role to the engine but also provide expectations. By comprehending the needs of both the user and the components, our LLM engine can classify the most appropriate application, extract relevant parameters, and subsequently execute precise predictions of the user's expected actions. Such an integration evolves static user interfaces into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.07938">https://arxiv.org/abs/2402.07938</a></p>
            <p><strong>Category:</strong> cs.HC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses building a system that leverages large language models (LLMs) capabilities to interact with a user interface (UI). This falls under your interest in 'Using large language models to control software' and 'Using large language models to control web browsers' in the 'llm-agents' tag, although it doesn't directly propose a new method but rather explores an application.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11809" target="_blank">Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding</a></h3>
            <a href="https://arxiv.org/html/2402.11809v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11809v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao</p>
            <p><strong>Summary:</strong> arXiv:2402.11809v2 Announce Type: replace-cross 
Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11809">https://arxiv.org/abs/2402.11809</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in large language models and their applications, particularly in software control. While it doesn't directly address the concept of automation or web browsing control, it delves into accelerating the inference speed of LLMs which could be useful in building efficient LLM-based agents.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10458" target="_blank">Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A Patch and Transformer-Based Approach</a></h3>
            <a href="https://arxiv.org/html/2404.10458v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10458v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qiuyi Hong, Fanlin Meng, Felipe Maldonado</p>
            <p><strong>Summary:</strong> arXiv:2404.10458v1 Announce Type: new 
Abstract: In the context of increasing demands for long-term multi-energy load forecasting in real-world applications, this paper introduces Patchformer, a novel model that integrates patch embedding with encoder-decoder Transformer-based architectures. To address the limitation in existing Transformer-based models, which struggle with intricate temporal patterns in long-term forecasting, Patchformer employs patch embedding, which predicts multivariate time-series data by separating it into multiple univariate data and segmenting each of them into multiple patches. This method effectively enhances the model's ability to capture local and global semantic dependencies. The numerical analysis shows that the Patchformer obtains overall better prediction accuracy in both multivariate and univariate long-term forecasting on the novel Multi-Energy dataset and other benchmark datasets. In addition, the positive effect of the interdependence among energy-related products on the performance of long-term time-series forecasting across Patchformer and other compared models is discovered, and the superiority of the Patchformer against other models is also demonstrated, which presents a significant advancement in handling the interdependence and complexities of long-term multi-energy forecasting. Lastly, Patchformer is illustrated as the only model that follows the positive correlation between model performance and the length of the past sequence, which states its ability to capture long-range past local semantic information.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10458">https://arxiv.org/abs/2404.10458</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes the 'Patchformer', a transformer and patch-based approach for long-term multi-energy load forecasting which is relevant to your interest in 'New transformer-like models for time series'. However, it doesn't seem to cover multimodal deep learning or specifically new foundation models, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10024" target="_blank">ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs</a></h3>
            <a href="https://arxiv.org/html/2404.10024v1/extracted/5536537/figs/fig1_v2_final.png" target="_blank"><img src="https://arxiv.org/html/2404.10024v1/extracted/5536537/figs/fig1_v2_final.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yogesh Verma, Markus Heinonen, Vikas Garg</p>
            <p><strong>Summary:</strong> arXiv:2404.10024v1 Announce Type: cross 
Abstract: Climate and weather prediction traditionally relies on complex numerical simulations of atmospheric physics. Deep learning approaches, such as transformers, have recently challenged the simulation paradigm with complex network forecasts. However, they often act as data-driven black-box models that neglect the underlying physics and lack uncertainty quantification. We address these limitations with ClimODE, a spatiotemporal continuous-time process that implements a key principle of advection from statistical mechanics, namely, weather changes due to a spatial movement of quantities over time. ClimODE models precise weather evolution with value-conserving dynamics, learning global weather transport as a neural flow, which also enables estimating the uncertainty in predictions. Our approach outperforms existing data-driven methods in global and regional forecasting with an order of magnitude smaller parameterization, establishing a new state of the art.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10024">https://arxiv.org/abs/2404.10024</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper focuses on time series data and weather forecasting. It proposes a Neural ODE method that takes physical principles into account as a new deep learning method specifically for time series data. The work is focused more on a method proposal rather than an application.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.07937" target="_blank">Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method</a></h3>
            
            <p><strong>Authors:</strong> Charis Stamouli, Ingvar Ziemann, George J. Pappas</p>
            <p><strong>Summary:</strong> arXiv:2404.07937v2 Announce Type: replace-cross 
Abstract: We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.07937">https://arxiv.org/abs/2404.07937</a></p>
            <p><strong>Category:</strong> math.ST</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper may be of interest as it discusses a method for time-varying parametric predictor models that can be applied to a class of identifiable AutoRegressive Moving Average (ARMA) models, which is useful for time series forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.03597" target="_blank">Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs</a></h3>
            <a href="https://arxiv.org/html/2401.03597v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.03597v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pengfei Ding, Yan Wang, Guanfeng Liu, Nan Wang, Xiaofang Zhou</p>
            <p><strong>Summary:</strong> arXiv:2401.03597v3 Announce Type: replace 
Abstract: Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby leading to a novel problem of out-of-distribution (OOD) generalization in HGFL. To address this challenging problem, we propose a novel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In COHF, we first characterize distribution shifts in HGs with a structural causal model, establishing an invariance principle for OOD generalization in HGFL. Then, following this invariance principle, we propose a new variational autoencoder-based heterogeneous graph neural network to mitigate the impact of distribution shifts. Finally, by integrating this network with a novel meta-learning framework, COHF effectively transfers knowledge to the target HG to predict new classes with few-labeled data. Extensive experiments on seven real-world datasets have demonstrated the superior performance of COHF over the state-of-the-art methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.03597">https://arxiv.org/abs/2401.03597</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses Causal Representation Learning, which is one of your subtopics of interest. Although it does not expressly involve large language models in causal discovery, it does propose a novel approach to diagnose and address distribution shifts in HGs with a structural causal model.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 17, 2024 at 21:37:16</div></body></html>