
            <html>
            <head>
                <title>Report Generated on April 18, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 18, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10942" target="_blank">What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2404.10942v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10942v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.10942v1 Announce Type: new 
Abstract: In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10942">https://arxiv.org/abs/2404.10942</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper may spark your interest as it delves into the topic of causal relationships and fairness in reinforcement learning. It provides insights into the dynamics of fairness and how unfairness can arise in RL problems through a causal lens.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11036" target="_blank">Cross-Platform Hate Speech Detection with Weakly Supervised Causal Disentanglement</a></h3>
            <a href="https://arxiv.org/html/2404.11036v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11036v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</p>
            <p><strong>Summary:</strong> arXiv:2404.11036v1 Announce Type: new 
Abstract: Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity. With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited. In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators. However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech. Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate. Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance. HATE WATCH advances scalable content moderation techniques towards developing safer online communities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11036">https://arxiv.org/abs/2404.11036</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper introduces a novel framework of causal disentanglement for hate speech detection across different platforms. It utilizes causality inspired disentanglement to differentiate universal hate indicators from platform-specific peculiarities, which is similar to 'Causal representation learning' and 'Causal discovery' in your listed interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10883" target="_blank">Automated Discovery of Functional Actual Causes in Complex Environments</a></h3>
            <a href="https://arxiv.org/html/2404.10883v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10883v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Caleb Chuck, Sankaran Vaidyanathan, Stephen Giguere, Amy Zhang, David Jensen, Scott Niekum</p>
            <p><strong>Summary:</strong> arXiv:2404.10883v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) algorithms often struggle to learn policies that generalize to novel situations due to issues such as causal confusion, overfitting to irrelevant factors, and failure to isolate control of state factors. These issues stem from a common source: a failure to accurately identify and exploit state-specific causal relationships in the environment. While some prior works in RL aim to identify these relationships explicitly, they rely on informal domain-specific heuristics such as spatial and temporal proximity. Actual causality offers a principled and general framework for determining the causes of particular events. However, existing definitions of actual cause often attribute causality to a large number of events, even if many of them rarely influence the outcome. Prior work on actual causality proposes normality as a solution to this problem, but its existing implementations are challenging to scale to complex and continuous-valued RL environments. This paper introduces functional actual cause (FAC), a framework that uses context-specific independencies in the environment to restrict the set of actual causes. We additionally introduce Joint Optimization for Actual Cause Inference (JACI), an algorithm that learns from observational data to infer functional actual causes. We demonstrate empirically that FAC agrees with known results on a suite of examples from the actual causality literature, and JACI identifies actual causes with significantly higher accuracy than existing heuristic methods in a set of complex, continuous-valued environments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10883">https://arxiv.org/abs/2404.10883</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interest in 'Causality and Machine Learning'. It proposes a new framework and algorithm 'Functional Actual Cause (FAC)' and 'Joint Optimization for Actual Cause Inference (JACI)' for determining actual causes in a general and principled way, which could be closely related to causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11341" target="_blank">The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology</a></h3>
            <a href="https://arxiv.org/html/2404.11341v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11341v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Juan L. Gamella, Jonas Peters, Peter B\"uhlmann</p>
            <p><strong>Summary:</strong> arXiv:2404.11341v1 Announce Type: cross 
Abstract: In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11341">https://arxiv.org/abs/2404.11341</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Causality and Machine Learning'. It discusses the use of causal chambers, which are computer-controlled laboratories, for tasks including causal discovery. The paper also makes use of real-world data, providing a suitable base for the development and testing of new models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.05771" target="_blank">Hacking Task Confounder in Meta-Learning</a></h3>
            <a href="https://arxiv.org/html/2312.05771v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.05771v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jingyao Wang, Yi Ren, Zeen Song, Jianqi Zhang, Changwen Zheng, Wenwen Qiang</p>
            <p><strong>Summary:</strong> arXiv:2312.05771v2 Announce Type: replace 
Abstract: Meta-learning enables rapid generalization to new tasks by learning knowledge from various tasks. It is intuitively assumed that as the training progresses, a model will acquire richer knowledge, leading to better generalization performance. However, our experiments reveal an unexpected result: there is negative knowledge transfer between tasks, affecting generalization performance. To explain this phenomenon, we conduct Structural Causal Models (SCMs) for causal analysis. Our investigation uncovers the presence of spurious correlations between task-specific causal factors and labels in meta-learning. Furthermore, the confounding factors differ across different batches. We refer to these confounding factors as ``Task Confounders". Based on these findings, we propose a plug-and-play Meta-learning Causal Representation Learner (MetaCRL) to eliminate task confounders. It encodes decoupled generating factors from multiple tasks and utilizes an invariant-based bi-level optimization mechanism to ensure their causality for meta-learning. Extensive experiments on various benchmark datasets demonstrate that our work achieves state-of-the-art (SOTA) performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.05771">https://arxiv.org/abs/2312.05771</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to be primarily about causal representation learning, specifically a technique called Meta-learning Causal Representation Learner (MetaCRL) for the purpose of eliminating confounding factors in meta-learning. This matches your interest in causal representation learning under the causality theme.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.05836" target="_blank">Can Large Language Models Infer Causation from Correlation?</a></h3>
            <a href="https://arxiv.org/html/2306.05836v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.05836v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Sch\"olkopf</p>
            <p><strong>Summary:</strong> arXiv:2306.05836v3 Announce Type: replace-cross 
Abstract: Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.05836">https://arxiv.org/abs/2306.05836</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning, specifically the subtopic of using large language models in causal discovery. It presents a new benchmark dataset for testing the causal inference skills of large language models, which can enhance your understanding in this area.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11018" target="_blank">Many-Shot In-Context Learning</a></h3>
            <a href="https://arxiv.org/html/2404.11018v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11018v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle</p>
            <p><strong>Summary:</strong> arXiv:2404.11018v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11018">https://arxiv.org/abs/2404.11018</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it explores large language models (LLMs) used in various tasks in the many-shot in-context learning regime. This may have implications for using LLMs for controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11049" target="_blank">Stepwise Alignment for Constrained Language Model Policy Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.11049v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11049v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Akifumi Wachi, Thien Q Tran, Rei Sato, Takumi Tanabe, Yohei Akimoto</p>
            <p><strong>Summary:</strong> arXiv:2404.11049v1 Announce Type: new 
Abstract: Safety and trustworthiness are indispensable requirements for applying AI systems based on large language models (LLMs) in real-world applications. This paper formulates a human value alignment as a language model policy optimization problem to maximize reward under a safety constraint and then proposes an algorithm called Stepwise Alignment for Constrained Policy Optimization (SACPO). A key idea behind SACPO, supported by theory, is that the optimal policy incorporating both reward and safety can be directly obtained from a reward-aligned policy. Based on this key idea, SACPO aligns the LLMs with each metric step-wise while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO provides many benefits such as simplicity, stability, computational efficiency, and flexibility regarding algorithms and dataset selection. Under mild assumption, our theoretical analysis provides the upper bounds regarding near-optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11049">https://arxiv.org/abs/2404.11049</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it discusses the application of Large Language Models (LLMs) for real-world applications with a focus on human value alignment, a crucial aspect of controlling software and automating processes. The proposed model, SACPO, optimizes the use of LLMs in a safe and efficient way, which is in line with your interest in using large language models for control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10779" target="_blank">Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations</a></h3>
            <a href="https://arxiv.org/html/2404.10779v1/extracted/5457444/text_finetune.jpg" target="_blank"><img src="https://arxiv.org/html/2404.10779v1/extracted/5457444/text_finetune.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mathav Raj J, Kushala VM, Harikrishna Warrier, Yogesh Gupta</p>
            <p><strong>Summary:</strong> arXiv:2404.10779v1 Announce Type: cross 
Abstract: There is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) o get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be ine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10779">https://arxiv.org/abs/2404.10779</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to you because it discusses how to fine-tune Large Language Models (LLMs) incorporating domain-specific knowledge. While it does not directly deal with control tasks like web browser or software control, the focus on practical aspects of fine-tuning such models might help in developing LLM based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10830" target="_blank">Fewer Truncations Improve Language Modeling</a></h3>
            <a href="https://arxiv.org/html/2404.10830v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10830v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, Stefano Soatto</p>
            <p><strong>Summary:</strong> arXiv:2404.10830v1 Announce Type: cross 
Abstract: In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10830">https://arxiv.org/abs/2404.10830</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't directly address agent-based approaches with large language models, it does present an improvement to large language model training that could indirectly enhance the abilities of large language model based agents, specifically in terms of generating more coherent and consistent output.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10859" target="_blank">Forcing Diffuse Distributions out of Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.10859v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10859v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yiming Zhang, Avi Schwarzschild, Nicholas Carlini, Zico Kolter, Daphne Ippolito</p>
            <p><strong>Summary:</strong> arXiv:2404.10859v1 Announce Type: cross 
Abstract: Despite being trained specifically to follow user instructions, today's language models perform poorly when instructed to produce random outputs. For example, when prompted to pick a number uniformly between one and ten Llama-2-13B-chat disproportionately favors the number five, and when tasked with picking a first name at random, Mistral-7B-Instruct chooses Avery 40 times more often than we would expect based on the U.S. population. When these language models are used for real-world tasks where diversity of outputs is crucial, such as language model assisted dataset construction, their inability to produce diffuse distributions over valid choices is a major hurdle. In this work, we propose a fine-tuning method that encourages language models to output distributions that are diffuse over valid outcomes. The methods we introduce generalize across a variety of tasks and distributions and make large language models practical for synthetic dataset generation with little human intervention.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10859">https://arxiv.org/abs/2404.10859</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper fits under your interest in agents based on large-language models. It presents a new method for fine-tuning these models to enhance their performance in real-world tasks - specifically, synthetic dataset generation. While it does not specifically cover controlling software or web browsers, the discussed improvements to language model behavior could have broad applications, including in the areas you mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10933" target="_blank">LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.10933v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10933v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta, Seyeon Kim, Yongin Kwon, Sangtae Ha</p>
            <p><strong>Summary:</strong> arXiv:2404.10933v1 Announce Type: cross 
Abstract: Fine-tuning pre-trained large language models (LLMs) with limited hardware presents challenges due to GPU memory constraints. Various distributed fine-tuning methods have been proposed to alleviate memory constraints on GPU. However, determining the most effective method for achieving rapid fine-tuning while preventing GPU out-of-memory issues in a given environment remains unclear. To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed fine-tuning methods across multiple GPUs and identifies the optimal method. We conduct GPU memory usage estimation prior to fine-tuning, leveraging the fundamental structure of transformer-based decoder models and the memory usage distribution of each method. Experimental results show that LLMem accurately estimates peak GPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally, it shows an average error rate of 3.0% when applying distributed fine-tuning methods to LLMs with more than a billion parameters on multi-GPU setups.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10933">https://arxiv.org/abs/2404.10933</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your topic 'Agents based on large-language models'. It discusses how large language models (LLMs) can be fine-tuned efficiently which is closely related to the concept of using LLMs for computer automation. However, it does not directly discuss controlling software or web browsers hence it's given a score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11041" target="_blank">On the Empirical Complexity of Reasoning and Planning in LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.11041v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11041v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Liwei Kang, Zirui Zhao, David Hsu, Wee Sun Lee</p>
            <p><strong>Summary:</strong> arXiv:2404.11041v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) work surprisingly well for some complex reasoning problems via chain-of-thought (CoT) or tree-of-thought (ToT), but the underlying reasons remain unclear. We seek to understand the performance of these methods by conducting experimental case studies and linking the outcomes to sample and computational complexity in machine learning. We found that if problems can be decomposed into a sequence of reasoning steps and learning to predict the next step has a low sample and computational complexity, explicitly outlining the reasoning chain with all necessary information for predicting the next step may improve performance. Conversely, for problems where predicting the next step is computationally hard, adopting ToT may yield better reasoning outcomes than attempting to formulate a short reasoning chain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11041">https://arxiv.org/abs/2404.11041</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the intricacies of Large Language Models (LLMs) in the context of complex reasoning and planning. Although it doesn't directly mention specific applications such as controlling software or web browsers, or computer automation, the understanding of how LLMs handle complex reasoning problems might aid in these areas. Thus, it could be an insightful read if you're interested in delving into the mechanics of LLMs before diving into their applications in controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11207" target="_blank">Exploring the Transferability of Visual Prompting for Multimodal Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.11207v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11207v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, Jun Zhu</p>
            <p><strong>Summary:</strong> arXiv:2404.11207v1 Announce Type: cross 
Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11207">https://arxiv.org/abs/2404.11207</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the method of Transferable Visual Prompting in the context of Multimodal Large Language Models (MLLMs), which might be useful for controlling software or automation using large language models. However it doesn't directly tackle any of the subtopics mentioned, hence the score isn't the highest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11216" target="_blank">Position Engineering: Boosting Large Language Models through Positional Information Manipulation</a></h3>
            
            <p><strong>Authors:</strong> Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu</p>
            <p><strong>Summary:</strong> arXiv:2404.11216v1 Announce Type: cross 
Abstract: The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11216">https://arxiv.org/abs/2404.11216</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces a new method called 'position engineering' to guide large language models, which aligns with your interest in agents based on large-language models. Furthermore, its investigation on retrieval-augmented generation and in-context learning could offer valuable insights for your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11449" target="_blank">AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large Language Models for Extracting Cognitive Pathways from Social Media Texts</a></h3>
            <a href="https://arxiv.org/html/2404.11449v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.11449v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Meng Jiang, Yi Jing Yu, Qing Zhao, Jianqiang Li, Changwei Song, Hongzhi Qi, Wei Zhai, Dan Luo, Xiaoqin Wang, Guanghui Fu, Bing Xiang Yang</p>
            <p><strong>Summary:</strong> arXiv:2404.11449v1 Announce Type: cross 
Abstract: Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care. In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases. Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online. In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework. We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories. Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information. Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks. The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task. Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance. However, it may suffer from an issue of hallucination. We have made all models and codes publicly available to support further research in this field.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11449">https://arxiv.org/abs/2404.11449</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper explores the use of Large Language Models (LLMs) in Cognitive Behavioral Therapy, a form of mental health treatment mediated via social media, which bears relevance to your interest in computer automation using large language models. However, the context does not entirely align with web browsers or software control, hence the score of 4 not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11483" target="_blank">AgentKit: Flow Engineering with Graphs, not Coding</a></h3>
            <a href="https://arxiv.org/html/2404.11483v1/extracted/5466767/figures/teaser_new.png" target="_blank"><img src="https://arxiv.org/html/2404.11483v1/extracted/5466767/figures/teaser_new.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen McAleer, Yonatan Bisk, Ruslan Salakhutdinov, Yuanzhi Li, Tom Mitchell</p>
            <p><strong>Summary:</strong> arXiv:2404.11483v1 Announce Type: cross 
Abstract: We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex "thought process" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured "thought process". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. https://github.com/holmeswww/AgentKit</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11483">https://arxiv.org/abs/2404.11483</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper introduces AgentKit, a framework that constructs intuitive and structured thought processes for agents powered by large language models which is very applicable to your interest in using large language models for computer automation. The agents designed through AgentKit have shown state-of-the-art performance in web control tasks although it's not specifically focused on web browsers control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.08358" target="_blank">Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF</a></h3>
            <a href="https://arxiv.org/html/2312.08358v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.08358v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell</p>
            <p><strong>Summary:</strong> arXiv:2312.08358v2 Announce Type: replace 
Abstract: In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.08358">https://arxiv.org/abs/2312.08358</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models.' It deals with the topic of reinforcement learning from human feedback (RLHF) for Large Language Model (LLM) chatbots and proposes Distributional Preference Learning methods which might help in further automation and control tasks using large language models. However, it doesn't directly pertain to software or web browser control using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14608" target="_blank">Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</a></h3>
            <a href="https://arxiv.org/html/2403.14608v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.14608v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.14608v3 Announce Type: replace 
Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14608">https://arxiv.org/abs/2403.14608</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a comprehensive survey of Parameter-Efficient Fine-Tuning (PEFT) algorithms for large scale models, like large language models, which are invaluable resources for agent-based systems. While it doesn't explicitly mention control of software or web browsers, it offers valuable insights into how to efficiently leverage large language models in agent-based systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.20208" target="_blank">Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science</a></h3>
            <a href="https://arxiv.org/html/2403.20208v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.20208v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu</p>
            <p><strong>Summary:</strong> arXiv:2403.20208v5 Announce Type: replace 
Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.20208">https://arxiv.org/abs/2403.20208</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interests in large-language models (LLMs) and their applications. Although it doesn't specifically mention web browsers or software control, it does detail the application of LLMs in predictive tasks with structured tabular data - a form of automation. Plus, it discusses a method for training LLMs more effectively, which could potentially be generalized to other tasks as well.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.15098" target="_blank">Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models</a></h3>
            <a href="https://arxiv.org/html/2309.15098v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.15098v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, Besmira Nushi</p>
            <p><strong>Summary:</strong> arXiv:2309.15098v2 Announce Type: replace-cross 
Abstract: We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.15098">https://arxiv.org/abs/2309.15098</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is focused on the behaviour of Large Language Models (LLMs) thereby aligning with your interest in 'Agents based on large language models'. It's centered on Transformers, which are a crucial part of many LLMs, and it discusses the enhancement of their reliability. However, its main focus is on error prediction rather than control of software or browsers, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.03031" target="_blank">How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses</a></h3>
            <a href="https://arxiv.org/html/2310.03031v2/extracted/5542008/figures/prof_prize_female.png" target="_blank"><img src="https://arxiv.org/html/2310.03031v2/extracted/5542008/figures/prof_prize_female.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Stefanie Urchs, Veronika Thurner, Matthias A{\ss}enmacher, Christian Heumann, Stephanie Thiemichen</p>
            <p><strong>Summary:</strong> arXiv:2310.03031v2 Announce Type: replace-cross 
Abstract: With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to thoroughly check the system's responses for biases as well as for syntactic and grammatical mistakes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.03031">https://arxiv.org/abs/2310.03031</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it explores biases in large language models like ChatGPT, which can be informative for the development and control of Agents based on large-language models. However, it focuses more on an analysis of existing systems, rather than proposing new methods for controlling software or automating processes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.14972" target="_blank">Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's LLM with Open Source SLMs in Production</a></h3>
            <a href="https://arxiv.org/html/2312.14972v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.14972v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chandra Irugalbandara, Ashish Mahendra, Roland Daynauth, Tharuka Kasthuri Arachchige, Jayanaka Dantanarayana, Krisztian Flautner, Lingjia Tang, Yiping Kang, Jason Mars</p>
            <p><strong>Summary:</strong> arXiv:2312.14972v3 Announce Type: replace-cross 
Abstract: Many companies use large language models (LLMs) offered as a service, like OpenAI's GPT-4, to create AI-enabled product experiences. Along with the benefits of ease-of-use and shortened time-to-solution, this reliance on proprietary services has downsides in model control, performance reliability, uptime predictability, and cost. At the same time, a flurry of open-source small language models (SLMs) has been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to holistically evaluate these SLMs is not readily available. This paper presents a systematic evaluation methodology and a characterization of modern open-source SLMs and their trade-offs when replacing proprietary LLMs for a real-world product feature. We have designed SLaM, an open-source automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine the quality and performance characteristics of modern SLMs relative to an existing customer-facing implementation using the OpenAI GPT-4 API. Across 9 SLMs and their 29 variants, we observe that SLMs provide competitive results, significant performance consistency improvements, and a cost reduction of 5x~29x when compared to GPT-4.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.14972">https://arxiv.org/abs/2312.14972</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the utilization of open-source small language models (SLMs) as opposed to the proprietary large language models (LLMs) like OpenAI's GPT-4. The discussion includes an analysis of the performance characteristics, quality, and cost efficiency of using SLMs in a real-world product application, making it relevant to your interest in 'Agents based on large-language models'. However, it doesn't specify using the model for software or browser control which is specifically mentioned in your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04437" target="_blank">Structured Entity Extraction Using Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu, James Hensman, Bhaskar Mitra</p>
            <p><strong>Summary:</strong> arXiv:2402.04437v3 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. Prior works typically represent information extraction as triplet-centric and use classical metrics such as precision and recall for evaluation. We reformulate the task to be entity-centric, enabling the use of diverse metrics that can provide more insights from various perspectives. We contribute to the field by introducing Structured Entity Extraction (SEE) and proposing the Approximate Entity Set OverlaP (AESOP) metric, designed to appropriately assess model performance. Later, we introduce a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency by decomposing the extraction task into multiple stages. Quantitative and human side-by-side evaluations confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04437">https://arxiv.org/abs/2402.04437</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your category of 'Agents based on large-language models' as it discusses the usage of Large Language Models (LLMs) in extracting structured information from unstructured text. However, it does not explicitly mention controlling software or web browsers or computer automation, which are your specific interests. Thus, it's relevance is not at the highest level.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09906" target="_blank">Generative Representational Instruction Tuning</a></h3>
            <a href="https://arxiv.org/html/2402.09906v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.09906v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</p>
            <p><strong>Summary:</strong> arXiv:2402.09906v2 Announce Type: replace-cross 
Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09906">https://arxiv.org/abs/2402.09906</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the application of large language models for both generative and embedding tasks via GRIT, offering potentially new methods that can be used for controlling software or automating processes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12219" target="_blank">Reformatted Alignment</a></h3>
            <a href="https://arxiv.org/html/2402.12219v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12219v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu</p>
            <p><strong>Summary:</strong> arXiv:2402.12219v2 Announce Type: replace-cross 
Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.
  Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12219">https://arxiv.org/abs/2402.12219</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLMs) as it focuses on the alignment and controlling ability of LLMs consistent with human values. It does not specifically mention software or web browser control but the overall content is related to your interest in agents based on LLMs. The work even showcases significant performance improvements in LLMs' abilities.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14301" target="_blank">GenSERP: Large Language Models for Whole Page Presentation</a></h3>
            
            <p><strong>Authors:</strong> Zhenning Zhang, Yunan Zhang, Suyu Ge, Guangwei Weng, Mridu Narang, Xia Song, Saurabh Tiwary</p>
            <p><strong>Summary:</strong> arXiv:2402.14301v2 Announce Type: replace-cross 
Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the page to each item, along with the UX display details. (3) A scoring phase where an LLM with vision scores all the generated SERPs based on how likely it can satisfy the user. It then send the one with highest score to rendering. GenSERP features two generation paradigms. First, coarse-to-fine, which allow it to approach optimal layout in a more manageable way, (2) beam search, which give it a better chance to hit the optimal solution compared to greedy decoding. Offline experimental results on real-world data demonstrate how LLMs can contextually organize heterogeneous search results on-the-fly and provide a promising user experience.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14301">https://arxiv.org/abs/2402.14301</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the research does not directly propose a new method, it does discuss a framework (GenSERP) which leverages Large Language Models for organizing search engine result pages. The LLM's application in software control, specifically in dynamically organizing and optimizing the ranking of search results, matches your interests in 'using large language models to control software'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19708" target="_blank">AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving</a></h3>
            <a href="https://arxiv.org/html/2403.19708v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.19708v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo</p>
            <p><strong>Summary:</strong> arXiv:2403.19708v2 Announce Type: replace-cross 
Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, AttentionStore employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, AttentionStore enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that AttentionStore significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by 7.8$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%. For long sequence inference, AttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling throughput by 22$\times$.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19708">https://arxiv.org/abs/2403.19708</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the mechanisms of large language models (LLMs) in multi-turn conversations, detailing on improvements to efficiency and cost-effectiveness. It strongly relates to your interest in large language models, particularly in context of their use in software handling and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.03441" target="_blank">Benchmarking ChatGPT on Algorithmic Reasoning</a></h3>
            <a href="https://arxiv.org/html/2404.03441v2/extracted/5540782/figures/Test_compare_F1.png" target="_blank"><img src="https://arxiv.org/html/2404.03441v2/extracted/5540782/figures/Test_compare_F1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sean McLeish, Avi Schwarzschild, Tom Goldstein</p>
            <p><strong>Summary:</strong> arXiv:2404.03441v2 Announce Type: replace-cross 
Abstract: We evaluate ChatGPT's ability to solve algorithm problems from the CLRS benchmark suite that is designed for GNNs. The benchmark requires the use of a specified classical algorithm to solve a given problem. We find that ChatGPT outperforms specialist GNN models, using Python to successfully solve these problems. This raises new points in the discussion about learning algorithms with neural networks and how we think about what out of distribution testing looks like with web scale training data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.03441">https://arxiv.org/abs/2404.03441</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, titled 'Benchmarking ChatGPT on Algorithmic Reasoning' is relevant to your interests in large-language model agents. Although it doesn't directly mention tools to control software or web browsers, it illustrates the potential of large language models like ChatGPT for complex tasks including algorithm problems, which reflects on the ability of such models to be used in automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.07922" target="_blank">LaVy: Vietnamese Multimodal Large Language Model</a></h3>
            <a href="https://arxiv.org/html/2404.07922v4/extracted/5541435/images/2.jpg" target="_blank"><img src="https://arxiv.org/html/2404.07922v4/extracted/5541435/images/2.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chi Tran, Huong Le Thanh</p>
            <p><strong>Summary:</strong> arXiv:2404.07922v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. Our project is public at https://github.com/baochi0212/LaVy</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.07922">https://arxiv.org/abs/2404.07922</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant because it introduces a new kind of multimodal large language model (LaVy), which potentially could be used in computer automation or software/web browser control, even though its specific application is not detailed in the description.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.10179" target="_blank">Scaling Instructable Agents Across Many Simulated Worlds</a></h3>
            <a href="https://arxiv.org/html/2404.10179v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.10179v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong>  SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, Nick Young</p>
            <p><strong>Summary:</strong> arXiv:2404.10179v2 Announce Type: replace-cross 
Abstract: Building embodied AI systems that can follow arbitrary language instructions in any 3D environment is a key challenge for creating general AI. Accomplishing this goal requires learning to ground language in perception and embodied actions, in order to accomplish complex tasks. The Scalable, Instructable, Multiworld Agent (SIMA) project tackles this by training agents to follow free-form instructions across a diverse range of virtual 3D environments, including curated research environments as well as open-ended, commercial video games. Our goal is to develop an instructable agent that can accomplish anything a human can do in any simulated 3D environment. Our approach focuses on language-driven generality while imposing minimal assumptions. Our agents interact with environments in real-time using a generic, human-like interface: the inputs are image observations and language instructions and the outputs are keyboard-and-mouse actions. This general approach is challenging, but it allows agents to ground language across many visually complex and semantically rich environments while also allowing us to readily run agents in new environments. In this paper we describe our motivation and goal, the initial progress we have made, and promising preliminary results on several diverse research environments and a variety of commercial video games.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.10179">https://arxiv.org/abs/2404.10179</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper highly corresponds to your interest in 'Agents based on large-language models'. It specifically touches on using language instructions to guide agents in simulated 3D environments, a topic closely related to computer automation using large language models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11117" target="_blank">Variational quantization for state space models</a></h3>
            <a href="https://arxiv.org/html/2404.11117v1/extracted/5541725/next_framework_part2_v2.png" target="_blank"><img src="https://arxiv.org/html/2404.11117v1/extracted/5541725/next_framework_part2_v2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Etienne David (IP Paris, ISTeC-SAMOVAR), Jean Bellot (LPSM), Sylvain Le Corff (LPSM)</p>
            <p><strong>Summary:</strong> arXiv:2404.11117v1 Announce Type: new 
Abstract: Forecasting tasks using large datasets gathering thousands of heterogeneous time series is a crucial statistical problem in numerous sectors. The main challenge is to model a rich variety of time series, leverage any available external signals and provide sharp predictions with statistical guarantees. In this work, we propose a new forecasting model that combines discrete state space hidden Markov models with recent neural network architectures and training procedures inspired by vector quantized variational autoencoders. We introduce a variational discrete posterior distribution of the latent states given the observations and a two-stage training procedure to alternatively train the parameters of the latent states and of the emission distributions. By learning a collection of emission laws and temporarily activating them depending on the hidden process dynamics, the proposed method allows to explore large datasets and leverage available external signals. We assess the performance of the proposed method using several datasets and show that it outperforms other state-of-the-art solutions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11117">https://arxiv.org/abs/2404.11117</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new forecasting model, which combines elements of both deep learning and time series analysis. Thus, it is relevant to your interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.11422" target="_blank">Short-term wind speed forecasting model based on an attention-gated recurrent neural network and error correction strategy</a></h3>
            
            <p><strong>Authors:</strong> Haojian Huang</p>
            <p><strong>Summary:</strong> arXiv:2404.11422v1 Announce Type: new 
Abstract: The accurate wind speed series forecast is very pivotal to security of grid dispatching and the application of wind power. Nevertheless, on account of their nonlinear and non-stationary nature, their short-term forecast is extremely challenging. Therefore, this dissertation raises one short-term wind speed forecast pattern on the foundation of attention with an improved gated recurrent neural network (AtGRU) and a tactic of error correction. That model uses the AtGRU model as the preliminary predictor and the GRU model as the error corrector. At the beginning, SSA (singular spectrum analysis) is employed in previous wind speed series for lessening the noise. Subsequently, historical wind speed series is going to be used for the predictor training. During this process, the prediction can have certain errors. The sequence of these errors processed by variational modal decomposition (VMD) is used to train the corrector of error. The eventual forecast consequence is just the sum of predictor forecast and error corrector. The proposed SSA-AtGRU-VMD-GRU model outperforms the compared models in three case studies on Woodburn, St. Thomas, and Santa Cruz. It is indicated that the model evidently enhances the correction of the wind speed forecast.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.11422">https://arxiv.org/abs/2404.11422</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests since it introduces a new deep learning model for time series forecasting, specifically for wind speed. However, it focuses more on an application scenario rather than just the method.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.00493" target="_blank">Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2307.00493v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.00493v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nhat Thanh Tran, Jack Xin</p>
            <p><strong>Summary:</strong> arXiv:2307.00493v3 Announce Type: replace 
Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention being local is a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 1.6 to 2 times. We also provide a mathematical definition of FWin attention, and prove that it is equivalent to the canonical full attention under the block diagonal invertibility (BDI) condition of the attention matrix. The BDI is shown experimentally to hold with high probability for typical benchmark datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.00493">https://arxiv.org/abs/2307.00493</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper focuses on a new deep learning method and a transformer-like model for time series forecasting, which aligns with your interests in new machine learning methods and transformer models for time series. However, it does not touch on foundational models or multimodal deep learning specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07793" target="_blank">GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.07793v5/extracted/5538518/figures/main_fig2.png" target="_blank"><img src="https://arxiv.org/html/2310.07793v5/extracted/5538518/figures/main_fig2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, Volker Tresp</p>
            <p><strong>Summary:</strong> arXiv:2310.07793v5 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. Code and data are released here: https://github.com/mayhugotong/GenTKG.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07793">https://arxiv.org/abs/2310.07793</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'New foundation models for time series' and 'New deep learning methods for time series'. It evaluates the application of Large Language Models in the context of making predictions on temporal knowledge graphs, with promising results. However, it doesn't fully cover the aspect of multimodality or transformer-like structures.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 18, 2024 at 21:36:42</div></body></html>