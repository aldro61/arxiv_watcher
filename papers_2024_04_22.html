
            <html>
            <head>
                <title>Report Generated on April 22, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 22, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.04997" target="_blank">Adapting LLMs for Efficient Context Processing through Soft Prompt Compression</a></h3>
            <a href="https://arxiv.org/html/2404.04997v2/extracted/5546007/Fig1.png" target="_blank"><img src="https://arxiv.org/html/2404.04997v2/extracted/5546007/Fig1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd</p>
            <p><strong>Summary:</strong> arXiv:2404.04997v2 Announce Type: replace 
Abstract: The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.04997">https://arxiv.org/abs/2404.04997</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper appears to be highly relevant to your interest in 'Agents based on large-language models'. It discusses a method to improve the efficiency of Large Language Models, which may have significant implications for tasks such as controlling software or automating computers. However, it does not directly focus on the latter subjects, hence the slightly reduced score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12535" target="_blank">HalluciBot: Is There No Such Thing as a Bad Question?</a></h3>
            <a href="https://arxiv.org/html/2404.12535v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.12535v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William Watson, Nicole Cho</p>
            <p><strong>Summary:</strong> arXiv:2404.12535v1 Announce Type: new 
Abstract: Hallucination continues to be one of the most critical challenges in the institutional adoption journey of Large Language Models (LLMs). In this context, an overwhelming number of studies have focused on analyzing the post-generation phase - refining outputs via feedback, analyzing logit output values, or deriving clues via the outputs' artifacts. We propose HalluciBot, a model that predicts the probability of hallucination $\textbf{before generation}$, for any query imposed to an LLM. In essence, HalluciBot does not invoke any generation during inference. To derive empirical evidence for HalluciBot, we employ a Multi-Agent Monte Carlo Simulation using a Query Perturbator to craft $n$ variations per query at train time. The construction of our Query Perturbator is motivated by our introduction of a new definition of hallucination - $\textit{truthful hallucination}$. Our training methodology generated 2,219,022 estimates for a training corpus of 369,837 queries, spanning 13 diverse datasets and 3 question-answering scenarios. HalluciBot predicts both binary and multi-class probabilities of hallucination, enabling a means to judge the query's quality with regards to its propensity to hallucinate. Therefore, HalluciBot paves the way to revise or cancel a query before generation and the ensuing computational waste. Moreover, it provides a lucid means to measure user accountability for hallucinatory queries.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12535">https://arxiv.org/abs/2404.12535</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest as it talks about the use of Large Language Models (LLMs) and proposes a new model, HalluciBot, which relates to the prevention of hallucination in generation in LLMs. While it might not be directly about controlling software or web browsers using LLMs, understanding how to prevent hallucination can be a critical aspect of ensuring that these LLM-controlled systems operate effectively and efficiently.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12843" target="_blank">Towards Logically Consistent Language Models via Probabilistic Reasoning</a></h3>
            <a href="https://arxiv.org/html/2404.12843v1/extracted/5547202/plus.png" target="_blank"><img src="https://arxiv.org/html/2404.12843v1/extracted/5547202/plus.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Diego Calanzone, Stefano Teso, Antonio Vergari</p>
            <p><strong>Summary:</strong> arXiv:2404.12843v1 Announce Type: new 
Abstract: Large language models (LLMs) are a promising venue for natural language understanding and generation tasks. However, current LLMs are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about beliefs of the world. These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools. In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules. Fine-tuning with our loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines and allows them to extrapolate to unseen but semantically similar factual knowledge more systematically.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12843">https://arxiv.org/abs/2404.12843</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses further optimizing large language models (LLMs) for more consistent reasoning, which falls under your interest in agent-based LLMs. The logical consistency aspect may have implications for managing LLM-controlled software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12457" target="_blank">RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation</a></h3>
            <a href="https://arxiv.org/html/2404.12457v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.12457v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, Xin Jin</p>
            <p><strong>Summary:</strong> arXiv:2404.12457v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose Thoth, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design Thoth, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. Thoth proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement Thoth and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that Thoth reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12457">https://arxiv.org/abs/2404.12457</a></p>
            <p><strong>Category:</strong> cs.DC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper doesn't directly deal with controlling software or web browsers using large language models, it does discuss improving the efficiency and performance of Retrieval-Augmented Generation (RAG) systems which use large language models. As such, the techniques proposed in the paper could potentially enhance the functionality of agent-based systems that employ large language models for controlling applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12803" target="_blank">TextSquare: Scaling up Text-Centric Visual Instruction Tuning</a></h3>
            <a href="https://arxiv.org/html/2404.12803v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.12803v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang</p>
            <p><strong>Summary:</strong> arXiv:2404.12803v1 Announce Type: cross 
Abstract: Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12803">https://arxiv.org/abs/2404.12803</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may be of interest as it discusses using Multimodal Large Language Models, which matches your interest in agents based on large-language models. Although it specifically does not focus on controlling software or web browsers, it does cover the concept of using these models for task-oriented operations, and could potentially be applicable to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12957" target="_blank">Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction</a></h3>
            <a href="https://arxiv.org/html/2404.12957v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.12957v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qinyuan Wu, Mohammad Aflah Khan, Soumi Das, Vedant Nanda, Bishwamittra Ghosh, Camila Kolling, Till Speicher, Laurent Bindschaedler, Krishna P. Gummadi, Evimaria Terzi</p>
            <p><strong>Summary:</strong> arXiv:2404.12957v1 Announce Type: cross 
Abstract: We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs). We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base. Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ICL-based knowledge estimation. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12957">https://arxiv.org/abs/2404.12957</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large language models' as it discusses how large language models, like OPT, Pythia, and others, can surface more of the latent knowledge embedded in them. The focus on knowledge extraction suggests potential directions for automation and controlling software. However, direct discussion on controlling software or web browsers is not explicitly mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.13043" target="_blank">Data Alignment for Zero-Shot Concept Generation in Dermatology AI</a></h3>
            <a href="https://arxiv.org/html/2404.13043v1/extracted/5547888/images/text.png" target="_blank"><img src="https://arxiv.org/html/2404.13043v1/extracted/5547888/images/text.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Soham Gadgil, Mahtab Bigverdi</p>
            <p><strong>Summary:</strong> arXiv:2404.13043v1 Announce Type: cross 
Abstract: AI in dermatology is evolving at a rapid pace but the major limitation to training trustworthy classifiers is the scarcity of data with ground-truth concept level labels, which are meta-labels semantically meaningful to humans. Foundation models like CLIP providing zero-shot capabilities can help alleviate this challenge by leveraging vast amounts of image-caption pairs available on the internet. CLIP can be fine-tuned using domain specific image-caption pairs to improve classification performance. However, CLIP's pre-training data is not well-aligned with the medical jargon that clinicians use to perform diagnoses. The development of large language models (LLMs) in recent years has led to the possibility of leveraging the expressive nature of these models to generate rich text. Our goal is to use these models to generate caption text that aligns well with both the clinical lexicon and with the natural human language used in CLIP's pre-training data. Starting with captions used for images in PubMed articles, we extend them by passing the raw captions through an LLM fine-tuned on the field's several textbooks. We find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.13043">https://arxiv.org/abs/2404.13043</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although not directly in the context of agents, this paper discusses the usage of large language models like GPT-3.5. It focuses on generating texts that align well with both the clinical lexicon and natural human language, which can be relevant to controlling software or automating tasks via large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.07553" target="_blank">Safe Reinforcement Learning with Free-form Natural Language Constraints and Pre-Trained Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.07553v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.07553v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xingzhou Lou, Junge Zhang, Ziyan Wang, Kaiqi Huang, Yali Du</p>
            <p><strong>Summary:</strong> arXiv:2401.07553v2 Announce Type: replace 
Abstract: Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents' comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.07553">https://arxiv.org/abs/2401.07553</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under the topic of 'Agents based on large-language models'. It discusses the use of pre-trained language models for reinforcement learning agents, which is similar to the subtopics you are interested in, such as 'using large language models to control software' and 'computer automation using large language models'. Although it doesn't specifically mention about controlling web browsers, it covers a significant area of your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.07876" target="_blank">Policy Improvement using Language Feedback Models</a></h3>
            <a href="https://arxiv.org/html/2402.07876v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.07876v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre C\^ot\'e</p>
            <p><strong>Summary:</strong> arXiv:2402.07876v4 Announce Type: replace 
Abstract: We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.07876">https://arxiv.org/abs/2402.07876</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the use of large language models (LLMs) in identifying desirable behaviour for imitation learning in instruction following tasks. It points to the efficiency of using LLMs for controlling actions in different languages grounding environments, thus aligning with your interest in using large language models for automation and software control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.16880" target="_blank">BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation</a></h3>
            <a href="https://arxiv.org/html/2402.16880v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.16880v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo</p>
            <p><strong>Summary:</strong> arXiv:2402.16880v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.16880">https://arxiv.org/abs/2402.16880</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not focus solely on controlling software or web browsers with large language models, it presents a novel method for pruning these models without hindering their performance. This innovation can be useful for operating large language models in a wide range of applications, including automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08707" target="_blank">Large Language Model Can Continue Evolving From Mistakes</a></h3>
            
            <p><strong>Authors:</strong> Haokun Zhao, Haixia Han, Jie Shi, Chengyu Du, Jiaqing Liang, Yanghua Xiao</p>
            <p><strong>Summary:</strong> arXiv:2404.08707v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate impressive performance in various downstream tasks. However, they may still generate incorrect responses in certain scenarios due to the knowledge deficiencies and the flawed pre-training data. Continual Learning (CL) is a commonly used method to address this issue. Traditional CL is task-oriented, using novel or factually accurate data to retrain LLMs from scratch. However, this method requires more task-related training data and incurs expensive training costs. To address this challenge, we propose the Continue Evolving from Mistakes (CEM) method, inspired by the 'summarize mistakes' learning skill, to achieve iterative refinement of LLMs. Specifically, the incorrect responses of LLMs indicate knowledge deficiencies related to the questions. Therefore, we collect corpora with these knowledge from multiple data sources and follow it up with iterative supplementary training for continuous, targeted knowledge updating and supplementation. Meanwhile, we developed two strategies to construct supplementary training sets to enhance the LLM's understanding of the corpus and prevent catastrophic forgetting. We conducted extensive experiments to validate the effectiveness of this CL method. In the best case, our method resulted in a 17.00\% improvement in the accuracy of the LLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08707">https://arxiv.org/abs/2404.08707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models as it discusses a method to refine these models, specifically by training them on data capturing their mistakes. While it does not directly deal with control of software or web browsers, its insights could be applied to those areas as improvements in language model accuracy could positively impact their efficacy in performing such tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.04205" target="_blank">Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</a></h3>
            <a href="https://arxiv.org/html/2311.04205v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.04205v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu</p>
            <p><strong>Summary:</strong> arXiv:2311.04205v2 Announce Type: replace-cross 
Abstract: Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.04205">https://arxiv.org/abs/2311.04205</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models as it presents a new method to improve their performance and effectively respond to prompts. However, it does not touch on controlling software or web browsers with large language models but could potentially be applicable in that context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.12871" target="_blank">An Embodied Generalist Agent in 3D World</a></h3>
            <a href="https://arxiv.org/html/2311.12871v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.12871v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang</p>
            <p><strong>Summary:</strong> arXiv:2311.12871v2 Announce Type: replace-cross 
Abstract: Leveraging massive knowledge and learning schemes from large language models (LLMs), recent machine learning models show notable successes in building generalist agents that exhibit the capability of general-purpose task solving in diverse domains, including natural language processing, computer vision, and robotics. However, a significant challenge remains as these models exhibit limited ability in understanding and interacting with the 3D world. We argue this limitation significantly hinders the current models from performing real-world tasks and further achieving general intelligence. To this end, we introduce an embodied multi-modal and multi-task generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our proposed agent, referred to as LEO, is trained with shared LLM-based model architectures, objectives, and weights in two stages: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning. To facilitate the training, we meticulously curate and generate an extensive dataset comprising object-level and scene-level multi-modal tasks with exceeding scale and complexity, necessitating a deep understanding of and interaction with the 3D world. Through rigorous experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, embodied navigation, and robotic manipulation. Our ablation results further provide valuable insights for the development of future embodied generalist agents.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.12871">https://arxiv.org/abs/2311.12871</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents an agent based on large language models (LLMs) called LEO, demonstrating its application in diverse tasks within a 3D environment. Although it doesn't specifically talk about control of software or web browsers, the fundamental methodology and results may be interesting for your research into LLM agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.00199" target="_blank">Improving Socratic Question Generation using Data Augmentation and Preference Optimization</a></h3>
            <a href="https://arxiv.org/html/2403.00199v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.00199v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nischal Ashok Kumar, Andrew Lan</p>
            <p><strong>Summary:</strong> arXiv:2403.00199v3 Announce Type: replace-cross 
Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.00199">https://arxiv.org/abs/2403.00199</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it involves the use of large language models to generate Socratic questions, thus making it a relevant work in the arena of automation using large language models. However, it does not focus specifically on control of software or web browsers, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.09611" target="_blank">MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a></h3>
            
            <p><strong>Authors:</strong> Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H\`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang</p>
            <p><strong>Summary:</strong> arXiv:2403.09611v4 Announce Type: replace-cross 
Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.09611">https://arxiv.org/abs/2403.09611</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't explicitly mention the control of software or web browsers by large language models, it discusses the construction of Multimodal Large Language Models (MLLMs) and their potential applications. This is still interesting as it could be used for agent control, based on the discussed 'in-context learning' and 'multi-image reasoning'. Therefore, this could be highly relevant if you are interested in the potential capabilities of large language models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.12416" target="_blank">Full Shot Predictions for the DIII-D Tokamak via Deep Recurrent Networks</a></h3>
            <a href="https://arxiv.org/html/2404.12416v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.12416v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ian Char, Youngseog Chung, Joseph Abbate, Egemen Kolemen, Jeff Schneider</p>
            <p><strong>Summary:</strong> arXiv:2404.12416v1 Announce Type: cross 
Abstract: Although tokamaks are one of the most promising devices for realizing nuclear fusion as an energy source, there are still key obstacles when it comes to understanding the dynamics of the plasma and controlling it. As such, it is crucial that high quality models are developed to assist in overcoming these obstacles. In this work, we take an entirely data driven approach to learn such a model. In particular, we use historical data from the DIII-D tokamak to train a deep recurrent network that is able to predict the full time evolution of plasma discharges (or "shots"). Following this, we investigate how different training and inference procedures affect the quality and calibration of the shot predictions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.12416">https://arxiv.org/abs/2404.12416</a></p>
            <p><strong>Category:</strong> physics.plasm-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses a data-driven approach for predicting the time evolution of plasma discharges, which relates to time series forecasting. However, it doesn't explicitly mention the development of new deep learning methods or transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.05927" target="_blank">Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals</a></h3>
            <a href="https://arxiv.org/html/2309.05927v2/extracted/5543695/imgs/motivation_anony.png" target="_blank"><img src="https://arxiv.org/html/2309.05927v2/extracted/5543695/imgs/motivation_anony.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ran Liu, Ellen L. Zippi, Hadi Pouransari, Chris Sandino, Jingping Nie, Hanlin Goh, Erdrin Azemi, Ali Moin</p>
            <p><strong>Summary:</strong> arXiv:2309.05927v2 Announce Type: replace 
Abstract: Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. We evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of $\uparrow$5.5% improvement in classification accuracy over the previous state-of-the-art. Furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. Code is available at https://github.com/apple/ml-famae .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.05927">https://arxiv.org/abs/2309.05927</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> While this paper does not explicitly reference time-series forecasting, it proposes a novel method, bioFAME, for handling multimodal biosignals via a frequency-aware transformer and autoencoder. There is a clear focus on deep learning methods for handling time-series data, thus it seems highly relevant to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07446" target="_blank">Position Paper: An Integrated Perspective on Data, Metrics, and Methodology for Deep Time-Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2310.07446v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.07446v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiawen Zhang, Xumeng Wen, Shun Zheng, Jia Li, Jiang Bian</p>
            <p><strong>Summary:</strong> arXiv:2310.07446v3 Announce Type: replace 
Abstract: Deep time-series forecasting plays an integral role in numerous practical applications. However, existing research fall short by focusing narrowly on either neural architecture designs for long-term point forecasts or probabilistic models for short-term scenarios. By proposing a comprehensive framework, facilitated by a novel tool, ProbTS, that integrates diverse data scenarios, evaluation metrics, and methodological focuses, we aim to transcend the limitations of current forecasting practices. Rigorous experimentation uncovers pivotal insights, including the supreme importance of aligning forecasting methodologies with the unique characteristics of the data; the necessity of a broad spectrum of metrics for accurately assessing both point and distributional forecasts; and the challenges inherent in adapting existing forecasting methods to a wider range of scenarios. These findings not only challenge conventional approaches but also illuminate promising avenues for future research, suggesting a more nuanced and effective strategy for advancing the field of deep time-series forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07446">https://arxiv.org/abs/2310.07446</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'New deep learning methods for time series' and 'New foundation models for time series'. It proposes a comprehensive framework for deep time-series forecasting and discusses the importance of method alignment with data characteristics. ProbTS tool might be regarded as a new method. However, there is no specific mention about multimodal deep learning or transformer-like models, which lowers the score a bit.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.10308" target="_blank">Event-Based Contrastive Learning for Medical Time Series</a></h3>
            <a href="https://arxiv.org/html/2312.10308v3/extracted/5518787/Images/EBCL.png" target="_blank"><img src="https://arxiv.org/html/2312.10308v3/extracted/5518787/Images/EBCL.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hyewon Jeong, Nassim Oufattole, Matthew Mcdermott, Aparna Balagopalan, Bryan Jangeesingh, Marzyeh Ghassemi, Collin Stultz</p>
            <p><strong>Summary:</strong> arXiv:2312.10308v3 Announce Type: replace 
Abstract: In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event. For example, quantifying the risk of adverse outcomes after an acute cardiovascular event helps healthcare providers identify those patients at the highest risk of poor outcomes; i.e., patients who benefit from invasive therapies that can lower their risk. Assessing the risk of adverse outcomes, however, is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL) - a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL can be used to construct models that yield improved performance on important downstream tasks relative to other pretraining methods. We develop and test the method using a cohort of heart failure patients obtained from a large hospital network and the publicly available MIMIC-IV dataset consisting of patients in an intensive care unit at a large tertiary care center. On both cohorts, EBCL pretraining yields models that are performant with respect to a number of downstream tasks, including mortality, hospital readmission, and length of stay. In addition, unsupervised EBCL embeddings effectively cluster heart failure patients into subgroups with distinct outcomes, thereby providing information that helps identify new heart failure phenotypes. The contrastive framework around the index event can be adapted to a wide array of time-series datasets and provides information that can be used to guide personalized care.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.10308">https://arxiv.org/abs/2312.10308</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper introduces a new method, Event-Based Contrastive Learning for handling time series data in a health context. Therefore, it aligns with your interest in new methods for time series analysis.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 22, 2024 at 21:36:23</div></body></html>