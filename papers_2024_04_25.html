
            <html>
            <head>
                <title>Report Generated on April 25, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for April 25, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15760" target="_blank">Debiasing Machine Unlearning with Counterfactual Examples</a></h3>
            <a href="https://arxiv.org/html/2404.15760v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15760v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ziheng Chen, Jia Wang, Jun Zhuang, Abbavaram Gowtham Reddy, Fabrizio Silvestri, Jin Huang, Kaushiki Nag, Kun Kuang, Xin Ning, Gabriele Tolomei</p>
            <p><strong>Summary:</strong> arXiv:2404.15760v1 Announce Type: new 
Abstract: The right to be forgotten (RTBF) seeks to safeguard individuals from the enduring effects of their historical actions by implementing machine-learning techniques. These techniques facilitate the deletion of previously acquired knowledge without requiring extensive model retraining. However, they often overlook a critical issue: unlearning processes bias. This bias emerges from two main sources: (1) data-level bias, characterized by uneven data removal, and (2) algorithm-level bias, which leads to the contamination of the remaining dataset, thereby degrading model accuracy. In this work, we analyze the causal factors behind the unlearning process and mitigate biases at both data and algorithmic levels. Typically, we introduce an intervention-based approach, where knowledge to forget is erased with a debiased dataset. Besides, we guide the forgetting procedure by leveraging counterfactual examples, as they maintain semantic data consistency without hurting performance on the remaining dataset. Experimental results demonstrate that our method outperforms existing machine unlearning baselines on evaluation metrics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15760">https://arxiv.org/abs/2404.15760</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper introduces an intervention-based approach and leverages counterfactual examples, which fits your sub-interest in causal representation learning and causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15746" target="_blank">Collaborative Heterogeneous Causal Inference Beyond Meta-analysis</a></h3>
            
            <p><strong>Authors:</strong> Tianyu Guo, Sai Praneeth Karimireddy, Michael I. Jordan</p>
            <p><strong>Summary:</strong> arXiv:2404.15746v1 Announce Type: cross 
Abstract: Collaboration between different data centers is often challenged by heterogeneity across sites. To account for the heterogeneity, the state-of-the-art method is to re-weight the covariate distributions in each site to match the distribution of the target population. Nevertheless, this method could easily fail when a certain site couldn't cover the entire population. Moreover, it still relies on the concept of traditional meta-analysis after adjusting for the distribution shift.
  In this work, we propose a collaborative inverse propensity score weighting estimator for causal inference with heterogeneous data. Instead of adjusting the distribution shift separately, we use weighted propensity score models to collaboratively adjust for the distribution shift. Our method shows significant improvements over the methods based on meta-analysis when heterogeneity increases. To account for the vulnerable density estimation, we further discuss the double machine method and show the possibility of using nonparametric density estimation with d<8 and a flexible machine learning method to guarantee asymptotic normality. We propose a federated learning algorithm to collaboratively train the outcome model while preserving privacy. Using synthetic and real datasets, we demonstrate the advantages of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15746">https://arxiv.org/abs/2404.15746</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper deals with causal inference with heterogeneous data, falling under the subtopic of 'Causal discovery'. It also uses machine learning methods, making it relevant to the topic of 'Causality and machine learning'. However, it does not seem to involve large language models, therefore it might not fully satisfy that particular subtopic interest.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15772" target="_blank">Bi-Mamba4TS: Bidirectional Mamba for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2404.15772v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15772v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aobo Liang, Xingguo Jiang, Yan Sun, Chang Lu</p>
            <p><strong>Summary:</strong> arXiv:2404.15772v1 Announce Type: new 
Abstract: Long-term time series forecasting (LTSF) provides longer insights into future trends and patterns. In recent years, deep learning models especially Transformers have achieved advanced performance in LTSF tasks. However, the quadratic complexity of Transformers rises the challenge of balancing computaional efficiency and predicting performance. Recently, a new state space model (SSM) named Mamba is proposed. With the selective capability on input data and the hardware-aware parallel computing algorithm, Mamba can well capture long-term dependencies while maintaining linear computational complexity. Mamba has shown great ability for long sequence modeling and is a potential competitor to Transformer-based models in LTSF. In this paper, we propose Bi-Mamba4TS, a bidirectional Mamba for time series forecasting. To address the sparsity of time series semantics, we adopt the patching technique to enrich the local information while capturing the evolutionary patterns of time series in a finer granularity. To select more appropriate modeling method based on the characteristics of the dataset, our model unifies the channel-independent and channel-mixing tokenization strategies and uses a series-relation-aware decider to control the strategy choosing process. Extensive experiments on seven real-world datasets show that our model achieves more accurate predictions compared with state-of-the-art methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15772">https://arxiv.org/abs/2404.15772</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'New deep learning methods for time series' and 'New transformer-like models for time series'. It introduces a model called Bi-Mamba4TS, which is a bidirectional Mamba for long-term time series forecasting, a potential competitor to Transformer-based models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.12667" target="_blank">TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers</a></h3>
            <a href="https://arxiv.org/html/2307.12667v2/extracted/5557209/fig/TransFusion3.png" target="_blank"><img src="https://arxiv.org/html/2307.12667v2/extracted/5557209/fig/TransFusion3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Md Fahim Sikder, Resmi Ramachandranpillai, Fredrik Heintz</p>
            <p><strong>Summary:</strong> arXiv:2307.12667v2 Announce Type: replace 
Abstract: The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion, a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We have stretched the sequence length to 384, and generated high-quality synthetic data. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. We evaluate TransFusion with a wide variety of visual and empirical metrics, and TransFusion outperforms the previous state-of-the-art by a significant margin.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.12667">https://arxiv.org/abs/2307.12667</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interests as it introduces 'TransFusion', a new transformer and diffusion-based generative model for generating long-sequence time-series data, which aligns with your interest in deep learning methods and transformer-like models for time-series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14081" target="_blank">Motion Code: Robust Time series Classification and Forecasting via Sparse Variational Multi-Stochastic Processes Learning</a></h3>
            <a href="https://arxiv.org/html/2402.14081v2/extracted/5555284/images/uneven_length_combine.png" target="_blank"><img src="https://arxiv.org/html/2402.14081v2/extracted/5555284/images/uneven_length_combine.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chandrajit Bajaj, Minh Nguyen</p>
            <p><strong>Summary:</strong> arXiv:2402.14081v2 Announce Type: replace 
Abstract: Despite being extensively studied, time series classification and forecasting on noisy data remain highly difficult. The main challenges lie in finding suitable mathematical concepts to describe time series and effectively separating noise from the true signals. Instead of treating time series as a static vector or a data sequence as often seen in previous methods, we introduce a novel framework that considers each time series, not necessarily of fixed length, as a sample realization of a continuous-time stochastic process. Such mathematical model explicitly captures the data dependence across several timestamps and detects the hidden time-dependent signals from noise. However, since the underlying data is often composed of several distinct dynamics, modeling using a single stochastic process is not sufficient. To handle such settings, we first assign each dynamics a signature vector. We then propose the abstract concept of the most informative timestamps to infer a sparse approximation of the individual dynamics based on their assigned vectors. The final model, referred to as Motion Code, contains parameters that can fully capture different underlying dynamics in an integrated manner. This allows unmixing classification and generation of specific sub-type forecasting simultaneously. Extensive experiments on sensors and devices noisy time series data demonstrate Motion Code's competitiveness against time series classification and forecasting benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14081">https://arxiv.org/abs/2402.14081</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it not only presents a new deep learning method for time series but also discusses the development of a more robust framework for time series classification and forecasting. Additionally, it includes experiments using sensor and device noisy datasets.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15899" target="_blank">ST-MambaSync: The Confluence of Mamba Structure and Spatio-Temporal Transformers for Precipitous Traffic Prediction</a></h3>
            <a href="https://arxiv.org/html/2404.15899v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15899v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiqi Shao, Xusheng Yao, Ze Wang, Junbin Gao</p>
            <p><strong>Summary:</strong> arXiv:2404.15899v1 Announce Type: new 
Abstract: Balancing accuracy with computational efficiency is paramount in machine learning, particularly when dealing with high-dimensional data, such as spatial-temporal datasets. This study introduces ST-MambaSync, an innovative framework that integrates a streamlined attention layer with a simplified state-space layer. The model achieves competitive accuracy in spatial-temporal prediction tasks. We delve into the relationship between attention mechanisms and the Mamba component, revealing that Mamba functions akin to attention within a residual network structure. This comparative analysis underpins the efficiency of state-space models, elucidating their capability to deliver superior performance at reduced computational costs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15899">https://arxiv.org/abs/2404.15899</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces ST-MambaSync, a new framework that presents a new approach for spatial-temporal prediction tasks which are closely related to time series forecasting. It also explores novel attention mechanisms which are similar to new transformer-like models for time series that you are interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15377" target="_blank">Fourier Series Guided Design of Quantum Convolutional Neural Networks for Enhanced Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2404.15377v1/extracted/5543927/VQC.png" target="_blank"><img src="https://arxiv.org/html/2404.15377v1/extracted/5543927/VQC.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sandra Leticia Ju\'arez Osorio, Mayra Alejandra Rivera Ruiz, Andres Mendez-Vazquez, Eduardo Rodriguez-Tello</p>
            <p><strong>Summary:</strong> arXiv:2404.15377v1 Announce Type: cross 
Abstract: In this study, we apply 1D quantum convolution to address the task of time series forecasting. By encoding multiple points into the quantum circuit to predict subsequent data, each point becomes a feature, transforming the problem into a multidimensional one. Building on theoretical foundations from prior research, which demonstrated that Variational Quantum Circuits (VQCs) can be expressed as multidimensional Fourier series, we explore the capabilities of different architectures and ansatz. This analysis considers the concepts of circuit expressibility and the presence of barren plateaus. Analyzing the problem within the framework of the Fourier series enabled the design of an architecture that incorporates data reuploading, resulting in enhanced performance. Rather than a strict requirement for the number of free parameters to exceed the degrees of freedom of the Fourier series, our findings suggest that even a limited number of parameters can produce Fourier functions of higher degrees. This highlights the remarkable expressive power of quantum circuits. This observation is also significant in reducing training times. The ansatz with greater expressibility and number of non-zero Fourier coefficients consistently delivers favorable results across different scenarios, with performance metrics improving as the number of qubits increases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15377">https://arxiv.org/abs/2404.15377</a></p>
            <p><strong>Category:</strong> quant-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it presents a novel approach to time series forecasting using quantum convolution. Although it does not specifically discuss deep learning, it offers a unique perspective on time series forecasting that may be of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15419" target="_blank">Using Deep Learning to Identify Initial Error Sensitivity of ENSO Forecasts</a></h3>
            
            <p><strong>Authors:</strong> Kinya Toride, Matthew Newman, Andrew Hoell, Antonietta Capotondi, Jak\"ob Schlor, Dillon Amaya</p>
            <p><strong>Summary:</strong> arXiv:2404.15419v1 Announce Type: cross 
Abstract: We introduce a hybrid method that integrates deep learning with model-analog forecasting, a straightforward yet effective approach that generates forecasts from similar initial climate states in a repository of model simulations. This hybrid framework employs a convolutional neural network to estimate state-dependent weights to identify analog states. The advantage of our method lies in its physical interpretability, offering insights into initial-error-sensitive regions through estimated weights and the ability to trace the physically-based temporal evolution of the system through analog forecasting. We evaluate our approach using the Community Earth System Model Version 2 Large Ensemble to forecast the El Ni\~no-Southern Oscillation (ENSO) on a seasonal-to-annual time scale. Results show a 10% improvement in forecasting sea surface temperature anomalies over the equatorial Pacific at 9-12 months leads compared to the traditional model-analog technique. Furthermore, our hybrid model demonstrates improvements in boreal winter and spring initialization when evaluated against a reanalysis dataset. Our deep learning-based approach reveals state-dependent sensitivity linked to various seasonally varying physical processes, including the Pacific Meridional Modes, equatorial recharge oscillator, and stochastic wind forcing. Notably, disparities emerge in the sensitivity associated with El Ni\~no and La Ni\~na events. We find that sea surface temperature over the tropical Pacific plays a more crucial role in El Ni\~no forecasting, while zonal wind stress over the same region exhibits greater significance in La Ni\~na prediction. This approach has broad implications for forecasting diverse climate phenomena, including regional temperature and precipitation, which are challenging for the traditional model-analog forecasting method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15419">https://arxiv.org/abs/2404.15419</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper should be relevant to your research as it presents a new method integrating deep learning and model-analog forecasting for time series forecasting. Even though it's applied in climate phenomena prediction, the methodology could still be useful for you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15552" target="_blank">Cross-Temporal Spectrogram Autoencoder (CTSAE): Unsupervised Dimensionality Reduction for Clustering Gravitational Wave Glitches</a></h3>
            <a href="https://arxiv.org/html/2404.15552v1/extracted/5540921/figure/figure_add.png" target="_blank"><img src="https://arxiv.org/html/2404.15552v1/extracted/5540921/figure/figure_add.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yi Li, Yunan Wu, Aggelos K. Katsaggelos</p>
            <p><strong>Summary:</strong> arXiv:2404.15552v1 Announce Type: cross 
Abstract: The advancement of The Laser Interferometer Gravitational-Wave Observatory (LIGO) has significantly enhanced the feasibility and reliability of gravitational wave detection. However, LIGO's high sensitivity makes it susceptible to transient noises known as glitches, which necessitate effective differentiation from real gravitational wave signals. Traditional approaches predominantly employ fully supervised or semi-supervised algorithms for the task of glitch classification and clustering. In the future task of identifying and classifying glitches across main and auxiliary channels, it is impractical to build a dataset with manually labeled ground-truth. In addition, the patterns of glitches can vary with time, generating new glitches without manual labels. In response to this challenge, we introduce the Cross-Temporal Spectrogram Autoencoder (CTSAE), a pioneering unsupervised method for the dimensionality reduction and clustering of gravitational wave glitches. CTSAE integrates a novel four-branch autoencoder with a hybrid of Convolutional Neural Networks (CNN) and Vision Transformers (ViT). To further extract features across multi-branches, we introduce a novel multi-branch fusion method using the CLS (Class) token. Our model, trained and evaluated on the GravitySpy O3 dataset on the main channel, demonstrates superior performance in clustering tasks when compared to state-of-the-art semi-supervised learning methods. To the best of our knowledge, CTSAE represents the first unsupervised approach tailored specifically for clustering LIGO data, marking a significant step forward in the field of gravitational wave research. The code of this paper is available at https://github.com/Zod-L/CTSAE</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15552">https://arxiv.org/abs/2404.15552</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Though this paper is primarily focused on gravitational wave datas, it introduces a new unsupervised method using a hybrid of Convolutional Neural Networks and Vision Transformers for dimensional reduction and clustering of time series data. This paper aligns with your interest in new transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.16556" target="_blank">LANISTR: Multimodal Learning from Structured and Unstructured Data</a></h3>
            <a href="https://arxiv.org/html/2305.16556v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.16556v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sayna Ebrahimi, Sercan O. Arik, Yihe Dong, Tomas Pfister</p>
            <p><strong>Summary:</strong> arXiv:2305.16556v3 Announce Type: replace 
Abstract: Multimodal large-scale pretraining has shown impressive performance for unstructured data such as language and image. However, a prevalent real-world scenario involves structured data types, tabular and time-series, along with unstructured data. Such scenarios have been understudied. To bridge this gap, we propose LANISTR, an attention-based framework to learn from LANguage, Image, and STRuctured data. The core of LANISTR's methodology is rooted in \textit{masking-based} training applied across both unimodal and multimodal levels. In particular, we introduce a new similarity-based multimodal masking loss that enables it to learn cross-modal relations from large-scale multimodal data with missing modalities. On two real-world datasets, MIMIC-IV (from healthcare) and Amazon Product Review (from retail), LANISTR demonstrates remarkable improvements, 6.6\% (in AUROC) and 14\% (in accuracy) when fine-tuned with 0.1\% and 0.01\% of labeled data, respectively, compared to the state-of-the-art alternatives. Notably, these improvements are observed even with very high ratio of samples (35.7\% and 99.8\% respectively) not containing all modalities, underlining the robustness of LANISTR to practical missing modality challenge. Our code and models will be available at https://github.com/google-research/lanistr</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.16556">https://arxiv.org/abs/2305.16556</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents LANISTR, a new multimodal learning method that handles both structured and unstructured data. It has a special focus on time-series data, which aligns with your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.08499" target="_blank">POCKET: Pruning Random Convolution Kernels for Time Series Classification from a Feature Selection Perspective</a></h3>
            
            <p><strong>Authors:</strong> Shaowu Chen, Weize Sun, Lei Huang, Xiaopeng Li, Qingyuan Wang, Deepu John</p>
            <p><strong>Summary:</strong> arXiv:2309.08499v3 Announce Type: replace 
Abstract: In recent years, two competitive time series classification models, namely, ROCKET and MINIROCKET, have garnered considerable attention due to their low training cost and high accuracy. However, they require a large number of random 1-D convolutional kernels to comprehensively capture features, which is incompatible with resource-constrained devices. Despite the development of heuristic algorithms designed to recognize and prune redundant kernels, the inherent time-consuming nature of evolutionary algorithms hinders efficient evaluation. To effectively prune models, this paper removes redundant random kernels from a feature selection perspective by eliminating associating connections in the sequential classifier. Two innovative algorithms are proposed, where the first ADMM-based algorithm formulates the pruning challenge as a group elastic net classification problem, and the second core algorithm named POCKET greatly accelerates the first one by bifurcating the problem into two sequential stages. Stage 1 of POCKET introduces dynamically varying penalties to efficiently implement group-level regularization to delete redundant kernels, and Stage 2 employs element-level regularization on the remaining features to refit a linear classifier for better performance. Experimental results on diverse time series datasets show that POCKET prunes up to 60% of kernels without a significant reduction in accuracy and performs 11 times faster than its counterparts. Our code is publicly available at https://github.com/ShaowuChen/POCKET.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.08499">https://arxiv.org/abs/2309.08499</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper details a new pruning method for time series classification models, which falls under your interest in new deep learning methods for time series. However, it mainly focuses on classification and not specifically on forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15317" target="_blank">Concept-Guided LLM Agents for Human-AI Safety Codesign</a></h3>
            <a href="https://arxiv.org/html/2404.15317v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15317v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Florian Geissler, Karsten Roscher, Mario Trapp</p>
            <p><strong>Summary:</strong> arXiv:2404.15317v1 Announce Type: cross 
Abstract: Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15317">https://arxiv.org/abs/2404.15317</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses using large language models (LLMs) in a novel way to ensure safety and introduces a novel LLM agent for safety purposes. It is particularly relevant because it is about using LLMs to guide software systems, which aligns with your interest in the applications of LLMs in controlling software and incorporating LLM-agents for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15778" target="_blank">BASS: Batched Attention-optimized Speculative Sampling</a></h3>
            <a href="https://arxiv.org/html/2404.15778v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15778v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras</p>
            <p><strong>Summary:</strong> arXiv:2404.15778v1 Announce Type: new 
Abstract: Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15778">https://arxiv.org/abs/2404.15778</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses an advance in large language models ('a 7.8B-size model') and generative AI applications, suggesting some relevance to agents based on large language models. However, it does not directly deal with controlling software, web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15804" target="_blank">GeckOpt: LLM System Efficiency via Intent-Based Tool Selection</a></h3>
            <a href="https://arxiv.org/html/2404.15804v1/extracted/5556352/geckopt-teaser.png" target="_blank"><img src="https://arxiv.org/html/2404.15804v1/extracted/5556352/geckopt-teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michael Fore, Simranjit Singh, Dimitrios Stamoulis</p>
            <p><strong>Summary:</strong> arXiv:2404.15804v1 Announce Type: new 
Abstract: In this preliminary study, we investigate a GPT-driven intent-based reasoning approach to streamline tool selection for large language models (LLMs) aimed at system efficiency. By identifying the intent behind user prompts at runtime, we narrow down the API toolset required for task execution, reducing token consumption by up to 24.6\%. Early results on a real-world, massively parallel Copilot platform with over 100 GPT-4-Turbo nodes show cost reductions and potential towards improving LLM-based system efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15804">https://arxiv.org/abs/2404.15804</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in 'Agents based on large-language models'. It discusses using GPT-driven intent-based reasoning to improve large language model (LLM) system efficiency, which relates to 'Using large language models to control software' and 'Computer automation using large language models'. However, it doesn't specifically mention controlling web browsers, so the score is 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15993" target="_blank">Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach</a></h3>
            <a href="https://arxiv.org/html/2404.15993v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15993v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen</p>
            <p><strong>Summary:</strong> arXiv:2404.15993v1 Announce Type: new 
Abstract: Large language models (LLMs) are highly capable of many tasks but they can sometimes generate unreliable or inaccurate outputs. To tackle this issue, this paper studies the problem of uncertainty estimation and calibration for LLMs. We begin by formulating the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden activations of the LLMs contain uncertainty information. Our designed approach effectively demonstrates the benefits of utilizing hidden activations for enhanced uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. Moreover, we distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. In practice, our method is easy to implement and is adaptable to different levels of model transparency including black box, grey box, and white box, each demonstrating strong performance based on the accessibility of the LLM's internal mechanisms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15993">https://arxiv.org/abs/2404.15993</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper provides concepts about the use of large language models (LLMs), focusing specifically on uncertainty estimation and calibration for LLMs. This is important for managing responses and improving the reliability of outputs, which can be crucial in applications like controlling software or web browsers using LLMs. However, it does not directly deal with the application of LLMs to control software or web browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.16014" target="_blank">Improving Dictionary Learning with Gated Sparse Autoencoders</a></h3>
            <a href="https://arxiv.org/html/2404.16014v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.16014v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, J\'anos Kram\'ar, Rohin Shah, Neel Nanda</p>
            <p><strong>Summary:</strong> arXiv:2404.16014v1 Announce Type: new 
Abstract: Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.16014">https://arxiv.org/abs/2404.16014</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses Gated Sparse Autoencoders, a novel approach that improves the learning of large language models. It could potentially be used in controlling software or computer automation, due to the emphasis on improving unsupervised discovery of interpretable features in language models' activations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.16032" target="_blank">Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts</a></h3>
            
            <p><strong>Authors:</strong> Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seong Joon Oh</p>
            <p><strong>Summary:</strong> arXiv:2404.16032v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, the model's knowledge can be updated from documents provided in context. This leads to cases of conflict between the model's parametric knowledge and the contextual information, where the model may not always update its knowledge. Previous work studied knowledge conflicts by creating synthetic documents that contradict the model's correct parametric answers. We present a framework for studying knowledge conflicts in a realistic setup. We update incorrect parametric knowledge using real conflicting documents. This reflects how knowledge conflicts arise in practice. In this realistic scenario, we find that knowledge updates fail less often than previously reported. In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail. These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors. Our code is available at https://github.com/kortukov/realistic_knowledge_conflicts/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.16032">https://arxiv.org/abs/2404.16032</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models especially in terms of knowledge conflicts and retrieval-augmented generation. This could be important to understand the behavior of LLMs when controlling software or web browsers, although it doesn't directly propose a new method.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15500" target="_blank">GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots</a></h3>
            <a href="https://arxiv.org/html/2404.15500v1/extracted/5555116/sec/figures/geollm-engine-camready.png" target="_blank"><img src="https://arxiv.org/html/2404.15500v1/extracted/5555116/sec/figures/geollm-engine-camready.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Simranjit Singh, Michael Fore, Dimitrios Stamoulis</p>
            <p><strong>Summary:</strong> arXiv:2404.15500v1 Announce Type: cross 
Abstract: Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions. However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios. In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms. We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent's proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions. By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images. By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15500">https://arxiv.org/abs/2404.15500</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses the development of GeoLLM-Engine, a tool-augmented agent that interprets natural language commands. It directly matches your interest in computer automation using large language models, particularly in regards to controlling software through natural language instructions.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15794" target="_blank">Large Language Models as In-context AI Generators for Quality-Diversity</a></h3>
            <a href="https://arxiv.org/html/2404.15794v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15794v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bryan Lim, Manon Flageat, Antoine Cully</p>
            <p><strong>Summary:</strong> arXiv:2404.15794v1 Announce Type: cross 
Abstract: Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches. While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions. As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search. In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations. We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using the QD archive as context. Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization. Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search. Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15794">https://arxiv.org/abs/2404.15794</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language model-based agents as it involves the use of large language models in an innovative, generative way which could potentially be applied to control software or for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15848" target="_blank">Detecting Conceptual Abstraction in LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.15848v1/extracted/5549053/Figures/All_Patterns/ALL_FP.png" target="_blank"><img src="https://arxiv.org/html/2404.15848v1/extracted/5549053/Figures/All_Patterns/ALL_FP.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michaela Regneri, Alhassan Abdelhalim, S\"oren Laue</p>
            <p><strong>Summary:</strong> arXiv:2404.15848v1 Announce Type: cross 
Abstract: We present a novel approach to detecting noun abstraction within a large language model (LLM). Starting from a psychologically motivated set of noun pairs in taxonomic relationships, we instantiate surface patterns indicating hypernymy and analyze the attention matrices produced by BERT. We compare the results to two sets of counterfactuals and show that we can detect hypernymy in the abstraction mechanism, which cannot solely be related to the distributional similarity of noun pairs. Our findings are a first step towards the explainability of conceptual abstraction in LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15848">https://arxiv.org/abs/2404.15848</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not discuss automation or control aspects, it does dive deep into understanding feature abstraction in large language models, which is a significant aspect of how these models could be further optimised to control software or web browsers. Detection and explanation of the conceptual abstraction in LLMs can be considered as one of fundamental works that might enhance performance and control of LLM based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15949" target="_blank">Sequence can Secretly Tell You What to Discard</a></h3>
            <a href="https://arxiv.org/html/2404.15949v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15949v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi</p>
            <p><strong>Summary:</strong> arXiv:2404.15949v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), despite their impressive performance on a wide range of tasks, require significant GPU memory and consume substantial computational resources. In addition to model weights, the memory occupied by KV cache increases linearly with sequence length, becoming a main bottleneck for inference. In this paper, we introduce a novel approach for optimizing the KV cache which significantly reduces its memory footprint. Through a comprehensive investigation, we find that on LLaMA2 series models, (i) the similarity between adjacent tokens' query vectors is remarkably high, and (ii) current query's attention calculation can rely solely on the attention information of a small portion of the preceding queries. Based on these observations, we propose CORM, a KV cache eviction policy that dynamically retains important key-value pairs for inference without finetuning the model. We validate that CORM reduces the inference memory usage of KV cache by up to 70% without noticeable performance degradation across six tasks in LongBench.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15949">https://arxiv.org/abs/2404.15949</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models (LLMs). It proposes an optimization method for the KV cache in LLMs, which could be useful for controlling software and automating computers with these models. However, the paper doesn't involve controlling web browsers or software specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.12399" target="_blank">A Survey of Graph Meets Large Language Model: Progress and Future Directions</a></h3>
            
            <p><strong>Authors:</strong> Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu</p>
            <p><strong>Summary:</strong> arXiv:2311.12399v4 Announce Type: replace 
Abstract: Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.12399">https://arxiv.org/abs/2311.12399</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the integration of Large Language Models (LLMs) with graphs, which could potentially be applied to controlling web browsers or software. However, it does not specifically discuss the areas of your interest which are controlling software or web browsers using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01799" target="_blank">Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward</a></h3>
            
            <p><strong>Authors:</strong> Arnav Chavan, Raghav Magazine, Shubham Kushwaha, M\'erouane Debbah, Deepak Gupta</p>
            <p><strong>Summary:</strong> arXiv:2402.01799v2 Announce Type: replace 
Abstract: Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01799">https://arxiv.org/abs/2402.01799</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper doesn't specifically discuss controlling software or web browsers, but it is highly relevant to large language models (LLMs), examining challenges and potential improvements in LLM inference efficiency. Such improvements to LLMs might be beneficial for their applications in controlling softwares or web browsers and computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.03170" target="_blank">Is Mamba Capable of In-Context Learning?</a></h3>
            
            <p><strong>Authors:</strong> Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter</p>
            <p><strong>Summary:</strong> arXiv:2402.03170v2 Announce Type: replace 
Abstract: State of the art foundation models such as GPT-4 perform surprisingly well at in-context learning (ICL), a variant of meta-learning concerning the learned ability to solve tasks during a neural network forward pass, exploiting contextual information provided as input to the model. This useful ability emerges as a side product of the foundation model's massive pretraining. While transformer models are currently the state of the art in ICL, this work provides empirical evidence that Mamba, a newly proposed state space model which scales better than transformers w.r.t. the input sequence length, has similar ICL capabilities. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that, across both categories of tasks, Mamba closely matches the performance of transformer models for ICL. Further analysis reveals that, like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving long input sequences. This is an exciting finding in meta-learning and may enable generalizations of in-context learned AutoML algorithms (like TabPFN or Optformer) to long input sequences.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.03170">https://arxiv.org/abs/2402.03170</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although not explicitly corresponding to the subtopics, this paper explores the capabilities of a large language model, Mamba, in in-context learning, which is a skill important in controlling software and automating tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.14462" target="_blank">Towards smaller, faster decoder-only transformers: Architectural variants and their implications</a></h3>
            <a href="https://arxiv.org/html/2404.14462v2/extracted/5555685/decoder_block.png" target="_blank"><img src="https://arxiv.org/html/2404.14462v2/extracted/5555685/decoder_block.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sathya Krishnan Suresh, Shunmugapriya P</p>
            <p><strong>Summary:</strong> arXiv:2404.14462v2 Announce Type: replace 
Abstract: Research on Large Language Models (LLMs) has recently seen exponential growth, largely focused on transformer-based architectures, as introduced by [1] and further advanced by the decoder-only variations in [2]. Contemporary studies typically aim to improve model capabilities by increasing both the architecture's complexity and the volume of training data. However, research exploring how to reduce model sizes while maintaining performance is limited. This study introduces three modifications to the decoder-only transformer architecture: ParallelGPT (p-gpt), LinearlyCompressedGPT (lc-gpt), and ConvCompressedGPT (cc-gpt). These variants achieve comparable performance to conventional architectures in code generation tasks while benefiting from reduced model sizes and faster training times. We open-source the model weights and codebase to support future research and development in this domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.14462">https://arxiv.org/abs/2404.14462</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses decoder-only transformer architecture modifications that can result in reduced model sizes and faster training times. This can be beneficial in creating more efficient large language models for controlling software and automating computers - matching your interest in 'Agents based on large-language models'. However, the paper doesn't seem to directly tackle controlling software or web browsers with these models, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.13788" target="_blank">Can LLM-Generated Misinformation Be Detected?</a></h3>
            
            <p><strong>Authors:</strong> Canyu Chen, Kai Shu</p>
            <p><strong>Summary:</strong> arXiv:2309.13788v5 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.13788">https://arxiv.org/abs/2309.13788</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the possible misuse of large language models (LLMs), a topic related to your interest in LLM agents. Although it doesn't focus specifically on using LLMs for software or web browser control, it provides valuable insight into the potential challenges and considerations associated with using this technology.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.00749" target="_blank">SEED: Domain-Specific Data Curation With Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.00749v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.00749v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zui Chen, Lei Cao, Sam Madden, Tim Kraska, Zeyuan Shang, Ju Fan, Nan Tang, Zihui Gu, Chunwei Liu, Michael Cafarella</p>
            <p><strong>Summary:</strong> arXiv:2310.00749v3 Announce Type: replace-cross 
Abstract: Data curation tasks that prepare data for analytics are critical for turning data into actionable insights. However, due to the diverse requirements of applications in different domains, generic off-the-shelf tools are typically insufficient. As a result, data scientists often have to develop domain-specific solutions tailored to both the dataset and the task, e.g. writing domain-specific code or training machine learning models on a sufficient number of annotated examples. This process is notoriously difficult and time-consuming. We present SEED, an LLM-as-compiler approach that automatically generates domain-specific data curation solutions via Large Language Models (LLMs). Once the user describes a task, input data, and expected output, the SEED compiler produces a hybrid pipeline that combines LLM querying with more cost-effective alternatives, such as vector-based caching, LLM-generated code, and small models trained on LLM-annotated data. SEED features an optimizer that automatically selects from the four LLM-assisted modules and forms a hybrid execution pipeline that best fits the task at hand. To validate this new, revolutionary approach, we conducted experiments on $9$ datasets spanning over $5$ data curation tasks. In comparison to solutions that use the LLM on every data record, SEED achieves state-of-the-art or comparable few-shot performance, while significantly reducing the number of LLM calls.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.00749">https://arxiv.org/abs/2310.00749</a></p>
            <p><strong>Category:</strong> cs.DB</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper talks about 'SEED', an approach that generates domain-specific data curation solutions using Large Language Models. It corresponds to your interest in using large language models for tasks like controlling software or managing web browsers, although it doesn't directly address these exact subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.12576" target="_blank">LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations</a></h3>
            <a href="https://arxiv.org/html/2401.12576v2/extracted/5557193/figures/dialogue-with-pictograms2.png" target="_blank"><img src="https://arxiv.org/html/2401.12576v2/extracted/5557193/figures/dialogue-with-pictograms2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leonhard Hennig, Sebastian M\"oller</p>
            <p><strong>Summary:</strong> arXiv:2401.12576v2 Announce Type: replace-cross 
Abstract: Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding (Slack et al., 2023; Shen et al., 2023), as one-off explanations may fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, often require external tools and modules and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate explanations and perform user intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) methods, including white-box explainability tools such as feature attributions, and self-explanations (e.g., for rationale generation). LLM-based (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckupprovides tutorials for operations available in the system, catering to individuals with varying levels of expertise in XAI and supporting multiple input modalities. We introduce a new parsing strategy that substantially enhances the user intent recognition accuracy of the LLM. Finally, we showcase LLMCheckup for the tasks of fact checking and commonsense question answering.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.12576">https://arxiv.org/abs/2401.12576</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may be of interest as it deals with large language models, a key part of your interests. Even though it doesn't focus on controlling software or web browsers, it does focus on understanding the behavior of large language models and improving their user interaction, which could indirectly contribute to better automation and control strategies.</p>
        </div>
        </div><div class='timestamp'>Report generated on April 25, 2024 at 21:36:27</div></body></html>