
            <html>
            <head>
                <title>Report Generated on May 01, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 01, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.08528" target="_blank">auto-sktime: Automated Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Marc-Andr\'e Z\"oller, Marius Lindauer, Marco F. Huber</p>
            <p><strong>Summary:</strong> arXiv:2312.08528v3 Announce Type: replace 
Abstract: In today's data-driven landscape, time series forecasting is pivotal in decision-making across various sectors. Yet, the proliferation of more diverse time series data, coupled with the expanding landscape of available forecasting methods, poses significant challenges for forecasters. To meet the growing demand for efficient forecasting, we introduce auto-sktime, a novel framework for automated time series forecasting. The proposed framework uses the power of automated machine learning (AutoML) techniques to automate the creation of the entire forecasting pipeline. The framework employs Bayesian optimization, to automatically construct pipelines from statistical, machine learning (ML) and deep neural network (DNN) models. Furthermore, we propose three essential improvements to adapt AutoML to time series data. First, pipeline templates to account for the different supported forecasting models. Second, a novel warm-starting technique to start the optimization from prior optimization runs. Third, we adapt multi-fidelity optimizations to make them applicable to a search space containing statistical, ML and DNN models. Experimental results on 64 diverse real-world time series datasets demonstrate the effectiveness and efficiency of the framework, outperforming traditional methods while requiring minimal human involvement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.08528">https://arxiv.org/abs/2312.08528</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests because it focuses on time series forecasting using deep learning. It introduces a new method, auto-sktime, that automates the creation of the entire forecasting pipeline, which includes new machine learning and DNN models. This aligns with your subtopic interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18948" target="_blank">Sub-Adjacent Transformer: Improving Time Series Anomaly Detection with Reconstruction Error from Sub-Adjacent Neighborhoods</a></h3>
            <a href="https://arxiv.org/html/2404.18948v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.18948v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenzhen Yue, Xianghua Ying, Ruohao Guo, DongDong Chen, Ji Shi, Bowei Xing, Yuqing Zhu, Taiyan Chen</p>
            <p><strong>Summary:</strong> arXiv:2404.18948v1 Announce Type: new 
Abstract: In this paper, we present the Sub-Adjacent Transformer with a novel attention mechanism for unsupervised time series anomaly detection. Unlike previous approaches that rely on all the points within some neighborhood for time point reconstruction, our method restricts the attention to regions not immediately adjacent to the target points, termed sub-adjacent neighborhoods. Our key observation is that owing to the rarity of anomalies, they typically exhibit more pronounced differences from their sub-adjacent neighborhoods than from their immediate vicinities. By focusing the attention on the sub-adjacent areas, we make the reconstruction of anomalies more challenging, thereby enhancing their detectability. Technically, our approach concentrates attention on the non-diagonal areas of the attention matrix by enlarging the corresponding elements in the training stage. To facilitate the implementation of the desired attention matrix pattern, we adopt linear attention because of its flexibility and adaptability. Moreover, a learnable mapping function is proposed to improve the performance of linear attention. Empirically, the Sub-Adjacent Transformer achieves state-of-the-art performance across six real-world anomaly detection benchmarks, covering diverse fields such as server monitoring, space exploration, and water treatment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18948">https://arxiv.org/abs/2404.18948</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new transformer-like model applied to time series data. The novel attention mechanism may be of interest given its relevance to the subtopic of new transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18976" target="_blank">Foundations of Multisensory Artificial Intelligence</a></h3>
            
            <p><strong>Authors:</strong> Paul Pu Liang</p>
            <p><strong>Summary:</strong> arXiv:2404.18976v1 Announce Type: new 
Abstract: Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the machine learning foundations of multisensory AI. In the first part, we present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task. These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets, design principled approaches to learn these interactions, and analyze whether their model has succeeded in learning. In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities. We introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas, followed by the cross-modal attention and multimodal transformer architectures that now underpin many of today's multimodal foundation models. Scaling these architectures on MultiBench enables the creation of general-purpose multisensory AI systems, and we discuss our collaborative efforts in applying these models for real-world impact in affective computing, mental health, cancer prognosis, and robotics. Finally, we conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multisensory AI.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18976">https://arxiv.org/abs/2404.18976</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> While primarily focused on auditory and visual sensors, the paper presents new multimodal foundation models and a benchmark for training these models which can be relevant to deep learning models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19228" target="_blank">Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information</a></h3>
            
            <p><strong>Authors:</strong> Toshimitsu Uesaka, Taiji Suzuki, Yuhta Takida, Chieh-Hsin Lai, Naoki Murata, Yuki Mitsufuji</p>
            <p><strong>Summary:</strong> arXiv:2404.19228v1 Announce Type: new 
Abstract: Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this work, we provide a theoretical understanding of the symmetric InfoNCE loss through the lens of the pointwise mutual information and show that encoders that achieve the optimal similarity in the pretraining provide a good representation for downstream classification tasks under mild assumptions. Based on our theoretical results, we also propose a new similarity metric for multimodal contrastive learning by utilizing a nonlinear kernel to enrich the capability. To verify the effectiveness of the proposed method, we demonstrate pretraining of multimodal representation models on the Conceptual Caption datasets and evaluate zero-shot classification and linear classification on common benchmark datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19228">https://arxiv.org/abs/2404.19228</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper discusses multimodal representation learning which falls under your interest in 'New multimodal deep learning models for time series'. The focus is not strictly on time series but the learning model introduced could be applied to it.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19508" target="_blank">Temporal Graph ODEs for Irregularly-Sampled Time Series</a></h3>
            
            <p><strong>Authors:</strong> Alessio Gravina, Daniele Zambon, Davide Bacciu, Cesare Alippi</p>
            <p><strong>Summary:</strong> arXiv:2404.19508v1 Announce Type: new 
Abstract: Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations. To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced. We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19508">https://arxiv.org/abs/2404.19508</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper presents a new method for learning temporal and spatial dynamics from time-series data using a Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which is directly related to your interest in deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19630" target="_blank">Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction</a></h3>
            <a href="https://arxiv.org/html/2404.19630v1/figure01.pdf" target="_blank"><img src="https://arxiv.org/html/2404.19630v1/figure01.pdf" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jared D. Willard, Peter Harrington, Shashank Subramanian, Ankur Mahesh, Travis A. O'Brien, William D. Collins</p>
            <p><strong>Summary:</strong> arXiv:2404.19630v1 Announce Type: new 
Abstract: The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP. However, among these leading DL models, there is a wide variance in both the training settings and architecture used. Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success. In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS. We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect. We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19630">https://arxiv.org/abs/2404.19630</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper showcases the use of a minimally modified SwinV2 transformer for weather prediction (which involves time series forecasting) and provides insights into optimal training procedures. While it does not elaborate specifically on some of your sub-interests like new foundation models or multimodal deep learning models for time series, the use of the SwinV2 transformer for time series data is worth noting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2303.02243" target="_blank">Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks</a></h3>
            
            <p><strong>Authors:</strong> Katarzyna Micha{\l}owska, Somdatta Goswami, George Em Karniadakis, Signe Riemer-S{\o}rensen</p>
            <p><strong>Summary:</strong> arXiv:2303.02243v3 Announce Type: replace 
Abstract: Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks, learning the operator mapping, while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2303.02243">https://arxiv.org/abs/2303.02243</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper heeds to your interest in the 'Time series and deep learning' topic. It explores the use of deep neural networks and recurrent neural networks for simulating complex dynamical systems, thus, it could be regarded as introducing a new method for time series. However, it doesn't explicitly mention forecasting or the use of foundation models for the same.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19708" target="_blank">Harmonic LLMs are Trustworthy</a></h3>
            
            <p><strong>Authors:</strong> Nicholas S. Kersting, Mohammad Rahman, Suchismitha Vedala, Yang Wang</p>
            <p><strong>Summary:</strong> arXiv:2404.19708v1 Announce Type: new 
Abstract: We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM in real-time, based upon the local deviation from harmoniticity, denoted as $\gamma$. To the best of our knowledge this is the first completely model-agnostic and unsupervised method of measuring the robustness of any given response from an LLM, based upon the model itself conforming to a purely mathematical standard. We conduct human annotation experiments to show the positive correlation of $\gamma$ with false or misleading answers, and demonstrate that following the gradient of $\gamma$ in stochastic gradient ascent efficiently exposes adversarial prompts. Measuring $\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT, Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory answers automatically and quantitatively rank the reliability of these models in various objective domains (Web QA, TruthfulQA, and Programming QA). Across all models and domains tested, human ratings confirm that $\gamma \to 0$ indicates trustworthiness, and the low-$\gamma$ leaders among these models are GPT-4, ChatGPT, and Smaug-72B.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19708">https://arxiv.org/abs/2404.19708</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is relevant as it discusses the reliability and robustness of Large Language Models (LLMs) and its use in different domains. The paper focuses on testing black-box LLMs, which aligns with your interest in using large language models for controlling software and other applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18978" target="_blank">Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs</a></h3>
            <a href="https://arxiv.org/html/2404.18978v1/extracted/5566124/Figures/pharmasim.png" target="_blank"><img src="https://arxiv.org/html/2404.18978v1/extracted/5566124/Figures/pharmasim.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bahar Radmehr, Adish Singla, Tanja K\"aser</p>
            <p><strong>Summary:</strong> arXiv:2404.18978v1 Announce Type: new 
Abstract: There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments. However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent's ability to generalize skills across tasks. In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs). We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model's general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents' performance and generalization. To support the development and evaluation of these agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations. Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions. In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task. Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop high-performing agents for open-ended learning environments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18978">https://arxiv.org/abs/2404.18978</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models as agents. It proposes a hybrid method integrating Reinforcement Learning with Large Language Models to improve generalization in educational environments. However, it does not mention controlling software or browser activities specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19484" target="_blank">More Compute Is What You Need</a></h3>
            
            <p><strong>Authors:</strong> Zhen Guo</p>
            <p><strong>Summary:</strong> arXiv:2404.19484v1 Announce Type: new 
Abstract: Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal. In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size. Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19484">https://arxiv.org/abs/2404.19484</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about large language model pre-training and how the amount of compute spent impacts the performance of transformer-based models. It aligns well with your interest in agents based on large language models, particularly in the aspect of efficiency and performance improvement.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19631" target="_blank">On Training a Neural Network to Explain Binaries</a></h3>
            <a href="https://arxiv.org/html/2404.19631v1/extracted/5569070/figures/binary-prose-dist-correl.png" target="_blank"><img src="https://arxiv.org/html/2404.19631v1/extracted/5569070/figures/binary-prose-dist-correl.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Interrante-Grant, Andy Davis, Heather Preslier, Tim Leek</p>
            <p><strong>Summary:</strong> arXiv:2404.19631v1 Announce Type: new 
Abstract: In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding. Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign. Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction. However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models. Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries. A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs. Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space. We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated. We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19631">https://arxiv.org/abs/2404.19631</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses training a large language model on the task of understanding and explaining binary codes, which can be seen as a form of controlling software. However, it does not directly address web browsers or overall computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19065" target="_blank">HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models</a></h3>
            
            <p><strong>Authors:</strong> Gabriel Sarch, Sahil Somani, Raghav Kapoor, Michael J. Tarr, Katerina Fragkiadaki</p>
            <p><strong>Summary:</strong> arXiv:2404.19065v1 Announce Type: cross 
Abstract: Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans. In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions. This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization. We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19065">https://arxiv.org/abs/2404.19065</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper talks about the use of Large Language Models (LLMs) as task planners in instructable agents and further explains how the agent works across various domains using a shared memory. This relates to your interest in using large language models to control software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19094" target="_blank">In-Context Symbolic Regression: Leveraging Language Models for Function Discovery</a></h3>
            <a href="https://arxiv.org/html/2404.19094v1/extracted/5567046/img/sketch.png" target="_blank"><img src="https://arxiv.org/html/2404.19094v1/extracted/5567046/img/sketch.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Matteo Merler, Nicola Dainese, Katsiaryna Haitsiukevich</p>
            <p><strong>Summary:</strong> arXiv:2404.19094v1 Announce Type: cross 
Abstract: Symbolic Regression (SR) is a task which aims to extract the mathematical expression underlying a set of empirical observations. Transformer-based methods trained on SR datasets detain the current state-of-the-art in this task, while the application of Large Language Models (LLMs) to SR remains unexplored. This work investigates the integration of pre-trained LLMs into the SR pipeline, utilizing an approach that iteratively refines a functional form based on the prediction error it achieves on the observation set, until it reaches convergence. Our method leverages LLMs to propose an initial set of possible functions based on the observations, exploiting their strong pre-training prior. These functions are then iteratively refined by the model itself and by an external optimizer for their coefficients. The process is repeated until the results are satisfactory. We then analyze Vision-Language Models in this context, exploring the inclusion of plots as visual inputs to aid the optimization process. Our findings reveal that LLMs are able to successfully recover good symbolic equations that fit the given data, outperforming SR baselines based on Genetic Programming, with the addition of images in the input showing promising results for the most complex benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19094">https://arxiv.org/abs/2404.19094</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest as it investigates the integration of Large Language Models into the Symbolic Regression pipeline, providing an approach for function discovery. The methods discussed in the paper could be relevant to tasks like controlling software or automating computers using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.13639" target="_blank">Contrastive Preference Learning: Learning from Human Feedback without RL</a></h3>
            
            <p><strong>Authors:</strong> Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh</p>
            <p><strong>Summary:</strong> arXiv:2310.13639v3 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.13639">https://arxiv.org/abs/2310.13639</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Your interest in large language models and their use in controlling software aligns with the content of this paper, which discusses a new algorithm for learning optimal policies from human feedback without Reinforcement Learning and can be applied to large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.14925" target="_blank">A Survey of Reinforcement Learning from Human Feedback</a></h3>
            
            <p><strong>Authors:</strong> Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke H\"ullermeier</p>
            <p><strong>Summary:</strong> arXiv:2312.14925v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.14925">https://arxiv.org/abs/2312.14925</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems relevant to your interests as it delves into Reinforcement learning from human feedback (RLHF), which is pivotal in guiding the capabilities of large language model (LLMs) towards human objectives. It provides a fundamental understanding of RLHF, though it doesn't specifically mention controlling software or web browsers with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.11585" target="_blank">Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</a></h3>
            
            <p><strong>Authors:</strong> Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin</p>
            <p><strong>Summary:</strong> arXiv:2403.11585v2 Announce Type: replace 
Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.11585">https://arxiv.org/abs/2403.11585</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests in 'Agents based on large language models'. It presents 'Linguacodus', a framework that uses a large language model for automated code generation, providing a new method of computer automation using large language models. It may not specifically discuss controlling software or web browsers, but it presents a novel use of large language models in a similar vein, which might intrigue you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.03147" target="_blank">Eigenpruning</a></h3>
            
            <p><strong>Authors:</strong> Tom\'as Vergara-Browne, \'Alvaro Soto, Akiko Aizawa</p>
            <p><strong>Summary:</strong> arXiv:2404.03147v3 Announce Type: replace 
Abstract: We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we publicly release our implementation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.03147">https://arxiv.org/abs/2404.03147</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models (LLMs) and how manipulating elements within them (via eigenpruning) can improve performance in specific tasks. It is related to understanding and maximizing the potential of LLMs, which might align with your interest in agent-based applications of such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18311" target="_blank">Towards Real-time Learning in Large Language Models: A Critical Review</a></h3>
            
            <p><strong>Authors:</strong> Mladjan Jovanovic, Peter Voss</p>
            <p><strong>Summary:</strong> arXiv:2404.18311v2 Announce Type: replace 
Abstract: Real-time learning concerns the ability of learning systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data may be insufficient or difficult to obtain. This review provides a comprehensive analysis of real-time learning in Large Language Models. It synthesizes the state-of-the-art real-time learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for real-time learning by describing specific achievements from these related topics and their critical factors. Finally, the paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of real-time learning and its implications for designing and developing LLM-based learning systems addressing real-world problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18311">https://arxiv.org/abs/2404.18311</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models, specifically its application in real-time learning systems. Its exploration of real-time learning paradigms including continual learning, meta-learning, and parameter-efficient learning may offer key insights for managing large language models in various automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07240" target="_blank">CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving</a></h3>
            
            <p><strong>Authors:</strong> Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang</p>
            <p><strong>Summary:</strong> arXiv:2310.07240v5 Announce Type: replace-cross 
Abstract: As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. .
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache's distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x while having negligible impact on the LLM response quality in accuracy or perplexity.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07240">https://arxiv.org/abs/2310.07240</a></p>
            <p><strong>Category:</strong> cs.NI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper doesn't focus directly on using LLMs to control software or browsers, it describes an improved serving system for LLMs that could enhance their performance in such tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14846" target="_blank">Stick to Your Role! Context-dependence and Stability of Personal Value Expression in Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Grgur Kova\v{c}, R\'emy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</p>
            <p><strong>Summary:</strong> arXiv:2402.14846v3 Announce Type: replace-cross 
Abstract: The standard way to study Large Language Models (LLMs) with benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLMs' highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence (specifically, value stability) should be studied a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size). We present a case-study on the stability of value expression over different contexts (simulated conversations on different topics) as measured using a standard psychology questionnaire (PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks. We observe consistent trends in the stability of models and model families - Mixtral, Mistral, GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency of these trends implies that some models exhibit higher value-stability than others, and that value stability can be estimated with the set of introduced methodological tools. When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, which further diminishes with conversation length. This highlights the need for future research on LLMs that coherently simulate different personas. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14846">https://arxiv.org/abs/2402.14846</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the stability of values in Large Language Models (LLMs) and their behavior in various contexts, which is a foundational aspect of using LLMs to control software or for automation. However, it doesn't specifically delve into software or web browsers control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.01081" target="_blank">LAB: Large-Scale Alignment for ChatBots</a></h3>
            <a href="https://arxiv.org/html/2403.01081v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.01081v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, Akash Srivastava</p>
            <p><strong>Summary:</strong> arXiv:2403.01081v3 Announce Type: replace-cross 
Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.01081">https://arxiv.org/abs/2403.01081</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces a novel approach (LAB) to large language model (LLM) training for instruction-following behaviors, which carries promise for applications in software control and computer automation. Although web browsers aren't specifically addressed, the implications carry across to similar uses.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18081" target="_blank">ComposerX: Multi-Agent Symbolic Music Composition with LLMs</a></h3>
            
            <p><strong>Authors:</strong> Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, Yizhi Li, Yinghao Ma, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenwu Wang, Guangyu Xia, Wei Xue, Yike Guo</p>
            <p><strong>Summary:</strong> arXiv:2404.18081v2 Announce Type: replace-cross 
Abstract: Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18081">https://arxiv.org/abs/2404.18081</a></p>
            <p><strong>Category:</strong> cs.SD</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper describes ComposerX, an agent-based symbolic music generation framework leveraging large language models. Although it isn't directly related to controlling software or web browsers, the agent's ability to follow user instructions and generate creative content is relevant to large language model-based agents.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.19596" target="_blank">Debiased Collaborative Filtering with Kernel-Based Causal Balancing</a></h3>
            
            <p><strong>Authors:</strong> Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, Peng Cui</p>
            <p><strong>Summary:</strong> arXiv:2404.19596v1 Announce Type: cross 
Abstract: Debiased collaborative filtering aims to learn an unbiased prediction model by removing different biases in observational datasets. To solve this problem, one of the simple and effective methods is based on the propensity score, which adjusts the observational sample distribution to the target one by reweighting observed instances. Ideally, propensity scores should be learned with causal balancing constraints. However, existing methods usually ignore such constraints or implement them with unreasonable approximations, which may affect the accuracy of the learned propensity scores. To bridge this gap, in this paper, we first analyze the gaps between the causal balancing requirements and existing methods such as learning the propensity with cross-entropy loss or manually selecting functions to balance. Inspired by these gaps, we propose to approximate the balancing functions in reproducing kernel Hilbert space and demonstrate that, based on the universal property and representer theorem of kernel functions, the causal balancing constraints can be better satisfied. Meanwhile, we propose an algorithm that adaptively balances the kernel function and theoretically analyze the generalization error bound of our methods. We conduct extensive experiments to demonstrate the effectiveness of our methods, and to promote this research direction, we have released our project at https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.19596">https://arxiv.org/abs/2404.19596</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses new methods for debiasing collaborative filtering using causality, particularly encompassing 'causal representation learning' and 'causal discovery'. It proposes new methods and insights, which match your interest in novel causality and machine learning methods. However, it does not cover the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.08765" target="_blank">Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms</a></h3>
            
            <p><strong>Authors:</strong> Daria Bystrova, Charles K. Assaad, Julyan Arbel, Emilie Devijver, Eric Gaussier, Wilfried Thuiller</p>
            <p><strong>Summary:</strong> arXiv:2306.08765v2 Announce Type: replace-cross 
Abstract: Constraint-based methods and noise-based methods are two distinct families of methods proposed for uncovering causal graphs from observational data. However, both operate under strong assumptions that may be challenging to validate or could be violated in real-world scenarios. In response to these challenges, there is a growing interest in hybrid methods that amalgamate principles from both methods, showing robustness to assumption violations. This paper introduces a novel comprehensive framework for hybridizing constraint-based and noise-based methods designed to uncover causal graphs from observational time series. The framework is structured into two classes. The first class employs a noise-based strategy to identify a super graph, containing the true graph, followed by a constraint-based strategy to eliminate unnecessary edges. In the second class, a constraint-based strategy is applied to identify a skeleton, which is then oriented using a noise-based strategy. The paper provides theoretical guarantees for each class under the condition that all assumptions are satisfied, and it outlines some properties when assumptions are violated. To validate the efficacy of the framework, two algorithms from each class are experimentally tested on simulated data, realistic ecological data, and real datasets sourced from diverse applications. Notably, two novel datasets related to Information Technology monitoring are introduced within the set of considered real datasets. The experimental results underscore the robustness and effectiveness of the hybrid approaches across a broad spectrum of datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.08765">https://arxiv.org/abs/2306.08765</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning. It focuses specifically on causal discovery, presenting a hybrid methodology. It might not involve large language models in its methodology, hence the reduction in score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.00957" target="_blank">Causal Inference with Differentially Private (Clustered) Outcomes</a></h3>
            <a href="https://arxiv.org/html/2308.00957v2/extracted/5566861/figures/new_illustration.png" target="_blank"><img src="https://arxiv.org/html/2308.00957v2/extracted/5566861/figures/new_illustration.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie</p>
            <p><strong>Summary:</strong> arXiv:2308.00957v2 Announce Type: replace-cross 
Abstract: Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, Cluster-DP, which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. We compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the Cluster-DP algorithm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.00957">https://arxiv.org/abs/2308.00957</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is relevant as it discusses the concept of Causal Inference, which is one of your sub-interests. It proposes a new privacy mechanism for estimating causal effects, which directly aligns with your interest in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.06665" target="_blank">The Essential Role of Causality in Foundation World Models for Embodied AI</a></h3>
            <a href="https://arxiv.org/html/2402.06665v2/extracted/5567228/figures/2.png" target="_blank"><img src="https://arxiv.org/html/2402.06665v2/extracted/5567228/figures/2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Marc Rigter, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard Sch\"olkopf, Cheng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2402.06665v2 Announce Type: replace-cross 
Abstract: Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.06665">https://arxiv.org/abs/2402.06665</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is primarily about the concept of causality in the context of AI and foundation models, which is one of your areas of interest. However, it seems more focused on physical interactions and embodied AI, therefore not fully matching all your interests in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14385" target="_blank">Estimating Causal Effects with Double Machine Learning -- A Method Evaluation</a></h3>
            
            <p><strong>Authors:</strong> Jonathan Fuhr, Philipp Berens, Dominik Papies</p>
            <p><strong>Summary:</strong> arXiv:2403.14385v2 Announce Type: replace-cross 
Abstract: The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - "double/debiased machine learning" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14385">https://arxiv.org/abs/2403.14385</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to align with your interest in causality and machine learning. It not only presents a review of the 'double machine learning' method for estimating causal effects but also provides an evaluation of its performance compared to traditional methods. It doesn't introduce a new method but discusses relaxing assumptions via a machine-learning approach. Hence, it could still be of interest to you.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 01, 2024 at 21:34:36</div></body></html>