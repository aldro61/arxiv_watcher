
            <html>
            <head>
                <title>Report Generated on May 02, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 02, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00319" target="_blank">Data Augmentation Policy Search for Long-Term Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Liran Nochumsohn, Omri Azencot</p>
            <p><strong>Summary:</strong> arXiv:2405.00319v1 Announce Type: new 
Abstract: Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00319">https://arxiv.org/abs/2405.00319</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns nicely with your interest in 'New deep learning methods for time series'. It presents a new time-series automatic augmentation approach for long-term forecasting, which could be a vital addition to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.16233" target="_blank">AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2404.16233v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.16233v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Tony Hu, Katrin Kirchhoff, George Karypis</p>
            <p><strong>Summary:</strong> arXiv:2404.16233v2 Announce Type: replace 
Abstract: AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundation models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.16233">https://arxiv.org/abs/2404.16233</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be of interest to you because it introduces a new open-source library (AutoMM) specifically designed for multimodal learning, which is one of your subtopics. However, the paper does not explicitly mention time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.11166" target="_blank">Volume-Preserving Transformers for Learning Time Series Data with Structure</a></h3>
            
            <p><strong>Authors:</strong> Benedikt Brantner, Guillaume de Romemont, Michael Kraus, Zeyuan Li</p>
            <p><strong>Summary:</strong> arXiv:2312.11166v2 Announce Type: replace-cross 
Abstract: Two of the many trends in neural network research of the past few years have been (i) the learning of dynamical systems, especially with recurrent neural networks such as long short-term memory networks (LSTMs) and (ii) the introduction of transformer neural networks for natural language processing (NLP) tasks. Both of these trends have created enormous amounts of traction, particularly the second one: transformer networks now dominate the field of NLP. Even though some work has been performed on the intersection of these two trends, those efforts was largely limited to using the vanilla transformer directly without adjusting its architecture for the setting of a physical system. In this work we use a transformer-inspired neural network to learn a dynamical system and furthermore (for the first time) imbue it with structure-preserving properties to improve long-term stability. This is shown to be of great advantage when applying the neural network to real world applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.11166">https://arxiv.org/abs/2312.11166</a></p>
            <p><strong>Category:</strong> math.NA</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research papers offers novel insights about a transformer-inspired neural network for learning a dynamical system with structure-preserving properties, which can be useful for time series forecasting. However, it doesn't specifically tackle the topic of multimodal deep learning models for time series.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00451" target="_blank">Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning</a></h3>
            
            <p><strong>Authors:</strong> Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, Michael Shieh</p>
            <p><strong>Summary:</strong> arXiv:2405.00451v1 Announce Type: cross 
Abstract: We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the critical importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and SciQ, with substantial percentage increases in accuracy to $80.7\%$ (+$4.8\%$), $32.2\%$ (+$3.3\%$), and $88.5\%$ (+$7.7\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00451">https://arxiv.org/abs/2405.00451</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper describes an innovative method to improve the reasoning capabilities of large language models (LLMs), which aligns closely with your interest in agents based on LLMs. Although the paper does not specifically mention controlling software or web browsers, the concept potentially has broad applications including those areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00516" target="_blank">Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2405.00516v1/extracted/5555101/model_graphs/comparison_existing_works.jpg" target="_blank"><img src="https://arxiv.org/html/2405.00516v1/extracted/5555101/model_graphs/comparison_existing_works.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lucas-Andre\"i Thil, Mirela Popa, Gerasimos Spanakis</p>
            <p><strong>Summary:</strong> arXiv:2405.00516v1 Announce Type: new 
Abstract: Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\% average accuracy in SL and 36.69\% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00516">https://arxiv.org/abs/2405.00516</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in things like software control and web browsing automation by large language models. It proposes a combined SL and RL approach for web navigation tasks through large language models and tests against the MiniWoB benchmark.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00675" target="_blank">Self-Play Preference Optimization for Language Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.00675v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00675v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, Quanquan Gu</p>
            <p><strong>Summary:</strong> arXiv:2405.00675v1 Announce Type: new 
Abstract: Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00675">https://arxiv.org/abs/2405.00675</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in 'Agents based on large-language models'. It proposes a new method for language model alignment, which may have implications for controlling software or automating tasks using large language models. However, it doesn't specifically focus on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00099" target="_blank">Creative Beam Search</a></h3>
            <a href="https://arxiv.org/html/2405.00099v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00099v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Giorgio Franceschelli, Mirco Musolesi</p>
            <p><strong>Summary:</strong> arXiv:2405.00099v1 Announce Type: cross 
Abstract: Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00099">https://arxiv.org/abs/2405.00099</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper, 'Creative Beam Search', involves using large language models in the process of generation. It is highly relevant to your interest in the use of large language models for tasks such as controlling software. While it specifically explores artificial creativity, the principles could be applicable to other tasks as well.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00216" target="_blank">Graphical Reasoning: LLM-based Semi-Open Relation Extraction</a></h3>
            <a href="https://arxiv.org/html/2405.00216v1/extracted/5569721/eecs598_LLM_proj.png" target="_blank"><img src="https://arxiv.org/html/2405.00216v1/extracted/5569721/eecs598_LLM_proj.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yicheng Tao, Yiqun Wang, Longju Bai</p>
            <p><strong>Summary:</strong> arXiv:2405.00216v1 Announce Type: cross 
Abstract: This paper presents a comprehensive exploration of relation extraction utilizing advanced language models, specifically Chain of Thought (CoT) and Graphical Reasoning (GRE) techniques. We demonstrate how leveraging in-context learning with GPT-3.5 can significantly enhance the extraction process, particularly through detailed example-based reasoning. Additionally, we introduce a novel graphical reasoning approach that dissects relation extraction into sequential sub-tasks, improving precision and adaptability in processing complex relational data. Our experiments, conducted on multiple datasets, including manually annotated data, show considerable improvements in performance metrics, underscoring the effectiveness of our methodologies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00216">https://arxiv.org/abs/2405.00216</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in large-language model agents. It utilizes GPT-3.5, a large language model, to enhance the extraction process in the context of relation extraction, which can be considered a subsection of overall task automation using large language models. However, it does not specifically reference control of software or web browsers; thus the relevance score is 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00218" target="_blank">Constrained Decoding for Secure Code Generation</a></h3>
            <a href="https://arxiv.org/html/2405.00218v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00218v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yanjun Fu, Ethan Baker, Yizheng Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.00218v1 Announce Type: cross 
Abstract: Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation.
  This paper introduces a new benchmark, CodeGuard+, along with two new metrics, secure-pass@k and secure@$k_{\text{pass}}$, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs.
  Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate code that satisfies security and correctness constraints simultaneously. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, constrained decoding can be used together with prefix tuning to further improve the security of Code LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00218">https://arxiv.org/abs/2405.00218</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models, particularly as they pertain to code generation. It may be valuable for understanding techniques to improve the security and correctness of code generated by these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00254" target="_blank">Principled RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation</a></h3>
            <a href="https://arxiv.org/html/2405.00254v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00254v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chanwoo Park, Mingyang Liu, Kaiqing Zhang, Asuman Ozdaglar</p>
            <p><strong>Summary:</strong> arXiv:2405.00254v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00254">https://arxiv.org/abs/2405.00254</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper seems to have a focus on using reinforcement learning, a subset of machine learning, with large-language models, directly relating to your 'Large-Language Models agents' interest. It discusses personalization and preference aggregation, which could potentially be applied to control software or web browsers, though it doesn't specifically mention those applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00263" target="_blank">Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge</a></h3>
            
            <p><strong>Authors:</strong> Bin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin Cui</p>
            <p><strong>Summary:</strong> arXiv:2405.00263v1 Announce Type: cross 
Abstract: Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00263">https://arxiv.org/abs/2405.00263</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't discuss controlling software or browsers with large language models, it presents a new method to improve the efficiency of large language models. This could potentially enhance the performance of agent-based LLMs in the tasks you're interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00602" target="_blank">Investigating Automatic Scoring and Feedback using Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.00602v1/extracted/5571142/figures/cogwheel.png" target="_blank"><img src="https://arxiv.org/html/2405.00602v1/extracted/5571142/figures/cogwheel.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gloria Ashiya Katuka, Alexander Gain, Yen-Yun Yu</p>
            <p><strong>Summary:</strong> arXiv:2405.00602v1 Announce Type: cross 
Abstract: Automatic grading and feedback have been long studied using traditional machine learning and deep learning techniques using language models. With the recent accessibility to high performing large language models (LLMs) like LLaMA-2, there is an opportunity to investigate the use of these LLMs for automatic grading and feedback generation. Despite the increase in performance, LLMs require significant computational resources for fine-tuning and additional specific adjustments to enhance their performance for such tasks. To address these issues, Parameter Efficient Fine-tuning (PEFT) methods, such as LoRA and QLoRA, have been adopted to decrease memory and computational requirements in model fine-tuning. This paper explores the efficacy of PEFT-based quantized models, employing classification or regression head, to fine-tune LLMs for automatically assigning continuous numerical grades to short answers and essays, as well as generating corresponding feedback. We conducted experiments on both proprietary and open-source datasets for our tasks. The results show that prediction of grade scores via finetuned LLMs are highly accurate, achieving less than 3% error in grade percentage on average. For providing graded feedback fine-tuned 4-bit quantized LLaMA-2 13B models outperform competitive base models and achieve high similarity with subject matter expert feedback in terms of high BLEU and ROUGE scores and qualitatively in terms of feedback. The findings from this study provide important insights into the impacts of the emerging capabilities of using quantization approaches to fine-tune LLMs for various downstream tasks, such as automatic short answer scoring and feedback generation at comparatively lower costs and latency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00602">https://arxiv.org/abs/2405.00602</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not primarily focus on controlling software or web browsers, it discusses using large language models (LLM) for the task of automatic scoring and feedback generation, which can be an aspect of computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00664" target="_blank">Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3</a></h3>
            
            <p><strong>Authors:</strong> Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli</p>
            <p><strong>Summary:</strong> arXiv:2405.00664v1 Announce Type: cross 
Abstract: This study presents a targeted model editing analysis focused on the latest large language model, Llama-3. We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00664">https://arxiv.org/abs/2405.00664</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses an aspect of managing large language models specifically addressing the issue of model editing techniques. Though it doesn't focus on controlling software or browsers, it contributes valuable insights into the use and optimization of large language models, which is significantly relevant to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.12081" target="_blank">MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement</a></h3>
            <a href="https://arxiv.org/html/2305.12081v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.12081v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zifeng Wang, Chufan Gao, Cao Xiao, Jimeng Sun</p>
            <p><strong>Summary:</strong> arXiv:2305.12081v4 Announce Type: replace 
Abstract: Tabular data prediction has been employed in medical applications such as patient health risk prediction. However, existing methods usually revolve around the algorithm design while overlooking the significance of data engineering. Medical tabular datasets frequently exhibit significant heterogeneity across different sources, with limited sample sizes per source. As such, previous predictors are often trained on manually curated small datasets that struggle to generalize across different tabular datasets during inference. This paper proposes to scale medical tabular data predictors (MediTab) to various tabular inputs with varying features. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema. It also aligns out-domain data with the target task using a "learn, annotate, and refinement" pipeline. The expanded training data then enables the pre-trained MediTab to infer for arbitrary tabular input in the domain without fine-tuning, resulting in significant improvements over supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3 trial outcome prediction datasets, respectively. In addition, MediTab exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8.9% and 17.2% on average in two prediction tasks, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.12081">https://arxiv.org/abs/2305.12081</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models to aid in medical tabular data prediction, a form of software control. It mentions the utilization of large language models for data consolidation, which implies its potential for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.11456" target="_blank">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint</a></h3>
            <a href="https://arxiv.org/html/2312.11456v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.11456v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang</p>
            <p><strong>Summary:</strong> arXiv:2312.11456v4 Announce Type: replace 
Abstract: This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.
  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.11456">https://arxiv.org/abs/2312.11456</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses reinforcement learning from human feedback which could in theory be applied to large language models to control software or browsers. However, the paper does not explicitly focus on large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.12931" target="_blank">Eureka: Human-Level Reward Design via Coding Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.12931v2/extracted/5569687/figures/envs.png" target="_blank"><img src="https://arxiv.org/html/2310.12931v2/extracted/5569687/figures/envs.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar</p>
            <p><strong>Summary:</strong> arXiv:2310.12931v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12931">https://arxiv.org/abs/2310.12931</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses the use of large language models such as GPT-4 for complex tasks related to reinforcement learning. Furthermore, it introduces 'Eureka', an algorithm powered by Large Language Models for performing evolutionary optimization over reward code, which is relevant to your interest in computer automation using large language models. However, it doesn't directly discuss controlling software or web browsers using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.02843" target="_blank">Thousands of AI Authors on the Future of AI</a></h3>
            <a href="https://arxiv.org/html/2401.02843v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.02843v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Katja Grace, Harlan Stewart, Julia Fabienne Sandk\"uhler, Stephen Thomas, Ben Weinstein-Raun, Jan Brauner</p>
            <p><strong>Summary:</strong> arXiv:2401.02843v2 Announce Type: replace-cross 
Abstract: In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
  Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.02843">https://arxiv.org/abs/2401.02843</a></p>
            <p><strong>Category:</strong> cs.CY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems relevant to your interest in agents based on large language models, specifically with regard to the forecasted ability of AI to download and fine-tune a large language model autonomously, potentially indicating advancements in automation using these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10962" target="_blank">Measuring and Controlling Instruction (In)Stability in Language Model Dialogs</a></h3>
            <a href="https://arxiv.org/html/2402.10962v3/extracted/5571226/figs/fig1.png" target="_blank"><img src="https://arxiv.org/html/2402.10962v3/extracted/5571226/figs/fig1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Vi\'egas, Hanspeter Pfister, Martin Wattenberg</p>
            <p><strong>Summary:</strong> arXiv:2402.10962v3 Announce Type: replace-cross 
Abstract: System-prompting is a standard tool for customizing language-model chatbots, enabling them to follow a specific instruction. An implicit assumption in the use of system prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated instructions for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating instruction stability via self-chats between two instructed chatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and instruction drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10962">https://arxiv.org/abs/2402.10962</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper would be relevant to your interests as it discusses the use of large language models in chatbots, implying their potential for controlling other software. The paper also proposes a new method - split-softmax, which could be innovative in the field of large language model agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.07066" target="_blank">Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?</a></h3>
            
            <p><strong>Authors:</strong> Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.07066v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of "Concept Depth" to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, QWen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at https://github.com/Luckfort/CD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.07066">https://arxiv.org/abs/2404.07066</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper deals with the functions and intricacies of large language models, which is directly related to your third area of interest. Even though it does not explicitly mention control over software or web browsers, the concepts covered could be beneficial to understanding how large language models work as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18191" target="_blank">Exploring the Robustness of In-Context Learning with Noisy Labels</a></h3>
            <a href="https://arxiv.org/html/2404.18191v2/extracted/5567826/eval_figs/noise_type/diff_std/error_curve_std_expotential.png" target="_blank"><img src="https://arxiv.org/html/2404.18191v2/extracted/5567826/eval_figs/noise_type/diff_std/error_curve_std_expotential.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chen Cheng, Xinzhi Yu, Haodong Wen, Jingsong Sun, Guanzhang Yue, Yihao Zhang, Zeming Wei</p>
            <p><strong>Summary:</strong> arXiv:2404.18191v2 Announce Type: replace-cross 
Abstract: Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels. Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels. Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL. Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing. Our code is available at https://github.com/InezYu0928/in-context-learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18191">https://arxiv.org/abs/2404.18191</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the In-Context Learning (ICL) ability of large language models (LLMs), specifically how they deal with noisy labels. Although it doesn't directly mention language models controlling software or web browsers, it sheds light on how Transformers' robustness against diverse types of noise labels could affect computer automation applications.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00622" target="_blank">Causal Evaluation of Language Models</a></h3>
            
            <p><strong>Authors:</strong> Sirui Chen, Bo Peng, Meiqi Chen, Ruiqi Wang, Mengying Xu, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Yu Qiao, Chaochao Lu</p>
            <p><strong>Summary:</strong> arXiv:2405.00622v1 Announce Type: cross 
Abstract: Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for causal reasoning. In this work, we introduce Causal evaluation of Language Models (CaLM), which, to the best of our knowledge, is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. First, we propose the CaLM framework, which establishes a foundational taxonomy consisting of four modules: causal target (i.e., what to evaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to measure the results), and error (i.e., how to analyze the bad results). This taxonomy defines a broad evaluation design space while systematically selecting criteria and priorities. Second, we compose the CaLM dataset, comprising 126,334 data samples, to provide curated sets of causal targets, adaptations, metrics, and errors, offering extensive coverage for diverse research pursuits. Third, we conduct an extensive evaluation of 28 leading language models on a core set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types. Fourth, we perform detailed analyses of the evaluation results across various dimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical findings across 9 dimensions (e.g., model), providing valuable guidance for future language model development. Finally, we develop a multifaceted platform, including a website, leaderboards, datasets, and toolkits, to support scalable and adaptable assessments. We envision CaLM as an ever-evolving benchmark for the community, systematically updated with new causal targets, adaptations, models, metrics, and error types to reflect ongoing research advancements. Project website is at https://opencausalab.github.io/CaLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00622">https://arxiv.org/abs/2405.00622</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Causality and Machine Learning' as it discusses the causal reasoning capabilities of language models. However, it does not seem to particularly focus on new methods of Causal Discovery or Causal Representation Learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.03871" target="_blank">Hidden yet quantifiable: A lower bound for confounding strength using randomized trials</a></h3>
            <a href="https://arxiv.org/html/2312.03871v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.03871v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang</p>
            <p><strong>Summary:</strong> arXiv:2312.03871v3 Announce Type: replace-cross 
Abstract: In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.03871">https://arxiv.org/abs/2312.03871</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interests in Causality and Machine Learning. It proposes a novel strategy to quantify unobserved confounding, which is important in causal discovery.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 02, 2024 at 21:33:55</div></body></html>