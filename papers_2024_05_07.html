
            <html>
            <head>
                <title>Report Generated on May 07, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 07, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.14423" target="_blank">Prompt Design and Engineering: Introduction and Advanced Methods</a></h3>
            <a href="https://arxiv.org/html/2401.14423v4/extracted/5577285/images/PromptExample1.png" target="_blank"><img src="https://arxiv.org/html/2401.14423v4/extracted/5577285/images/PromptExample1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xavier Amatriain</p>
            <p><strong>Summary:</strong> arXiv:2401.14423v4 Announce Type: replace-cross 
Abstract: Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.14423">https://arxiv.org/abs/2401.14423</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it discusses the usage and engineering of large language models, which aligns with your interest in agents based on large-language models. More specifically, it discusses principles behind building LLM-based agents and tools which aligns with your interests in using large language models for controlling software and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02347" target="_blank">COPAL: Continual Pruning in Large Language Generative Models</a></h3>
            <a href="https://arxiv.org/html/2405.02347v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.02347v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Srikanth Malla, Joon Hee Choi, Chiho Choi</p>
            <p><strong>Summary:</strong> arXiv:2405.02347v1 Announce Type: new 
Abstract: Adapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (COntinual Pruning in Adaptive Language settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02347">https://arxiv.org/abs/2405.02347</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not specifically deal with software or web control, it presents a novel approach to adapt large language models to new domains which could have implications for your interest in using large language models in agent-based applications. It presents a new method for continual pruning in adaptive language settings which could potentially improve the efficiency and adaptability of large language models operating as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02596" target="_blank">Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning</a></h3>
            <a href="https://arxiv.org/html/2405.02596v1/extracted/5576535/figures/intro.png" target="_blank"><img src="https://arxiv.org/html/2405.02596v1/extracted/5576535/figures/intro.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jing Xu, Jingzhao Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.02596v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLM) can be costly. Parameter-efficient fine-tuning (PEFT) addresses the problems by training a fraction of the parameters, whose success reveals the expressiveness and flexibility of pretrained models. This paper studies the limit of PEFT, by further simplifying its design and reducing the number of trainable parameters beyond standard setups. To this end, we use Random Masking to fine-tune the pretrained model. Despite its simplicity, we show that Random Masking is surprisingly effective: with a larger-than-expected learning rate, Random Masking can match the performance of standard PEFT algorithms such as LoRA on various tasks, using fewer trainable parameters. We provide both empirical and theoretical explorations into the success of Random Masking. We show that masking induces a flatter loss landscape and more distant solutions, which allows for and necessitates large learning rates.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02596">https://arxiv.org/abs/2405.02596</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models and discusses improvement in fine-tuning such models, which could potentially be applicable in contexts of using these models for software control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02749" target="_blank">Sub-goal Distillation: A Method to Improve Small Language Agents</a></h3>
            
            <p><strong>Authors:</strong> Maryam Hashemzadeh, Elias Stengel-Eskin, Sarath Chandar, Marc-Alexandre Cote</p>
            <p><strong>Summary:</strong> arXiv:2405.02749v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated significant promise as agents in interactive tasks, their substantial computational requirements and restricted number of calls constrain their practical utility, especially in long-horizon interactive tasks such as decision-making or in scenarios involving continuous ongoing tasks. To address these constraints, we propose a method for transferring the performance of an LLM with billions of parameters to a much smaller language model (770M parameters). Our approach involves constructing a hierarchical agent comprising a planning module, which learns through Knowledge Distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions. In detail, we leverage an LLM to annotate an oracle path with a sequence of sub-goals towards completing a goal. Subsequently, we utilize this annotated data to fine-tune both the planning and execution modules. Importantly, neither module relies on real-time access to an LLM during inference, significantly reducing the overall cost associated with LLM interactions to a fixed cost. In ScienceWorld, a challenging and multi-task interactive text environment, our method surpasses standard imitation learning based solely on elementary actions by 16.7% (absolute). Our analysis highlights the efficiency of our approach compared to other LLM-based methods. Our code and annotated data for distillation can be found on GitHub.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02749">https://arxiv.org/abs/2405.02749</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant because it discusses the use of a large language model for task accomplishment as agents, which is a topic point of your interest. Nevertheless, despite largely discussing about language model agents, it doesn’t explicitly mention controller for software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02774" target="_blank">Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.02774v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.02774v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, Ruoxi Jia</p>
            <p><strong>Summary:</strong> arXiv:2405.02774v1 Announce Type: new 
Abstract: This work focuses on leveraging and selecting from vast, unlabeled, open data to pre-fine-tune a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales. However, they often prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution. Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. We demonstrate the efficacy of our methodology across a diverse array of tasks (NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently surpasses other selection methods. Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour. Our code is open-sourced (Code repository: https://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02774">https://arxiv.org/abs/2405.02774</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'llm-agents' interest as it is about fine-tuning large language models (LLMs). Though it does not directly address LLMs controlling software or browsers, or computer automation, it discusses methods for improving the performance of LLMs, which could indirectly contribute to their capability in such tasks. However, it does not propose a totally new method, but rather presents a different perspective on data selection for fine-tuning LLMs, which is why the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03103" target="_blank">Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.03103v1/extracted/5578360/figures/quantized_values.png" target="_blank"><img src="https://arxiv.org/html/2405.03103v1/extracted/5578360/figures/quantized_values.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jordan Dotzel, Yuzong Chen, Bahaa Kotb, Sushma Prasad, Gang Wu, Sheng Li, Mohamed S. Abdelfattah, Zhiru Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.03103v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict latency and power demands. Deep neural network (DNN) quantization has traditionally addressed these limitations by converting models to low-precision integer formats. Yet recently alternative formats, such as Normal Float (NF4), have been shown to consistently increase model accuracy, albeit at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks to conclude most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), with respect to this distribution, that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and performance frontier across 11 datatypes, including non-traditional formats like Additive-Powers-of-Two (APoT), by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03103">https://arxiv.org/abs/2405.03103</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper proposes efficient methods to run large language models on limited hardware, hence enabling more applications including computer automation with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03146" target="_blank">Quantifying the Capabilities of LLMs across Scale and Precision</a></h3>
            
            <p><strong>Authors:</strong> Sher Badshah, Hassan Sajjad</p>
            <p><strong>Summary:</strong> arXiv:2405.03146v1 Announce Type: new 
Abstract: Scale is often attributed as one of the factors that cause an increase in the performance of LLMs, resulting in models with billion and trillion parameters. One of the limitations of such large models is the high computational requirements that limit their usage, deployment, and debugging in resource-constrained scenarios. Two commonly used alternatives to bypass these limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of Llama 70B) and lower the memory requirements by using quantization. While these approaches effectively address the limitation of resources, their impact on model performance needs thorough examination. In this study, we perform a comprehensive evaluation to investigate the effect of model scale and quantization on the performance. We experiment with two major families of open-source instruct models ranging from 7 billion to 70 billion parameters. Our extensive zero-shot experiments across various tasks including natural language understanding, reasoning, misinformation detection, and hallucination reveal that larger models generally outperform their smaller counterparts, suggesting that scale remains an important factor in enhancing performance. We found that larger models show exceptional resilience to precision reduction and can maintain high accuracy even at 4-bit quantization for numerous tasks and they serve as a better solution than using smaller models at high precision under similar memory requirements.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03146">https://arxiv.org/abs/2405.03146</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it doesn't propose a new method, this paper provides valuable insights into the impact of scale and precision on the performance of large language models. This information could be very useful in deploying such models for tasks like controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03341" target="_blank">Enhancing Q-Learning with Large Language Model Heuristics</a></h3>
            
            <p><strong>Authors:</strong> Xiefeng Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.03341v1 Announce Type: new 
Abstract: Q-learning excels in learning from feedback within sequential decision-making tasks but requires extensive sampling for significant improvements. Although reward shaping is a powerful technique for enhancing learning efficiency, it can introduce biases that affect agent performance. Furthermore, potential-based reward shaping is constrained as it does not allow for reward modifications based on actions or terminal states, potentially limiting its effectiveness in complex environments. Additionally, large language models (LLMs) can achieve zero-shot learning, but this is generally limited to simpler tasks. They also exhibit low inference speeds and occasionally produce hallucinations. To address these issues, we propose \textbf{LLM-guided Q-learning} that employs LLMs as heuristic to aid in learning the Q-function for reinforcement learning. It combines the advantages of both technologies without introducing performance bias. Our theoretical analysis demonstrates that the LLM heuristic provides action-level guidance. Additionally, our architecture has the capability to convert the impact of hallucinations into exploration costs. Moreover, the converged Q function corresponds to the MDP optimal Q function. Experiment results demonstrated that our algorithm enables agents to avoid ineffective exploration, enhances sampling efficiency, and is well-suited for complex control tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03341">https://arxiv.org/abs/2405.03341</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest to you as it explores the use of Large Language Models (LLMs) for the improvement of Q-learning in reinforcement learning. Although it doesn't directly focus on controlling software or web browsers, the principles discussed could be applicable to these niches. It is centred on new methods for using LLMs though, which matches your primary interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03547" target="_blank">Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions</a></h3>
            
            <p><strong>Authors:</strong> Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.03547v1 Announce Type: new 
Abstract: Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.
  However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03547">https://arxiv.org/abs/2405.03547</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not perfectly match your interests, it discusses the substantial impact of LLMs on diverse fields such as robotics, indicating some potential relevance. Furthermore, it discusses the potential revolutionization of optimization by LLM, indirectly indicating the potential for automation, including potentially controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02318" target="_blank">NL2FOL: Translating Natural Language to First-Order Logic for Logical Fallacy Detection</a></h3>
            
            <p><strong>Authors:</strong> Abhinav Lalwani, Lovish Chopra, Christopher Hahn, Caroline Trippel, Zhijing Jin, Mrinmaya Sachan</p>
            <p><strong>Summary:</strong> arXiv:2405.02318v1 Announce Type: cross 
Abstract: Logical fallacies are common errors in reasoning that undermine the logic of an argument. Automatically detecting logical fallacies has important applications in tracking misinformation and validating claims. In this paper, we design a process to reliably detect logical fallacies by translating natural language to First-order Logic (FOL) step-by-step using Large Language Models (LLMs). We then utilize Satisfiability Modulo Theory (SMT) solvers to reason about the validity of the formula and classify inputs as either a fallacy or valid statement. Our model also provides a novel means of utilizing LLMs to interpret the output of the SMT solver, offering insights into the counter-examples that illustrate why a given sentence is considered a logical fallacy. Our approach is robust, interpretable and does not require training data or fine-tuning. We evaluate our model on a mixed dataset of fallacies and valid sentences. The results demonstrate improved performance compared to end-to-end LLMs, with our classifier achieving an F1-score of 71\% on the Logic dataset. The approach is able to generalize effectively, achieving an F1-score of 73% on the challenge set, LogicClimate, outperforming state-of-the-art models by 21% despite its much smaller size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02318">https://arxiv.org/abs/2405.02318</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper is not centered around using large language models for specific controls like software or web browsers, it introduces a novel way of using large language models for logic reasoning and fallacy detection. It could provide useful insight into how to implement and improve agent functionality using these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02326" target="_blank">Evaluating LLMs for Hardware Design and Test</a></h3>
            
            <p><strong>Authors:</strong> Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce</p>
            <p><strong>Summary:</strong> arXiv:2405.02326v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated capabilities for producing code in Hardware Description Languages (HDLs). However, most of the focus remains on their abilities to write functional code, not test code. The hardware design process consists of both design and test, and so eschewing validation and verification leaves considerable potential benefit unexplored, given that a design and test framework may allow for progress towards full automation of the digital design pipeline. In this work, we perform one of the first studies exploring how a LLM can both design and test hardware modules from provided specifications. Using a suite of 8 representative benchmarks, we examined the capabilities and limitations of the state-of-the-art conversational LLMs when producing Verilog for functional and verification purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received the functional chip.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02326">https://arxiv.org/abs/2405.02326</a></p>
            <p><strong>Category:</strong> cs.AR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Despite not being directly about the control of software or web browsers, this paper discusses the usage of LLMs in the context of hardware design, including designing and testing modules from specifications. Therefore, it might provide insights into the potential applications of LLMs in automation and control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02429" target="_blank">CALRec: Contrastive Alignment of Generative LLMs For Sequential Recommendation</a></h3>
            <a href="https://arxiv.org/html/2405.02429v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.02429v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yaoyiran Li, Xiang Zhai, Moustafa Alzantot, Keyi Yu, Ivan Vuli\'c, Anna Korhonen, Mohamed Hammad</p>
            <p><strong>Summary:</strong> arXiv:2405.02429v1 Announce Type: cross 
Abstract: Traditional recommender systems such as matrix factorization methods rely on learning a shared dense embedding space to represent both items and user preferences. Sequence models such as RNN, GRUs, and, recently, Transformers have also excelled in the task of sequential recommendation. This task requires understanding the sequential structure present in users' historical interactions to predict the next item they may like. Building upon the success of Large Language Models (LLMs) in a variety of tasks, researchers have recently explored using LLMs that are pretrained on vast corpora of text for sequential recommendation. To use LLMs in sequential recommendations, both the history of user interactions and the model's prediction of the next item are expressed in text form. We propose CALRec, a two-stage LLM finetuning framework that finetunes a pretrained LLM in a two-tower fashion using a mixture of two contrastive losses and a language modeling loss: the LLM is first finetuned on a data mixture from multiple domains followed by another round of target domain finetuning. Our model significantly outperforms many state-of-the-art baselines (+37% in Recall@1 and +24% in NDCG@10) and systematic ablation studies reveal that (i) both stages of finetuning are crucial, and, when combined, we achieve improved performance, and (ii) contrastive alignment is effective among the target domains explored in our experiments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02429">https://arxiv.org/abs/2405.02429</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't center on controlling software or browsers with LLMs, it is relevant because it provides a novel use of large language models for the task of sequential recommendation. This can help deepen your understanding of the capabilities and applications of LLMs, and possibly inspire new approaches for control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02828" target="_blank">Trojans in Large Language Models of Code: A Critical Review through a Trigger-Based Taxonomy</a></h3>
            <a href="https://arxiv.org/html/2405.02828v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.02828v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Bowen Xu, Premkumar Devanbu, Mohammad Amin Alipour</p>
            <p><strong>Summary:</strong> arXiv:2405.02828v1 Announce Type: cross 
Abstract: Large language models (LLMs) have provided a lot of exciting new capabilities in software development. However, the opaque nature of these models makes them difficult to reason about and inspect. Their opacity gives rise to potential security risks, as adversaries can train and deploy compromised models to disrupt the software development process in the victims' organization.
  This work presents an overview of the current state-of-the-art trojan attacks on large language models of code, with a focus on triggers -- the main design point of trojans -- with the aid of a novel unifying trigger taxonomy framework. We also aim to provide a uniform definition of the fundamental concepts in the area of trojans in Code LLMs. Finally, we draw implications of findings on how code models learn on trigger design.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02828">https://arxiv.org/abs/2405.02828</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper explores the realm of Large Language Models in relation to software development, focusing on the potential security risks and providing a critical review of the current state-of-the-art trojan attacks on LLMs. While it doesn't specifically mention the control of software/web browsers or automation using LLMs, it provides significant insights about the effects and potential risks of using Large Language Models in software development which could be beneficial for your research interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02876" target="_blank">Exploring the Improvement of Evolutionary Computation via Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Jinyu Cai, Jinglue Xu, Jialong Li, Takuto Ymauchi, Hitoshi Iba, Kenji Tei</p>
            <p><strong>Summary:</strong> arXiv:2405.02876v1 Announce Type: cross 
Abstract: Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains. However, as the complexity of problems increases, the limitations of EC have become more apparent. The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields. By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements. This presents a promising direction for future research at the intersection of LLMs and EC.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02876">https://arxiv.org/abs/2405.02876</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper does not directly discuss the control of software or web browsers via large language models, or automation, but it does potentially offer insightful views in terms of using LLMs for improving Evolutionary Computation algorithms. As such, it could offer useful foundational insights for your interests in utilising LLMs in broader agent environments.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02917" target="_blank">Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.02917v1/x1.jpg" target="_blank"><img src="https://arxiv.org/html/2405.02917v1/x1.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tobias Groot, Matias Valdenegro-Toro</p>
            <p><strong>Summary:</strong> arXiv:2405.02917v1 Announce Type: cross 
Abstract: Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field of AI by their ability to generate human-like text and understand images, but ensuring their reliability is crucial. This paper aims to evaluate the ability of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro Vision) to estimate their verbalized uncertainty via prompting. We propose the new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities via difficult queries and object counting, and the Net Calibration Error (NCE) to measure direction of miscalibration. Results show that both LLMs and VLMs have a high calibration error and are overconfident most of the time, indicating a poor capability for uncertainty estimation. Additionally we develop prompts for regression tasks, and we show that VLMs have poor calibration when producing mean/standard deviation and 95% confidence intervals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02917">https://arxiv.org/abs/2405.02917</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not propose control of software or computer automation via language models, it provides an important insight into understanding the reliability of large language models, which is crucial for developing robust agents based on these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03131" target="_blank">WDMoE: Wireless Distributed Large Language Models with Mixture of Experts</a></h3>
            <a href="https://arxiv.org/html/2405.03131v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03131v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nan Xue, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Liang Qian, Shuguang Cui, Ping Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.03131v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but how wireless communications can support LLMs has not been extensively studied. In this paper, we propose a wireless distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE, deploying LLMs collaboratively across edge servers of base station (BS) and mobile devices in the wireless communications system. Specifically, we decompose the MoE layer in LLMs by deploying the gating network and the preceding neural network layer at BS, while distributing the expert networks across the devices. This arrangement leverages the parallel capabilities of expert networks on distributed devices. Moreover, to overcome the instability of wireless communications, we design an expert selection policy by taking into account both the performance of the model and the end-to-end latency, which includes both transmission delay and inference delay. Evaluations conducted across various LLMs and multiple datasets demonstrate that WDMoE not only outperforms existing models, such as Llama 2 with 70 billion parameters, but also significantly reduces end-to-end latency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03131">https://arxiv.org/abs/2405.03131</a></p>
            <p><strong>Category:</strong> cs.IT</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it does not directly hit on all aspects of your interests in large-language models (LLMs) as agents, the paper proposes a new method, WDMoE, focusing on distributed LLMs for wireless communications system. It may offer some insights into computer automation using large language models, especially in networking aspects.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03133" target="_blank">Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training</a></h3>
            
            <p><strong>Authors:</strong> Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis</p>
            <p><strong>Summary:</strong> arXiv:2405.03133v1 Announce Type: cross 
Abstract: Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03133">https://arxiv.org/abs/2405.03133</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest because it discusses the usage of large language models for pre-training, which could potentially be used for controlling software or web browsers as you mentioned. However, it doesn't directly focus on control tasks, hence it doesn't score a perfect 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03205" target="_blank">Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions</a></h3>
            <a href="https://arxiv.org/html/2405.03205v1/extracted/5578793/figures/anchor.png" target="_blank"><img src="https://arxiv.org/html/2405.03205v1/extracted/5578793/figures/anchor.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruizhe Li, Yanjun Gao</p>
            <p><strong>Summary:</strong> arXiv:2405.03205v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the "logit lens" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only correct the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03205">https://arxiv.org/abs/2405.03205</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interest in large language models (LLM). While it doesn't necessarily propose control or automation methods, the paper focuses on robustness and accuracy of GPT-2 family which is crucial in developing LLM-based applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03688" target="_blank">Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames</a></h3>
            
            <p><strong>Authors:</strong> Keith Burghardt, Kai Chen, Kristina Lerman</p>
            <p><strong>Summary:</strong> arXiv:2405.03688v1 Announce Type: cross 
Abstract: Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03688">https://arxiv.org/abs/2405.03688</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper uses GPT-3.5, a large language model, to understand and interpret information operations, which corresponds to your interest in how large language models are used in different contexts. However, it does not directly tackle software or browser level control, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.13883" target="_blank">ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</a></h3>
            <a href="https://arxiv.org/html/2307.13883v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.13883v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kensen Shi, Joey Hong, Yinlin Deng, Pengcheng Yin, Manzil Zaheer, Charles Sutton</p>
            <p><strong>Summary:</strong> arXiv:2307.13883v2 Announce Type: replace 
Abstract: When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. When used with Transformer models trained from scratch, ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting, but an ExeDec-style prompting approach can improve the generalization ability and overall performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.13883">https://arxiv.org/abs/2307.13883</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language model agents as it discusses ExeDec, a novel decomposition-based synthesis strategy that uses transformer models for program synthesis. However, it's more focused on programming-by-example rather than controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.05175" target="_blank">Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</a></h3>
            
            <p><strong>Authors:</strong> Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, Ajay Jaiswal, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, Shiwei Liu</p>
            <p><strong>Summary:</strong> arXiv:2310.05175v3 Announce Type: replace 
Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.05175">https://arxiv.org/abs/2310.05175</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests since it presents a new method (Outlier Weighed Layerwise sparsity - OWL) for pruning Large Language Models (LLMs), which could facilitate their usage for controlling software and web browsers. The research conducted in this paper might enhance the operational efficiency of LLMs as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.06387" target="_blank">Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations</a></h3>
            
            <p><strong>Authors:</strong> Zeming Wei, Yifei Wang, Yisen Wang</p>
            <p><strong>Summary:</strong> arXiv:2310.06387v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating harmful content have emerged. In this paper, we delve into the potential of In-Context Learning (ICL) to modulate the alignment of LLMs. Specifically, we propose the In-Context Attack (ICA), which employs strategically crafted harmful demonstrations to subvert LLMs, and the In-Context Defense (ICD), which bolsters model resilience through examples that demonstrate refusal to produce harmful responses. Through extensive experiments, we demonstrate the efficacy of ICA and ICD in respectively elevating and mitigating the success rates of jailbreaking prompts. Moreover, we offer theoretical insights into the mechanism by which a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs. Our findings illuminate the profound influence of ICL on LLM behavior, opening new avenues for improving the safety and alignment of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.06387">https://arxiv.org/abs/2310.06387</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper primarily explores the potential of Large Language Models (LLMs) in mitigating harmful content creation, but its insights on using in-context demonstrations (both harmful and protective) to influence LLM behavior may provide valuable insights for controlling software and automating processes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17191" target="_blank">How do Language Models Bind Entities in Context?</a></h3>
            
            <p><strong>Authors:</strong> Jiahai Feng, Jacob Steinhardt</p>
            <p><strong>Summary:</strong> arXiv:2310.17191v2 Announce Type: replace 
Abstract: To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17191">https://arxiv.org/abs/2310.17191</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper investigates how large language models use context to bind entities to their attributes. While it does not specifically address the use of LMs to control software or provide automation, it does contribute towards a deeper understanding of how these models process information and reason contextually, which could be of interest to your research in large language model-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.02807" target="_blank">QualEval: Qualitative Evaluation for Model Improvement</a></h3>
            <a href="https://arxiv.org/html/2311.02807v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.02807v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, Ashwin Kalyan</p>
            <p><strong>Summary:</strong> arXiv:2311.02807v2 Announce Type: replace 
Abstract: Quantitative evaluation metrics have traditionally been pivotal in gauging the advancements of artificial intelligence systems, including large language models (LLMs). However, these metrics have inherent limitations. Given the intricate nature of real-world tasks, a single scalar to quantify and compare is insufficient to capture the fine-grained nuances of model behavior. Metrics serve only as a way to compare and benchmark models, and do not yield actionable diagnostics, thus making the model improvement process challenging. Model developers find themselves amid extensive manual efforts involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which augments quantitative scalar metrics with automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are backed by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace of model development, thus in essence serving as a data-scientist-in-a-box. Given the focus on critiquing and improving current evaluation metrics, our method serves as a refreshingly new technique for both model evaluation and improvement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.02807">https://arxiv.org/abs/2311.02807</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest because it discusses the use of large language models in generating human-readable insights for improving model development. Although it does not directly address controlling software or web browsers, the concepts might be applicable in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02619" target="_blank">Increasing Trust in Language Models through the Reuse of Verified Circuits</a></h3>
            <a href="https://arxiv.org/html/2402.02619v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02619v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Philip Quirke, Clement Neo, Fazl Barez</p>
            <p><strong>Summary:</strong> arXiv:2402.02619v4 Announce Type: replace 
Abstract: Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of language models built using them. The reuse of verified circuits reduces the effort to verify more complex composite models which we believe to be a significant step towards safety of language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02619">https://arxiv.org/abs/2402.02619</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research investigates how using large language models can increase trust in task algorithms, which is closely related to your interest in using these models for tasks like software and web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10517" target="_blank">Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.10517v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10517v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee</p>
            <p><strong>Summary:</strong> arXiv:2402.10517v2 Announce Type: replace 
Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. The source code will be publicly available soon.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10517">https://arxiv.org/abs/2402.10517</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses Large Language Models (LLMs) and proposes a method for their efficient deployment. While it doesn't specifically mention controlling software or web browsers, it's a relevant foundational text for understanding how to leverage LLMs in these contexts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.18415" target="_blank">The Topos of Transformer Networks</a></h3>
            
            <p><strong>Authors:</strong> Mattia Jacopo Villani, Peter McBurney</p>
            <p><strong>Summary:</strong> arXiv:2403.18415v3 Announce Type: replace 
Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.18415">https://arxiv.org/abs/2403.18415</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest as it provides a theoretical analysis of the expressivity of the transformer architecture, which powers large language models. Although not strictly about the control of software or web browsers, understanding the underpinnings of these models could inform such applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09491" target="_blank">Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning</a></h3>
            
            <p><strong>Authors:</strong> Sungwon Han, Jinsung Yoon, Sercan O Arik, Tomas Pfister</p>
            <p><strong>Summary:</strong> arXiv:2404.09491v2 Announce Type: replace 
Abstract: Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09491">https://arxiv.org/abs/2404.09491</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on using large language models to engineer features for tabular learning, which can be viewed as a type of software control. It may notably interest you as it explores a unique application and proposal of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18311" target="_blank">Towards Incremental Learning in Large Language Models: A Critical Review</a></h3>
            
            <p><strong>Authors:</strong> Mladjan Jovanovic, Peter Voss</p>
            <p><strong>Summary:</strong> arXiv:2404.18311v4 Announce Type: replace 
Abstract: Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18311">https://arxiv.org/abs/2404.18311</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses large language models and incremental learning strategies that could potentially be applied to controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.11695" target="_blank">A Simple and Effective Pruning Approach for Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</p>
            <p><strong>Summary:</strong> arXiv:2306.11695v3 Announce Type: replace-cross 
Abstract: As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.11695">https://arxiv.org/abs/2306.11695</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses a technique for pruning large language models. While it does not directly address controlling software or web browsers with large language models, this technique could improve the efficiency of such applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.15447" target="_blank">Are aligned neural networks adversarially aligned?</a></h3>
            <a href="https://arxiv.org/html/2306.15447v2/extracted/5578764/figures/fig1betternoise.jpg" target="_blank"><img src="https://arxiv.org/html/2306.15447v2/extracted/5578764/figures/fig1betternoise.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt</p>
            <p><strong>Summary:</strong> arXiv:2306.15447v2 Announce Type: replace-cross 
Abstract: Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study adversarial alignment, and ask to what extent these models remain aligned when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.
  However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.15447">https://arxiv.org/abs/2306.15447</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it does not directly address your concern about using large language models to control software or web browsers, the paper is relevant to your interest in large language models, specifically as it discusses about aligned neural networks and their adversarial alignment. Moreover, it talks about ML-based optimization attacks and adversarial inputs, which might be of interest when thinking about the robustness and security of large language models as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.14726" target="_blank">PLMM: Personal Large Language Models on Mobile Devices</a></h3>
            <a href="https://arxiv.org/html/2309.14726v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.14726v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuanhao Gong</p>
            <p><strong>Summary:</strong> arXiv:2309.14726v2 Announce Type: replace-cross 
Abstract: Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user experience and produce high quality results. The proposed personal large models can be applied in a wide range of applications such as language and vision tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.14726">https://arxiv.org/abs/2309.14726</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though the paper does not cover agent behavior or explicit control applications, it proposes a concept of personal large language models which could be of interest as foundation for controlling software or automating tasks in future. It presents a method to adaptively use large language models considering user's personal information, potentially providing a personalized agent.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08744" target="_blank">Circuit Component Reuse Across Tasks in Transformer Language Models</a></h3>
            
            <p><strong>Authors:</strong> Jack Merullo, Carsten Eickhoff, Ellie Pavlick</p>
            <p><strong>Summary:</strong> arXiv:2310.08744v3 Announce Type: replace-cross 
Abstract: Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08744">https://arxiv.org/abs/2310.08744</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on large language models and their behaviors which could have implications for controlling software and automation tasks. However, it does not directly address the use of large language models for controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.12973" target="_blank">Frozen Transformers in Language Models Are Effective Visual Encoder Layers</a></h3>
            
            <p><strong>Authors:</strong> Ziqi Pang, Ziyang Xie, Yunze Man, Yu-Xiong Wang</p>
            <p><strong>Summary:</strong> arXiv:2310.12973v2 Announce Type: replace-cross 
Abstract: This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -- the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms. Code is available at https://github.com/ziqipang/LM4VisualEncoding.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12973">https://arxiv.org/abs/2310.12973</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this research isn't dealing directly with control software or browsers, it emphasizes a novel approach of employing large language models for tasks beyond language, including computer vision tasks and motion forecasting. Therefore, it may provide new perspectives for applications of large language models, including controlling software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02338" target="_blank">NetLLM: Adapting Large Language Models for Networking</a></h3>
            
            <p><strong>Authors:</strong> Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui, Fangxin Wang</p>
            <p><strong>Summary:</strong> arXiv:2402.02338v2 Announce Type: replace-cross 
Abstract: Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.
  Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in LLM adaptation, from how to process task-specific information with LLMs, to how to improve the efficiency of answer generation and acquiring domain knowledge for networking. Across three networking-related use cases - viewport prediction (VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we demonstrate the effectiveness of NetLLM in LLM adaptation for networking, and showcase that the adapted LLM significantly outperforms state-of-the-art algorithms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02338">https://arxiv.org/abs/2402.02338</a></p>
            <p><strong>Category:</strong> cs.NI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the study is centered around networking, the paper is focused on the adaptation and usage of large language models (LLMs) for various tasks. This makes it relevant to your research interest in agents based on large language models for software and web browers control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.18312" target="_blank">How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</a></h3>
            
            <p><strong>Authors:</strong> Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty</p>
            <p><strong>Summary:</strong> arXiv:2402.18312v2 Announce Type: replace-cross 
Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.18312">https://arxiv.org/abs/2402.18312</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper provides valuable insights into the working mechanisms of Large Language Models (LLMs) and how they reason; this knowledge can prove pivotal in devising strategies for computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.19379" target="_blank">Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy</a></h3>
            
            <p><strong>Authors:</strong> Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock</p>
            <p><strong>Summary:</strong> arXiv:2402.19379v4 Announce Type: replace-cross 
Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.19379">https://arxiv.org/abs/2402.19379</a></p>
            <p><strong>Category:</strong> cs.CY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it explores the forecasting capability of large language models, which is closely related to the concept of controlling software or automation using large language models. Moreover, it discusses about how multiple LLMs can work in ensemble, demonstrating a methodology that you might be interested in and can potentially apply in your agent-based projects.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.15246" target="_blank">FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions</a></h3>
            
            <p><strong>Authors:</strong> Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini</p>
            <p><strong>Summary:</strong> arXiv:2403.15246v2 Announce Type: replace-cross 
Abstract: Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions -- also known as narratives -- developed for professional assessors to evaluate retrieval systems. In particular, we build our benchmark from three collections curated for shared tasks at the Text REtrieval Conference (TREC). These collections contains hundreds to thousands of labeled documents per query, making them suitable for our exploration. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements after fine-tuning on our training set.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.15246">https://arxiv.org/abs/2403.15246</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it discusses the use of large language models in Information Retrieval systems which can be seen as a form of software control. It's not directly about controlling web browsers or specifically about automation, but the concept of interpreting and responding to complex instructions can certainly be applicable to those areas.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02358" target="_blank">A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Mode</a></h3>
            <a href="https://arxiv.org/html/2405.02358v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.02358v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, Fugee Tsung</p>
            <p><strong>Summary:</strong> arXiv:2405.02358v1 Announce Type: new 
Abstract: Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely \textbf{pre-training foundation models from scratch for time series} and \textbf{adapting large language foundation models for time series}. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely \textbf{Effectiveness}, \textbf{Efficiency} and \textbf{Explainability}. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series.Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02358">https://arxiv.org/abs/2405.02358</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper seems highly relevant to your interests. It not only covers time series and deep learning but also discusses new foundation models and datasets for time series data as well as the adaptation of large language foundation models for time series analysis.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03429" target="_blank">ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers</a></h3>
            
            <p><strong>Authors:</strong> Arvid Weyrauch, Thomas Steens, Oskar Taubert, Benedikt Hanke, Aslan Eqbal, Ewa G\"otz, Achim Streit, Markus G\"otz, Charlotte Debus</p>
            <p><strong>Summary:</strong> arXiv:2405.03429v1 Announce Type: new 
Abstract: Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases. Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous, leading to models with computational demand infeasible for most practical applications. To bridge the gap between high method complexity and realistic computational resources, we introduce the Residual Cyclic Transformer, ReCycle. ReCycle utilizes primary cycle compression to address the computational complexity of the attention mechanism in long time series. By learning residuals from refined smoothing average techniques, ReCycle surpasses state-of-the-art accuracy in a variety of application use cases. The reliable and explainable fallback behavior ensured by simple, yet robust, smoothing average techniques additionally lowers the barrier for user acceptance. At the same time, our approach reduces the run time and energy consumption by more than an order of magnitude, making both training and inference feasible on low-performance, low-power and edge computing devices. Code is available at https://github.com/Helmholtz-AI-Energy/ReCycle</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03429">https://arxiv.org/abs/2405.03429</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new Transformer-like model (called the Residual Cyclic Transformer, or ReCycle) specifically designed for time series forecasting. The advancements are concerning both the accuracy and the computational efficiency of the model, which are two aspects you mentioned you're interested in. Furthermore, this new model is also beneficial for use cases with restricted computational resources.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02357" target="_blank">Large Language Models for Mobility in Transportation Systems: A Survey on Forecasting Tasks</a></h3>
            
            <p><strong>Authors:</strong> Zijian Zhang, Yujie Sun, Zepu Wang, Yuqi Nie, Xiaobo Ma, Peng Sun, Ruolin Li</p>
            <p><strong>Summary:</strong> arXiv:2405.02357v1 Announce Type: new 
Abstract: Mobility analysis is a crucial element in the research area of transportation systems. Forecasting traffic information offers a viable solution to address the conflict between increasing transportation demands and the limitations of transportation infrastructure. Predicting human travel is significant in aiding various transportation and urban management tasks, such as taxi dispatch and urban planning. Machine learning and deep learning methods are favored for their flexibility and accuracy. Nowadays, with the advent of large language models (LLMs), many researchers have combined these models with previous techniques or applied LLMs to directly predict future traffic information and human travel behaviors. However, there is a lack of comprehensive studies on how LLMs can contribute to this field. This survey explores existing approaches using LLMs for mobility forecasting problems. We provide a literature review concerning the forecasting applications within transportation systems, elucidating how researchers utilize LLMs, showcasing recent state-of-the-art advancements, and identifying the challenges that must be overcome to fully leverage LLMs in this domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02357">https://arxiv.org/abs/2405.02357</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Though this paper does not directly propose new models or methods, it involves the use of large language models in forecasting tasks within transportation systems, which comes under time series. It also provides a comprehensive review of state-of-the-art advancements and challenges in this field.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02670" target="_blank">From Generalization Analysis to Optimization Designs for State Space Models</a></h3>
            
            <p><strong>Authors:</strong> Fusheng Liu, Qianxiao Li</p>
            <p><strong>Summary:</strong> arXiv:2405.02670v1 Announce Type: new 
Abstract: A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02670">https://arxiv.org/abs/2405.02670</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper should be of interest as it discusses the use of State Space Models (a foundation model) in time series analysis and proposes improvements to their training algorithms. While it's not a new model per se, it provides novel methods for initialization and regularization in SSMs related to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02766" target="_blank">Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning</a></h3>
            
            <p><strong>Authors:</strong> Fahad Sarfraz, Bahram Zonooz, Elahe Arani</p>
            <p><strong>Summary:</strong> arXiv:2405.02766v1 Announce Type: new 
Abstract: While humans excel at continual learning (CL), deep neural networks (DNNs) exhibit catastrophic forgetting. A salient feature of the brain that allows effective CL is that it utilizes multiple modalities for learning and inference, which is underexplored in DNNs. Therefore, we study the role and interactions of multiple modalities in mitigating forgetting and introduce a benchmark for multimodal continual learning. Our findings demonstrate that leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations. This makes the model less vulnerable to modality-specific regularities and considerably mitigates forgetting. Furthermore, we observe that individual modalities exhibit varying degrees of robustness to distribution shift. Finally, we propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality. Our method sets a strong baseline that enables both single- and multimodal inference. Our study provides a promising case for further exploring the role of multiple modalities in enabling CL and provides a standard benchmark for future research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02766">https://arxiv.org/abs/2405.02766</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses the development and use of deep neural networks to handle multiple modalities. This is in relation to your interest in multimodal deep learning models for time series, though the paper is not specifically about time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03140" target="_blank">TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning</a></h3>
            
            <p><strong>Authors:</strong> Xiwen Chen, Peijie Qiu, Wenhui Zhu, Huayu Li, Hao Wang, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</p>
            <p><strong>Summary:</strong> arXiv:2405.03140v1 Announce Type: new 
Abstract: Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03140">https://arxiv.org/abs/2405.03140</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents the novel method TimeMIL for multivariate time series classification, incorporating a weakly supervised approach and a specific learnable wavelet positional token. Though it does not touch on all your sub-interests in time series and deep learning, it covers 'new deep learning methods for time series'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03199" target="_blank">Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Nannan Bian, Minhong Zhu, Li Chen, Weiran Cai</p>
            <p><strong>Summary:</strong> arXiv:2405.03199v1 Announce Type: new 
Abstract: Deep learning methods have been exerting their strengths in long-term time series forecasting. However, they often struggle to strike a balance between expressive power and computational efficiency. Here, we propose the Coarsened Perceptron Network (CP-Net), a novel architecture that efficiently enhances the predictive capability of MLPs while maintains a linear computational complexity. It utilizes a coarsening strategy as the backbone that leverages two-stage convolution-based sampling blocks. Based purely on convolution, they provide the functionality of extracting short-term semantic and contextual patterns, which is relatively deficient in the global point-wise projection of the MLP layer. With the architectural simplicity and low runtime, our experiments on seven time series forecasting benchmarks demonstrate that CP-Net achieves an improvement of 4.1% compared to the SOTA method. The model further shows effective utilization of the exposed information with a consistent improvement as the look-back window expands.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03199">https://arxiv.org/abs/2405.03199</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is of relevance to your interests because it presents a new deep learning method (CP-Net) for time series forecasting. However, it does not involve foundation models, multimodal models or transformer-like models, thus its relevance is not maximum.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03255" target="_blank">Multi-Modality Spatio-Temporal Forecasting via Self-Supervised Learning</a></h3>
            
            <p><strong>Authors:</strong> Jiewen Deng, Renhe Jiang, Jiaqi Zhang, Xuan Song</p>
            <p><strong>Summary:</strong> arXiv:2405.03255v1 Announce Type: new 
Abstract: Multi-modality spatio-temporal (MoST) data extends spatio-temporal (ST) data by incorporating multiple modalities, which is prevalent in monitoring systems, encompassing diverse traffic demands and air quality assessments. Despite significant strides in ST modeling in recent years, there remains a need to emphasize harnessing the potential of information from different modalities. Robust MoST forecasting is more challenging because it possesses (i) high-dimensional and complex internal structures and (ii) dynamic heterogeneity caused by temporal, spatial, and modality variations. In this study, we propose a novel MoST learning framework via Self-Supervised Learning, namely MoSSL, which aims to uncover latent patterns from temporal, spatial, and modality perspectives while quantifying dynamic heterogeneity. Experiment results on two real-world MoST datasets verify the superiority of our approach compared with the state-of-the-art baselines. Model implementation is available at https://github.com/beginner-sketch/MoSSL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03255">https://arxiv.org/abs/2405.03255</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Since this paper presents a novel framework for Multi-modality Spatio-Temporal (MoST) forecasting via Self-Supervised Learning, it is relevant to your interest in new multimodal deep learning models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03320" target="_blank">Denoising of Geodetic Time Series Using Spatiotemporal Graph Neural Networks: Application to Slow Slip Event Extraction</a></h3>
            <a href="https://arxiv.org/html/2405.03320v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03320v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Giuseppe Costantino, Sophie Giffard-Roisin, Mauro Dalla Mura, Anne Socquet</p>
            <p><strong>Summary:</strong> arXiv:2405.03320v1 Announce Type: new 
Abstract: Geospatial data has been transformative for the monitoring of the Earth, yet, as in the case of (geo)physical monitoring, the measurements can have variable spatial and temporal sampling and may be associated with a significant level of perturbations degrading the signal quality. Denoising geospatial data is, therefore, essential, yet often challenging because the observations may comprise noise coming from different origins, including both environmental signals and instrumental artifacts, which are spatially and temporally correlated, thus hard to disentangle. This study addresses the denoising of multivariate time series acquired by irregularly distributed networks of sensors, requiring specific methods to handle the spatiotemporal correlation of the noise and the signal of interest. Specifically, our method focuses on the denoising of geodetic position time series, used to monitor ground displacement worldwide with centimeter- to-millimeter precision. Among the signals affecting GNSS data, slow slip events (SSEs) are of interest to seismologists. These are transients of deformation that are weakly emerging compared to other signals. Here, we design SSEdenoiser, a multi-station spatiotemporal graph-based attentive denoiser that learns latent characteristics of GNSS noise to reveal SSE-related displacement with sub-millimeter precision. It is based on the key combination of graph recurrent networks and spatiotemporal Transformers. The proposed method is applied to the Cascadia subduction zone, where SSEs occur along with bursts of tectonic tremors, a seismic rumbling identified from independent seismic recordings. The extracted events match the spatiotemporal evolution of tremors. This good space-time correlation of the denoised GNSS signals with the tremors validates the proposed denoising procedure.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03320">https://arxiv.org/abs/2405.03320</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new approach combining graph recurrent networks and spatiotemporal transformers for time series denoising, which is an important aspect of time series analysis and can be particularly relevant for forecasting applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03582" target="_blank">Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Christian Kl\"otergens, Vijaya Krishna Yalavarthi, Maximilian Stubbemann, Lars Schmidt-Thieme</p>
            <p><strong>Summary:</strong> arXiv:2405.03582v1 Announce Type: new 
Abstract: Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03582">https://arxiv.org/abs/2405.03582</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant for you as it proposes a new method, Functional Latent Dynamics (FLD), for handling irregularly sampled time series data in deep learning models. This directly corresponds to your interest in 'New deep learning methods for time series'. The paper does not mention usage in multimodal or transformer-like models, and there's no explicit reference to foundation models, thus a score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07626" target="_blank">Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</a></h3>
            <a href="https://arxiv.org/html/2310.07626v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.07626v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria</p>
            <p><strong>Summary:</strong> arXiv:2310.07626v3 Announce Type: replace 
Abstract: Satellite-based remote sensing missions have revolutionized our understanding of the Ocean state and dynamics. Among them, space-borne altimetry provides valuable Sea Surface Height (SSH) measurements, used to estimate surface geostrophic currents. Due to the sensor technology employed, important gaps occur in SSH observations. Complete SSH maps are produced using linear Optimal Interpolations (OI) such as the widely-used Data Unification and Altimeter Combination System (DUACS). On the other hand, Sea Surface Temperature (SST) products have much higher data coverage and SST is physically linked to geostrophic currents through advection. We propose a new multi-variate Observing System Simulation Experiment (OSSE) emulating 20 years of SSH and SST satellite observations. We train an Attention-Based Encoder-Decoder deep learning network (\textsc{abed}) on this data, comparing two settings: one with access to ground truth during training and one without. On our OSSE, we compare ABED reconstructions when trained using either supervised or unsupervised loss functions, with or without SST information. We evaluate the SSH interpolations in terms of eddy detection. We also introduce a new way to transfer the learning from simulation to observations: supervised pre-training on our OSSE followed by unsupervised fine-tuning on satellite data. Based on real SSH observations from the Ocean Data Challenge 2021, we find that this learning strategy, combined with the use of SST, decreases the root mean squared error by 24% compared to OI.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07626">https://arxiv.org/abs/2310.07626</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new deep learning method (ABED), and uses a new approach of training the model using multi-variate (multimodal) data of SSH and SST for time series forecasting. However, it is more applied to real-world scenario so it might not fully satisfy your preference for methods-focused papers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.16310" target="_blank">REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories</a></h3>
            
            <p><strong>Authors:</strong> Bangchao Deng, Bingqing Qu, Pengyang Wang, Dingqi Yang, Benjamin Fankhauser, Philippe Cudre-Mauroux</p>
            <p><strong>Summary:</strong> arXiv:2402.16310v2 Announce Type: replace 
Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\%-10.9\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.16310">https://arxiv.org/abs/2402.16310</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces REPLAY, a new deep learning method for predicting location based on time-varying temporal regularities in human mobility data, which can be considered as a kind of time series. However, it does not directly relate to the 'transformer-like models' or 'multimodal' subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08472" target="_blank">TSLANet: Rethinking Transformers for Time Series Representation Learning</a></h3>
            
            <p><strong>Authors:</strong> Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li</p>
            <p><strong>Summary:</strong> arXiv:2404.08472v2 Announce Type: replace 
Abstract: Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal convolutional model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature representation and to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive Convolution Block and leverage self-supervised learning to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and anomaly detection, showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at https://github.com/emadeldeen24/TSLANet.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08472">https://arxiv.org/abs/2404.08472</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper presents a new deep learning model, TSLANet, designed specifically for time series. While it doesn't appear to be transformer-like or multimodal, it seems to be a novel method for time series and also contributes to datasets for model training in this field.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.00809" target="_blank">Towards Causal Foundation Model: on Duality between Causal Inference and Attention</a></h3>
            <a href="https://arxiv.org/html/2310.00809v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.00809v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma</p>
            <p><strong>Summary:</strong> arXiv:2310.00809v2 Announce Type: replace 
Abstract: Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.00809">https://arxiv.org/abs/2310.00809</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper directly addresses the intersection of causality and machine learning, specifically the development of causally-aware foundation models. It also proposes a novel method for causal inference, relevant to your interest in causal representation learning and causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02726" target="_blank">A Mathematical Model of the Hidden Feedback Loop Effect in Machine Learning Systems</a></h3>
            <a href="https://arxiv.org/html/2405.02726v1/extracted/5577054/fig1_Normal.png" target="_blank"><img src="https://arxiv.org/html/2405.02726v1/extracted/5577054/fig1_Normal.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andrey Veprikov, Alexander Afanasiev, Anton Khritankov</p>
            <p><strong>Summary:</strong> arXiv:2405.02726v1 Announce Type: new 
Abstract: Widespread deployment of societal-scale machine learning systems necessitates a thorough understanding of the resulting long-term effects these systems have on their environment, including loss of trustworthiness, bias amplification, and violation of AI safety requirements. We introduce a repeated learning process to jointly describe several phenomena attributed to unintended hidden feedback loops, such as error amplification, induced concept drift, echo chambers and others. The process comprises the entire cycle of obtaining the data, training the predictive model, and delivering predictions to end-users within a single mathematical model. A distinctive feature of such repeated learning setting is that the state of the environment becomes causally dependent on the learner itself over time, thus violating the usual assumptions about the data distribution. We present a novel dynamical systems model of the repeated learning process and prove the limiting set of probability distributions for positive and negative feedback loop modes of the system operation. We conduct a series of computational experiments using an exemplary supervised learning problem on two synthetic data sets. The results of the experiments correspond to the theoretical predictions derived from the dynamical model. Our results demonstrate the feasibility of the proposed approach for studying the repeated learning processes in machine learning systems and open a range of opportunities for further research in the area.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02726">https://arxiv.org/abs/2405.02726</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses the effect of machine learning systems on their environment with emphasis on causal dependence. This aligns with your interest in causal discovery and causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03056" target="_blank">Convolutional Learning on Directed Acyclic Graphs</a></h3>
            
            <p><strong>Authors:</strong> Samuel Rey, Hamed Ajorlou, Gonzalo Mateos</p>
            <p><strong>Summary:</strong> arXiv:2405.03056v1 Announce Type: new 
Abstract: We develop a novel convolutional architecture tailored for learning from data defined over directed acyclic graphs (DAGs). DAGs can be used to model causal relationships among variables, but their nilpotent adjacency matrices pose unique challenges towards developing DAG signal processing and machine learning tools. To address this limitation, we harness recent advances offering alternative definitions of causal shifts and convolutions for signals on DAGs. We develop a novel convolutional graph neural network that integrates learnable DAG filters to account for the partial ordering induced by the graph topology, thus providing valuable inductive bias to learn effective representations of DAG-supported data. We discuss the salient advantages and potential limitations of the proposed DAG convolutional network (DCN) and evaluate its performance on two learning tasks using synthetic data: network diffusion estimation and source identification. DCN compares favorably relative to several baselines, showcasing its promising potential.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03056">https://arxiv.org/abs/2405.03056</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems quite relevant to your interest in 'Causality and Machine Learning'. It specifically mentions the use of Directed Acyclic Graphs (DAGs) for modeling causal relationships, aligning with your second main topic. The development of a novel convolutional architecture for learning from data defined over DAGs relates to 'Causal Discovery' sub-topic. However, it does not directly address causal representation learning or use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03342" target="_blank">Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning</a></h3>
            <a href="https://arxiv.org/html/2405.03342v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03342v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Weilin Chen, Ruichu Cai, Zeqin Yang, Jie Qiao, Yuguang Yan, Zijian Li, Zhifeng Hao</p>
            <p><strong>Summary:</strong> arXiv:2405.03342v1 Announce Type: new 
Abstract: Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03342">https://arxiv.org/abs/2405.03342</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in causality and machine learning as it discusses a novel causal effect estimator. This relates to your subtopic interest in causal discovery. The paper also utilizes neural networks, indicating relevance to your interest in new methods employed in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03083" target="_blank">Causal K-Means Clustering</a></h3>
            <a href="https://arxiv.org/html/2405.03083v1/extracted/5577503/FIG/hyperbola.png" target="_blank"><img src="https://arxiv.org/html/2405.03083v1/extracted/5577503/FIG/hyperbola.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kwangho Kim, Jisu Kim, Edward H. Kennedy</p>
            <p><strong>Summary:</strong> arXiv:2405.03083v1 Announce Type: cross 
Abstract: Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03083">https://arxiv.org/abs/2405.03083</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is centered around the concept of causality, particularly in discovering causal effects across subgroups. It proposes a new solution known as Causal k-Means Clustering, which is relevant to your interest in causal discovery. However, it doesn't directly relate to machine learning or the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03130" target="_blank">Deep Learning for Causal Inference: A Comparison of Architectures for Heterogeneous Treatment Effect Estimation</a></h3>
            
            <p><strong>Authors:</strong> Demetrios Papakostas, Andrew Herren, P. Richard Hahn, Francisco Castillo</p>
            <p><strong>Summary:</strong> arXiv:2405.03130v1 Announce Type: cross 
Abstract: Causal inference has gained much popularity in recent years, with interests ranging from academic, to industrial, to educational, and all in between. Concurrently, the study and usage of neural networks has also grown profoundly (albeit at a far faster rate). What we aim to do in this blog write-up is demonstrate a Neural Network causal inference architecture. We develop a fully connected neural network implementation of the popular Bayesian Causal Forest algorithm, a state of the art tree based method for estimating heterogeneous treatment effects. We compare our implementation to existing neural network causal inference methodologies, showing improvements in performance in simulation settings. We apply our method to a dataset examining the effect of stress on sleep.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03130">https://arxiv.org/abs/2405.03130</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper corresponds with your interest in 'Causality and machine learning'. It implements a neural network model for causal inference, fulfilling your criteria of new methods. However, it doesn't directly involve large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2209.13816" target="_blank">Revisiting Few-Shot Learning from a Causal Perspective</a></h3>
            <a href="https://arxiv.org/html/2209.13816v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2209.13816v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Guoliang Lin, Hanjiang Lai</p>
            <p><strong>Summary:</strong> arXiv:2209.13816v2 Announce Type: replace 
Abstract: Few-shot learning with $N$-way $K$-shot scheme is an open challenge in machine learning. Many metric-based approaches have been proposed to tackle this problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these approaches have shown significant progress, the mechanism of why these methods succeed has not been well explored. In this paper, we try to interpret these metric-based few-shot learning methods via causal mechanism. We show that the existing approaches can be viewed as specific forms of front-door adjustment, which can alleviate the effect of spurious correlations and thus learn the causality. This causal interpretation could provide us a new perspective to better understand these existing metric-based methods. Further, based on this causal interpretation, we simply introduce two causal methods for metric-based few-shot learning, which considers not only the relationship between examples but also the diversity of representations. Experimental results demonstrate the superiority of our proposed methods in few-shot classification on various benchmark datasets. Code is available in https://github.com/lingl1024/causalFewShot.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2209.13816">https://arxiv.org/abs/2209.13816</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper explores causal mechanism in few-shot learning. Even though it doesn't directly deal with causal representation learning or causal discovery, it presents causal interpretations of the performance of metric-based few-shot learning methods. Also, this might contribute towards your understanding of causality in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.10614" target="_blank">Identifiable causal inference with noisy treatment and no side information</a></h3>
            <a href="https://arxiv.org/html/2306.10614v2/extracted/5576501/ceme_vs_ce_simple.jpg" target="_blank"><img src="https://arxiv.org/html/2306.10614v2/extracted/5576501/ceme_vs_ce_simple.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Antti P\"oll\"anen, Pekka Marttinen</p>
            <p><strong>Summary:</strong> arXiv:2306.10614v2 Announce Type: replace 
Abstract: In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method's good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.10614">https://arxiv.org/abs/2306.10614</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it addresses the topic of causal inference in the presence of noisy treatment data. It introduces a novel method that makes use of deep learning to overcome the challenges associated with such data. However, it does not discuss using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.13005" target="_blank">Towards Counterfactual Fairness-aware Domain Generalization in Changing Environments</a></h3>
            
            <p><strong>Authors:</strong> Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, Haifeng Chen</p>
            <p><strong>Summary:</strong> arXiv:2309.13005v2 Announce Type: replace 
Abstract: Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges related to unfair classification. Our strategy is rooted in the principles of causal inference to tackle these dual issues. To examine the intricate relationship between semantic information, sensitive attributes, and environmental cues, we systematically categorize exogenous uncertainty factors into four latent variables: 1) semantic information influenced by sensitive attributes, 2) semantic information unaffected by sensitive attributes, 3) environmental cues influenced by sensitive attributes, and 4) environmental cues unaffected by sensitive attributes. By incorporating fairness regularization, we exclusively employ semantic information for classification purposes. Empirical validation on synthetic and real-world datasets substantiates the effectiveness of our approach, demonstrating improved accuracy levels while ensuring the preservation of fairness in the evolving landscape of continuous domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.13005">https://arxiv.org/abs/2309.13005</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper presents an innovative solution to domain generalization and fairness issues by applying causal inference principles. This might be of interest to you because it combines several elements of machine learning with a focus on causal representation. However, it does not explicitly mention the usage of large language models for causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.03309" target="_blank">Neural Structure Learning with Stochastic Differential Equations</a></h3>
            
            <p><strong>Authors:</strong> Benjie Wang, Joel Jennings, Wenbo Gong</p>
            <p><strong>Summary:</strong> arXiv:2311.03309v2 Announce Type: replace 
Abstract: Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, SCOTCH, which combines neural stochastic differential equations (SDE) with variational inference to infer a posterior distribution over possible structures. This continuous-time approach can naturally handle both learning from and predicting observations at arbitrary time points. Theoretically, we establish sufficient conditions for an SDE and SCOTCH to be structurally identifiable, and prove its consistency under infinite data limits. Empirically, we demonstrate that our approach leads to improved structure learning performance on both synthetic and real-world datasets compared to relevant baselines under regular and irregular sampling intervals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.03309">https://arxiv.org/abs/2311.03309</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper introduces a new structure learning method which can help in causal discovery. It offers a fresh perspective on causal representation learning using stochastic processes, which are especially important in temporal aspects.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 07, 2024 at 21:50:20</div></body></html>