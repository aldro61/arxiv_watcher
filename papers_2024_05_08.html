
            <html>
            <head>
                <title>Report Generated on May 08, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 08, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03735" target="_blank">Select to Perfect: Imitating desired behavior from large multi-agent data</a></h3>
            <a href="https://arxiv.org/html/2405.03735v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03735v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tim Franzmeyer, Edith Elkind, Philip Torr, Jakob Foerster, Joao Henriques</p>
            <p><strong>Summary:</strong> arXiv:2405.03735v1 Announce Type: new 
Abstract: AI agents are commonly trained with large datasets of demonstrations of human behavior. However, not all behaviors are equally safe or desirable. Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories. For example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred. We first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents. This allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents. To enable this, we propose the concept of an agent's Exchange Value, which quantifies an individual agent's contribution to the collective desirability score. The Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent. We propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines. The project website can be found at https://tinyurl.com/select-to-perfect.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03735">https://arxiv.org/abs/2405.03735</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper involves the use of AI agents, a subfield relevant to your interest in large language model-based agents. In specific, it explores how AI agents can be trained to perform desired behaviors, a concept which could be translated to control software or web browsers, although not directly covered in the paper.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03869" target="_blank">Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions</a></h3>
            <a href="https://arxiv.org/html/2405.03869v1/extracted/5580190/figures/toy_full_comb3.png" target="_blank"><img src="https://arxiv.org/html/2405.03869v1/extracted/5580190/figures/toy_full_comb3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.03869v1 Announce Type: new 
Abstract: Influence functions offer a robust framework for assessing the impact of each training data sample on model predictions, serving as a prominent tool in data-centric learning. Despite their widespread use in various tasks, the strong convexity assumption on the model and the computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large deep models. This paper focuses on a classical data-centric scenario--trimming detrimental samples--and addresses both challenges within a unified framework. Specifically, we establish an equivalence transformation between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides profound insights into the role of the gradient in sample impact. Moreover, it relaxes the convexity assumption of influence functions, extending their applicability to non-convex deep models. Through systematic empirical evaluations, we first validate the correctness of our proposed outlier gradient analysis on synthetic datasets and then demonstrate its effectiveness in detecting mislabeled samples in vision models, selecting data samples for improving performance of transformer models for natural language processing, and identifying influential samples for fine-tuned Large Language Models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03869">https://arxiv.org/abs/2405.03869</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of your interest as it touches on the topic of improving the performance of Large Language Models, which corresponds to your interests in large-language model agents. Though it does not explicitly mention control automation, it can be useful for gaining insights into performance improvement approaches.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03917" target="_blank">KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization</a></h3>
            <a href="https://arxiv.org/html/2405.03917v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03917v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, Anshumali Shrivastava</p>
            <p><strong>Summary:</strong> arXiv:2405.03917v1 Announce Type: new 
Abstract: Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03917">https://arxiv.org/abs/2405.03917</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses Large Language Models (LLMs) and provides a new method for efficient LLM inference which can be applied in controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04118" target="_blank">Policy Learning with a Language Bottleneck</a></h3>
            
            <p><strong>Authors:</strong> Megha Srivastava, Cedric Colas, Dorsa Sadigh, Jacob Andreas</p>
            <p><strong>Summary:</strong> arXiv:2405.04118v1 Announce Type: new 
Abstract: Modern AI systems such as self-driving cars and game-playing agents achieve superhuman performance, but often lack human-like features such as generalization, interpretability and human inter-operability. Inspired by the rich interactions between language and decision-making in humans, we introduce Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI agents to generate linguistic rules that capture the strategies underlying their most rewarding behaviors. PLLB alternates between a rule generation step guided by language models, and an update step where agents learn new policies guided by rules. In a two-player communication game, a maze solving task, and two image reconstruction tasks, we show that PLLB agents are not only able to learn more interpretable and generalizable behaviors, but can also share the learned rules with human users, enabling more effective human-AI coordination.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04118">https://arxiv.org/abs/2405.04118</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large-language models. It discusses the Policy Learning with a Language Bottleneck (PLLB) framework, which uses language models to guide the development of policies for AI agents. This could be of interest when considering the use of large language models for tasks like controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04156" target="_blank">How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability</a></h3>
            <a href="https://arxiv.org/html/2405.04156v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.04156v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jorge Garc\'ia-Carrasco, Alejandro Mat\'e, Juan Trujillo</p>
            <p><strong>Summary:</strong> arXiv:2405.04156v1 Announce Type: new 
Abstract: Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern. Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components. In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms. Previous works in the MI field have focused so far on tasks that predict a single token. To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens. We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role. We also demonstrate that these heads concentrate the acronym prediction functionality. In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism. We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04156">https://arxiv.org/abs/2405.04156</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly investigates how the transformer architecture (exemplified by GPT-2) handles complex tasks. While it does not specifically reference software or browser control, the investigation into the underlying mechanisms of these large language models can contribute to the development of more effective agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04437" target="_blank">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a></h3>
            
            <p><strong>Authors:</strong> Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar</p>
            <p><strong>Summary:</strong> arXiv:2405.04437v1 Announce Type: new 
Abstract: Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.
  In this paper, we propose vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04437">https://arxiv.org/abs/2405.04437</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models (LLMs), and specifically talks about improving the efficiency of such models. It proposes a new method, vAttention, for dynamic memory management in LLMs, which can be critical in real-time control and automation applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03709" target="_blank">Generating Probabilistic Scenario Programs from Natural Language</a></h3>
            <a href="https://arxiv.org/html/2405.03709v1/extracted/5576235/figures/arch.png" target="_blank"><img src="https://arxiv.org/html/2405.03709v1/extracted/5576235/figures/arch.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Karim Elmaaroufi, Devan Shankar, Ana Cismaru, Marcell Vazquez-Chanlatte, Alberto Sangiovanni-Vincentelli, Matei Zaharia, Sanjit A. Seshia</p>
            <p><strong>Summary:</strong> arXiv:2405.03709v1 Announce Type: cross 
Abstract: For cyber-physical systems (CPS), including robotics and autonomous vehicles, mass deployment has been hindered by fatal errors that occur when operating in rare events. To replicate rare events such as vehicle crashes, many companies have created logging systems and employed crash reconstruction experts to meticulously recreate these valuable events in simulation. However, in these methods, "what if" questions are not easily formulated and answered. We present ScenarioNL, an AI System for creating scenario programs from natural language. Specifically, we generate these programs from police crash reports. Reports normally contain uncertainty about the exact details of the incidents which we represent through a Probabilistic Programming Language (PPL), Scenic. By using Scenic, we can clearly and concisely represent uncertainty and variation over CPS behaviors, properties, and interactions. We demonstrate how commonplace prompting techniques with the best Large Language Models (LLM) are incapable of reasoning about probabilistic scenario programs and generating code for low-resource languages such as Scenic. Our system is comprised of several LLMs chained together with several kinds of prompting strategies, a compiler, and a simulator. We evaluate our system on publicly available autonomous vehicle crash reports in California from the last five years and share insights into how we generate code that is both semantically meaningful and syntactically correct.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03709">https://arxiv.org/abs/2405.03709</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper showcases an AI system, ScenarioNL, that uses large language models for operating cyber-physical systems which could be classified under controlling software with large language models. However, it does not directly address web browsers or other automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03710" target="_blank">Automating the Enterprise with Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2405.03710v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03710v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michael Wornow, Avanika Narayan, Krista Opsahl-Ong, Quinn McIntyre, Nigam H. Shah, Christopher Re</p>
            <p><strong>Summary:</strong> arXiv:2405.03710v1 Announce Type: cross 
Abstract: Automating enterprise workflows could unlock $4 trillion/year in productivity gains. Despite being of interest to the data management community for decades, the ultimate vision of end-to-end workflow automation has remained elusive. Current solutions rely on process mining and robotic process automation (RPA), in which a bot is hard-coded to follow a set of predefined rules for completing a workflow. Through case studies of a hospital and large B2B enterprise, we find that the adoption of RPA has been inhibited by high set-up costs (12-18 months), unreliable execution (60% initial accuracy), and burdensome maintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such as GPT-4 offer a promising new approach for end-to-end workflow automation given their generalized reasoning and planning abilities. To study these capabilities we propose ECLAIR, a system to automate enterprise workflows with minimal human supervision. We conduct initial experiments showing that multimodal FMs can address the limitations of traditional RPA with (1) near-human-level understanding of workflows (93% accuracy on a workflow understanding task) and (2) instant set-up with minimal technical barrier (based solely on a natural language description of a workflow, ECLAIR achieves end-to-end completion rates of 40%). We identify human-AI collaboration, validation, and self-improvement as open challenges, and suggest ways they can be solved with data management techniques. Code is available at: https://github.com/HazyResearch/eclair-agents</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03710">https://arxiv.org/abs/2405.03710</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it explores the use of multimodal foundation models (like a large language model) for automating workflows, which could include software control. However, it doesn't specifically mention control of web browsers or new methods, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03727" target="_blank">Large Language Models Synergize with Automated Machine Learning</a></h3>
            <a href="https://arxiv.org/html/2405.03727v1/extracted/5578934/fig_2.png" target="_blank"><img src="https://arxiv.org/html/2405.03727v1/extracted/5578934/fig_2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinglue Xu, Zhen Liu, Nagar Anthel Venkatesh Suryanarayanan, Hitoshi Iba</p>
            <p><strong>Summary:</strong> arXiv:2405.03727v1 Announce Type: cross 
Abstract: Recently, code generation driven by large language models (LLMs) has become increasingly popular. However, automatically generating code for machine learning (ML) tasks still poses significant challenges. This paper explores the limits of program synthesis for ML by combining LLMs and automated machine learning (autoML). Specifically, our goal is to fully automate the code generation process for the entire ML workflow, from data preparation to modeling and post-processing, utilizing only textual descriptions of the ML tasks. To manage the length and diversity of ML programs, we propose to break each ML program into smaller, manageable parts. Each part is generated separately by the LLM, with careful consideration of their compatibilities. To implement the approach, we design a testing technique for ML programs. Furthermore, our approach enables integration with autoML. In our approach, autoML serves to numerically assess and optimize the ML programs generated by LLMs. LLMs, in turn, help to bridge the gap between theoretical, algorithm-centered autoML and practical autoML applications. This mutual enhancement underscores the synergy between LLMs and autoML in program synthesis for ML. In experiments across various ML tasks, our method outperforms existing methods in 10 out of 12 tasks for generating ML programs. In addition, autoML significantly improves the performance of the generated ML programs. In the experiments, our method, Text-to-ML, achieves fully automated synthesis of the entire ML pipeline based solely on textual descriptions of the ML tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03727">https://arxiv.org/abs/2405.03727</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a novel method of integrating Large Language Models (LLMs) and Automated Machine Learning (autoML), specifically being used for code generation in Machine Learning tasks. This aligns closely with your interest in using LLMs for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03963" target="_blank">ERATTA: Extreme RAG for Table To Answers with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.03963v1/extracted/5581038/images/sys.png" target="_blank"><img src="https://arxiv.org/html/2405.03963v1/extracted/5581038/images/sys.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra</p>
            <p><strong>Summary:</strong> arXiv:2405.03963v1 Announce Type: cross 
Abstract: Large language models (LLMs) with residual augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size. Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds. One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03963">https://arxiv.org/abs/2405.03963</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper's focus is on using large language models (LLMs) to interact with databases for question answering, which is related to controlling software with LLMs. However, it doesn't specifically address web browsers or automation, which slightly limits its relevance to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04053" target="_blank">Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT</a></h3>
            
            <p><strong>Authors:</strong> Hassan Shakil, Atqiya Munawara Mahi, Phuoc Nguyen, Zeydy Ortiz, Mamoun T. Mardini</p>
            <p><strong>Summary:</strong> arXiv:2405.04053v1 Announce Type: cross 
Abstract: This research examines the effectiveness of OpenAI's GPT models as independent evaluators of text summaries generated by six transformer-based models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS. We evaluated these summaries based on essential properties of high-quality summary - conciseness, relevance, coherence, and readability - using traditional metrics such as ROUGE and Latent Semantic Analysis (LSA). Uniquely, we also employed GPT not as a summarizer but as an evaluator, allowing it to independently assess summary quality without predefined metrics. Our analysis revealed significant correlations between GPT evaluations and traditional metrics, particularly in assessing relevance and coherence. The results demonstrate GPT's potential as a robust tool for evaluating text summaries, offering insights that complement established metrics and providing a basis for comparative analysis of transformer-based models in natural language processing tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04053">https://arxiv.org/abs/2405.04053</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not specifically discuss using large language models to control software or web browsers, it explores the capabilities of large language models as independent evaluators, which is applicable to computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04495" target="_blank">Toward In-Context Teaching: Adapting Examples to Students' Misconceptions</a></h3>
            <a href="https://arxiv.org/html/2405.04495v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.04495v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexis Ross, Jacob Andreas</p>
            <p><strong>Summary:</strong> arXiv:2405.04495v1 Announce Type: cross 
Abstract: When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04495">https://arxiv.org/abs/2405.04495</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models as pedagogical tools, which aligns with your interest in agents based on large-language models. However, it does not directly mention controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04520" target="_blank">NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts</a></h3>
            
            <p><strong>Authors:</strong> Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, Jie Tang</p>
            <p><strong>Summary:</strong> arXiv:2405.04520v1 Announce Type: cross 
Abstract: Large language models (LLMs) have manifested strong ability to generate codes for productive activities. However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding. To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains. Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction. Comparing with manual solutions, it achieves an efficiency increase of more than 4 times. Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval. On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB. The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04520">https://arxiv.org/abs/2405.04520</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents 'NaturalCodeBench', a benchmark for code synthesis from large language models. It's relevant to your interest in using large language models to control software as it deals with the ability of these models to generate codes, which could be used in software control and automation. However, its focus is more on the evaluation of the code synthesis capability of these models, and less on the development of new methods for controlling software, hence the score is not the highest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04533" target="_blank">ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning</a></h3>
            
            <p><strong>Authors:</strong> Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black</p>
            <p><strong>Summary:</strong> arXiv:2405.04533v1 Announce Type: cross 
Abstract: Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more. Each of these methods works in isolation instead of synergistically. Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods. To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs. In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans. The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding. Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks. ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04533">https://arxiv.org/abs/2405.04533</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it discusses the use of a large language model for tool selection and reasoning, similar to controlling software or automating tasks. However, it doesn't specifically mention controlling web browsers or specific software platforms, hence the score is 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.16242" target="_blank">ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality</a></h3>
            <a href="https://arxiv.org/html/2310.16242v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.16242v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yonchanok Khaokaew, Kaixin Ji, Thuc Hanh Nguyen, Hiruni Kegalle, Marwah Alaofi, Hao Xue, Flora D. Salim</p>
            <p><strong>Summary:</strong> arXiv:2310.16242v2 Announce Type: replace 
Abstract: This paper explores the intersection of technology and sleep pattern comprehension, presenting a cutting-edge two-stage framework that harnesses the power of Large Language Models (LLMs). The primary objective is to deliver precise sleep predictions paired with actionable feedback, addressing the limitations of existing solutions. This innovative approach involves leveraging the GLOBEM dataset alongside synthetic data generated by LLMs. The results highlight significant improvements, underlining the efficacy of merging advanced machine-learning techniques with a user-centric design ethos. Through this exploration, we bridge the gap between technological sophistication and user-friendly design, ensuring that our framework yields accurate predictions and translates them into actionable insights.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.16242">https://arxiv.org/abs/2310.16242</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper uses a large language model to enhance sleep quality and to produce actionable feedback. This is highly relevant to your interest in using large language models for tasks such as controlling software. However, it didn't directly mention automation or controlling web browsers, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12875" target="_blank">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a></h3>
            
            <p><strong>Authors:</strong> Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma</p>
            <p><strong>Summary:</strong> arXiv:2402.12875v2 Announce Type: replace 
Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12875">https://arxiv.org/abs/2402.12875</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It discusses improving the accuracy of large language models (LLMs) by generating a sequence of intermediate steps or 'Chain of Thought'. This approach expands the expressiveness of the model and enhances its problem-solving capability. While it does not specifically focus on controlling software or web browsers, the discussed techniques can contribute to computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09173" target="_blank">TransformerFAM: Feedback attention is working memory</a></h3>
            
            <p><strong>Authors:</strong> Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, Pedro Moreno Mengibar</p>
            <p><strong>Summary:</strong> arXiv:2404.09173v3 Announce Type: replace 
Abstract: While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09173">https://arxiv.org/abs/2404.09173</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is somewhat relevant to your interest. It doesn't explicitly mention controlling software or web browsers with large language models. However, the proposed model has the potential to improve the performance of Large Language Models on long-context tasks, which is useful for automating tasks with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.05950" target="_blank">Language Models as Black-Box Optimizers for Vision-Language Models</a></h3>
            <a href="https://arxiv.org/html/2309.05950v4/extracted/5581488/figures/promptgpt.jpg" target="_blank"><img src="https://arxiv.org/html/2309.05950v4/extracted/5581488/figures/promptgpt.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, Deva Ramanan</p>
            <p><strong>Summary:</strong> arXiv:2309.05950v4 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. As such, we aim to develop a black-box approach to optimize VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or even output logits. We propose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup, our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We highlight the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search. In addition, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different VLM architectures in a black-box manner. Lastly, we apply our framework to optimize the state-of-the-art black-box VLM (DALL-E 3) for text-to-image generation, prompt inversion, and personalization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.05950">https://arxiv.org/abs/2309.05950</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper describes the use of large language models (LLMs) to optimize vision-language models in a 'black box' manner. While it does not precisely align with controlling software or web browsers, it does provide insights into advanced uses of LLMs for optimization which could be relevant for envisioning similar applications in agent-based control. It represents a unique use case of LLMs broadly related to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.06692" target="_blank">An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.06692v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.06692v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gantavya Bhatt, Yifang Chen, Arnav M. Das, Jifan Zhang, Sang T. Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon S. Du, Kevin Jamieson, Jordan T. Ash, Robert D. Nowak</p>
            <p><strong>Summary:</strong> arXiv:2401.06692v2 Announce Type: replace-cross 
Abstract: Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimental design techniques and find that these methods consistently yield significant gains in label efficiency with little computational overhead. On generative tasks, our methods achieve the same generalization performance with only $50\%$ of annotation cost required by random sampling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.06692">https://arxiv.org/abs/2401.06692</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on the utilization of large language models, a topic you're interested in. It's relevant to your research because it discusses how to make annotation more efficient when using large language models, which could be particularly useful if you're considering implementing such models for software control or automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.06407" target="_blank">Rethinking How to Evaluate Language Model Jailbreak</a></h3>
            
            <p><strong>Authors:</strong> Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik</p>
            <p><strong>Summary:</strong> arXiv:2404.06407v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not. In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.06407">https://arxiv.org/abs/2404.06407</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper investigates the safety implications of Large Language Models (LLMs) and proposes a new method to evaluate their risk. This fits your interest in understanding the use and management of LLMs in context of software and automation. However, it focuses more on security than direct control. Therefore, the relevance is high but not maximum.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10517" target="_blank">Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs</a></h3>
            
            <p><strong>Authors:</strong> Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee</p>
            <p><strong>Summary:</strong> arXiv:2402.10517v3 Announce Type: replace 
Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. Our code is open-sourced and available online.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10517">https://arxiv.org/abs/2402.10517</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 3.5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper talks about the deployment of Large Language Models (LLMs) of varying sizes which is of interest to you. However, it is more focused on the deployment costs and efficiency, thereby not entirely targeting your interest in using LLMs for software control, web browser control and automation.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03892" target="_blank">Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows</a></h3>
            <a href="https://arxiv.org/html/2405.03892v1/extracted/5580249/FIG/cgraph1.png" target="_blank"><img src="https://arxiv.org/html/2405.03892v1/extracted/5580249/FIG/cgraph1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Minjae Cho, Jonathan P. How, Chuangchuang Sun</p>
            <p><strong>Summary:</strong> arXiv:2405.03892v1 Announce Type: new 
Abstract: Despite notable successes of Reinforcement Learning (RL), the prevalent use of an online learning paradigm prevents its widespread adoption, especially in hazardous or costly scenarios. Offline RL has emerged as an alternative solution, learning from pre-collected static datasets. However, this offline learning introduces a new challenge known as distributional shift, degrading the performance when the policy is evaluated on scenarios that are Out-Of-Distribution (OOD) from the training dataset. Most existing offline RL resolves this issue by regularizing policy learning within the information supported by the given dataset. However, such regularization overlooks the potential for high-reward regions that may exist beyond the dataset. This motivates exploring novel offline learning techniques that can make improvements beyond the data support without compromising policy performance, potentially by learning causation (cause-and-effect) instead of correlation from the dataset. In this paper, we propose the MOOD-CRL (Model-based Offline OOD-Adapting Causal RL) algorithm, which aims to address the challenge of extrapolation for offline policy training through causal inference instead of policy-regularizing methods. Specifically, Causal Normalizing Flow (CNF) is developed to learn the transition and reward functions for data generation and augmentation in offline policy evaluation and training. Based on the data-invariant, physics-based qualitative causal graph and the observational data, we develop a novel learning scheme for CNF to learn the quantitative structural causal model. As a result, CNF gains predictive and counterfactual reasoning capabilities for sequential decision-making tasks, revealing a high potential for OOD adaptation. Our CNF-based offline RL approach is validated through empirical evaluations, outperforming model-free and model-based methods by a significant margin.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03892">https://arxiv.org/abs/2405.03892</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Causality and machine learning'. It discusses the use of causal inference for offline policy training to tackle the challenge of extrapolation, a topic closely aligned with 'Causal discovery'. Furthermore, the paper introduces a new method, the MOOD-CRL algorithm, which also fits your interest in new methods over applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2209.13816" target="_blank">Revisiting Few-Shot Learning from a Causal Perspective</a></h3>
            
            <p><strong>Authors:</strong> Guoliang Lin, Yongheng Xu, Hanjiang Lai, Jian Yin</p>
            <p><strong>Summary:</strong> arXiv:2209.13816v3 Announce Type: replace 
Abstract: Few-shot learning with $N$-way $K$-shot scheme is an open challenge in machine learning. Many metric-based approaches have been proposed to tackle this problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these approaches have shown significant progress, the mechanism of why these methods succeed has not been well explored. In this paper, we try to interpret these metric-based few-shot learning methods via causal mechanism. We show that the existing approaches can be viewed as specific forms of front-door adjustment, which can alleviate the effect of spurious correlations and thus learn the causality. This causal interpretation could provide us a new perspective to better understand these existing metric-based methods. Further, based on this causal interpretation, we simply introduce two causal methods for metric-based few-shot learning, which considers not only the relationship between examples but also the diversity of representations. Experimental results demonstrate the superiority of our proposed methods in few-shot classification on various benchmark datasets. Code is available in https://github.com/lingl1024/causalFewShot.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2209.13816">https://arxiv.org/abs/2209.13816</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper brings a new perspective to understanding metric-based few-shot learning through the lens of causality, thus exploring the sub-topic of causal discovery in a new dimension.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.05429" target="_blank">Mitigating Nonlinear Algorithmic Bias in Binary Classification</a></h3>
            
            <p><strong>Authors:</strong> Wendy Hui, Wai Kwong Lau</p>
            <p><strong>Summary:</strong> arXiv:2312.05429v3 Announce Type: replace 
Abstract: This paper proposes the use of causal modeling to detect and mitigate algorithmic bias that is nonlinear in the protected attribute. We provide a general overview of our approach. We use the German Credit data set, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on age bias and the problem of binary classification. We show that the probability of getting correctly classified as "low risk" is lowest among young people. The probability increases with age nonlinearly. To incorporate the nonlinearity into the causal model, we introduce a higher order polynomial term. Based on the fitted causal model, the de-biased probability estimates are computed, showing improved fairness with little impact on overall classification accuracy. Causal modeling is intuitive and, hence, its use can enhance explicability and promotes trust among different stakeholders of AI.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.05429">https://arxiv.org/abs/2312.05429</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems highly relevant to your interest in causality and machine learning. It discusses the use of causal modeling for detecting and mitigating algorithmic bias. However, it doesn't directly address causal representation learning or causal discovery, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11355" target="_blank">Natural Language Counterfactuals through Representation Surgery</a></h3>
            
            <p><strong>Authors:</strong> Matan Avitan, Ryan Cotterell, Yoav Goldberg, Shauli Ravfogel</p>
            <p><strong>Summary:</strong> arXiv:2402.11355v3 Announce Type: replace-cross 
Abstract: Interventions targeting the representation space of language models (LMs) have emerged as an effective means to influence model behavior. Such methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations and, in so doing, create a counterfactual representation. However, because the intervention operates within the representation space, understanding precisely what aspects of the text it modifies poses a challenge. In this paper, we give a method to convert representation counterfactuals into string counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation space intervention and to interpret the features utilized to encode a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification through data augmentation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11355">https://arxiv.org/abs/2402.11355</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper fits under your causality interest. It discusses interventions and representation space that are commonly used in understanding causality. Although it does not specifically mention 'causal discovery', it does discuss model behavior modification, which is quite relevant to the causal representation learning subtopic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.01015" target="_blank">Network reconstruction via the minimum description length principle</a></h3>
            
            <p><strong>Authors:</strong> Tiago P. Peixoto</p>
            <p><strong>Summary:</strong> arXiv:2405.01015v2 Announce Type: replace-cross 
Abstract: A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. However, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight "shrinkage". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.01015">https://arxiv.org/abs/2405.01015</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to be relevant to your interest in causal discovery as it discusses the task of network reconstruction, a concept closely related to discovering causality in machine learning. It proposes a new nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, thus presenting a new method in the field.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02358" target="_blank">A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model</a></h3>
            
            <p><strong>Authors:</strong> Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, Fugee Tsung</p>
            <p><strong>Summary:</strong> arXiv:2405.02358v2 Announce Type: replace 
Abstract: Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02358">https://arxiv.org/abs/2405.02358</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This survey paper tackles numerous subtopics relevant to your interests in time series and deep learning. It addresses new methods for using large language models for time series, the issue of training foundation models for time series, and it includes resources about datasets and libraries. It appears to be very relevant as per your preferences in primarily new methods over applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03943" target="_blank">Predictive Modeling with Temporal Graphical Representation on Electronic Health Records</a></h3>
            
            <p><strong>Authors:</strong> Jiayuan Chen, Changchang Yin, Yuanlong Wang, Ping Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.03943v1 Announce Type: new 
Abstract: Deep learning-based predictive models, leveraging Electronic Health Records (EHR), are receiving increasing attention in healthcare. An effective representation of a patient's EHR should hierarchically encompass both the temporal relationships between historical visits and medical events, and the inherent structural information within these elements. Existing patient representation methods can be roughly categorized into sequential representation and graphical representation. The sequential representation methods focus only on the temporal relationships among longitudinal visits. On the other hand, the graphical representation approaches, while adept at extracting the graph-structured relationships between various medical events, fall short in effectively integrate temporal information. To capture both types of information, we model a patient's EHR as a novel temporal heterogeneous graph. This graph includes historical visits nodes and medical events nodes. It propagates structured information from medical event nodes to visit nodes and utilizes time-aware visit nodes to capture changes in the patient's health status. Furthermore, we introduce a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information. We validate the effectiveness of TRANS through extensive experiments on three real-world datasets. The results show that our proposed approach achieves state-of-the-art performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03943">https://arxiv.org/abs/2405.03943</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Time series and deep learning' as it discusses a new method of deep learning-based predictive models for time-series data with Temporal Graph Transformer. However, it is specifically pertaining to healthcare, which wasn't referenced in your subtopics. Hence, the 4 out of 5 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04252" target="_blank">VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Alireza Koochali, Ensiye Tahaei, Andreas Dengel, Sheraz Ahmed</p>
            <p><strong>Summary:</strong> arXiv:2405.04252v1 Announce Type: new 
Abstract: This paper presents VAEneu, an innovative autoregressive method for multistep ahead univariate probabilistic time series forecasting. We employ the conditional VAE framework and optimize the lower bound of the predictive distribution likelihood function by adopting the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function. This novel pipeline results in forecasting sharp and well-calibrated predictive distribution. Through a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets. The results unequivocally demonstrate VAEneu's remarkable forecasting performance. VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04252">https://arxiv.org/abs/2405.04252</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper seems quite relevant to your interests in time-series forecasting and 'New deep learning methods for time series'. It presents a novel autoregressive method (VAEneu) for time-series forecasting. Although it doesn't cover foundation models or transformer-like models for time series, its approach to univariate probabilistic time series forecasting might be appealing.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04517" target="_blank">xLSTM: Extended Long Short-Term Memory</a></h3>
            
            <p><strong>Authors:</strong> Maximilian Beck, Korbinian P\"oppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\"unter Klambauer, Johannes Brandstetter, Sepp Hochreiter</p>
            <p><strong>Summary:</strong> arXiv:2405.04517v1 Announce Type: new 
Abstract: In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04517">https://arxiv.org/abs/2405.04517</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interests in time series and deep learning specifically for proposing a new deep learning method - the xLSTM. Although it does not specifically mention time-series forecasting, it brings in innovations in LSTM, which is a crucial model in time series analysis and forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01801" target="_blank">Large Language Models for Time Series: A Survey</a></h3>
            <a href="https://arxiv.org/html/2402.01801v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.01801v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang</p>
            <p><strong>Summary:</strong> arXiv:2402.01801v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01801">https://arxiv.org/abs/2402.01801</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper provides an exploration and taxonomy of various methodologies for using Large Language Models (LLMs) for time series analysis. Although it doesn't introduce new methods, it offers a comprehensive overview which might be of interest. However, it isn't directly focused on forecasting, hence the score of 4.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 08, 2024 at 21:36:49</div></body></html>