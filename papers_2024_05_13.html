
            <html>
            <head>
                <title>Report Generated on May 13, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 13, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.05985" target="_blank">TrafficGPT: Towards Multi-Scale Traffic Analysis and Generation with Spatial-Temporal Agent Framework</a></h3>
            
            <p><strong>Authors:</strong> Jinhui Ouyang, Yijie Zhu, Xiang Yuan, Di Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.05985v1 Announce Type: new 
Abstract: The precise prediction of multi-scale traffic is a ubiquitous challenge in the urbanization process for car owners, road administrators, and governments. In the case of complex road networks, current and past traffic information from both upstream and downstream roads are crucial since various road networks have different semantic information about traffic. Rationalizing the utilization of semantic information can realize short-term, long-term, and unseen road traffic prediction. As the demands of multi-scale traffic analysis increase, on-demand interactions and visualizations are expected to be available for transportation participants. We have designed a multi-scale traffic generation system, namely TrafficGPT, using three AI agents to process multi-scale traffic data, conduct multi-scale traffic analysis, and present multi-scale visualization results. TrafficGPT consists of three essential AI agents: 1) a text-to-demand agent that is employed with Question & Answer AI to interact with users and extract prediction tasks through texts; 2) a traffic prediction agent that leverages multi-scale traffic data to generate temporal features and similarity, and fuse them with limited spatial features and similarity, to achieve accurate prediction of three tasks; and 3) a suggestion and visualization agent that uses the prediction results to generate suggestions and visualizations, providing users with a comprehensive understanding of traffic conditions. Our TrafficGPT system focuses on addressing concerns about traffic prediction from transportation participants, and conducted extensive experiments on five real-world road datasets to demonstrate its superior predictive and interactive performance</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.05985">https://arxiv.org/abs/2405.05985</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper involves the use of three different AI agents, including one which uses a large language model to interact with users and predict traffic needs. While not directly focused on controlling software or web browsers, it does show application of large language models in automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06001" target="_blank">LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.06001v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06001v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Yunchen Zhang, Xianglong Liu, Dacheng Tao</p>
            <p><strong>Summary:</strong> arXiv:2405.06001v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence, thanks to their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements of LLMs limit their widespread adoption. Quan- tization, a key compression technique, offers a viable solution to mitigate these demands by compressing and accelerating LLMs, albeit with poten- tial risks to model accuracy. Numerous studies have aimed to minimize the accuracy loss associated with quantization. However, the quantization configurations in these studies vary and may not be optimized for hard- ware compatibility. In this paper, we focus on identifying the most effective practices for quantizing LLMs, with the goal of balancing performance with computational efficiency. For a fair analysis, we develop a quantization toolkit LLMC, and design four crucial principles considering the inference efficiency, quantized accuracy, calibration cost, and modularization. By benchmarking on various models and datasets with over 500 experiments, three takeaways corresponding to calibration data, quantization algorithm, and quantization schemes are derived. Finally, a best practice of LLM PTQ pipeline is constructed. All the benchmark results and the toolkit can be found at https://github.com/ModelTC/llmc.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06001">https://arxiv.org/abs/2405.06001</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about quantization techniques for large language models, which are substantial components of agent-based systems. Though it does not directly discuss controlling software or web browsers, understanding the process of quantizing these models can be very beneficial for efficiently deploying such agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06063" target="_blank">A Minimalist Prompt for Zero-Shot Policy Learning</a></h3>
            <a href="https://arxiv.org/html/2405.06063v1/extracted/5587554/figures/model.png" target="_blank"><img src="https://arxiv.org/html/2405.06063v1/extracted/5587554/figures/model.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Meng Song, Xuezhi Wang, Tanay Biradar, Yao Qin, Manmohan Chandraker</p>
            <p><strong>Summary:</strong> arXiv:2405.06063v1 Announce Type: new 
Abstract: Transformer-based methods have exhibited significant generalization ability when prompted with target-domain demonstrations or example solutions during inference. Although demonstrations, as a way of task specification, can capture rich information that may be hard to specify by language, it remains unclear what information is extracted from the demonstrations to help generalization. Moreover, assuming access to demonstrations of an unseen task is impractical or unreasonable in many real-world scenarios, especially in robotics applications. These questions motivate us to explore what the minimally sufficient prompt could be to elicit the same level of generalization ability as the demonstrations. We study this problem in the contextural RL setting which allows for quantitative measurement of generalization and is commonly adopted by meta-RL and multi-task RL benchmarks. In this setting, the training and test Markov Decision Processes (MDPs) only differ in certain properties, which we refer to as task parameters. We show that conditioning a decision transformer on these task parameters alone can enable zero-shot generalization on par with or better than its demonstration-conditioned counterpart. This suggests that task parameters are essential for the generalization and DT models are trying to recover it from the demonstration prompt. To extract the remaining generalizable information from the supervision, we introduce an additional learnable prompt which is demonstrated to further boost zero-shot generalization across a range of robotic control, manipulation, and navigation benchmark tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06063">https://arxiv.org/abs/2405.06063</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of transformer-based methods in reinforcement learning tasks, which implies the use of large language models. Moreover, navigation and control tasks mentioned in the paper could be related to software or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06219" target="_blank">SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin</p>
            <p><strong>Summary:</strong> arXiv:2405.06219v1 Announce Type: new 
Abstract: Large language models (LLMs) can now handle longer sequences of tokens, enabling complex tasks like book understanding and generating lengthy novels. However, the key-value (KV) cache required for LLMs consumes substantial memory as context length increasing, becoming the bottleneck for deployment. In this paper, we present a strategy called SKVQ, which stands for sliding-window KV cache quantization, to address the issue of extremely low bitwidth KV cache quantization. To achieve this, SKVQ rearranges the channels of the KV cache in order to improve the similarity of channels in quantization groups, and applies clipped dynamic quantization at the group level. Additionally, SKVQ ensures that the most recent window tokens in the KV cache are preserved with high precision. This helps maintain the accuracy of a small but important portion of the KV cache.SKVQ achieves high compression ratios while maintaining accuracy. Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit values with minimal loss of accuracy. With SKVQ, it is possible to process context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7 times faster decoding.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06219">https://arxiv.org/abs/2405.06219</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> You might find this paper interesting as it explores methods for optimizing and compressing large language models, which is closely related to your interest in 'Agents based on large-language models'. While the specific applications you mentioned are not directly addressed, the methodologies could be beneficial to those areas by possibly improving the efficiency of operations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06270" target="_blank">XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</a></h3>
            
            <p><strong>Authors:</strong> Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</p>
            <p><strong>Summary:</strong> arXiv:2405.06270v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into healthcare diagnostics offers a promising avenue for clinical decision-making. This study outlines the development of a novel method for zero-shot/few-shot in-context learning (ICL) by integrating medical domain knowledge using a multi-layered structured prompt. We also explore the efficacy of two communication styles between the user and LLMs: the Numerical Conversational (NC) style, which processes data incrementally, and the Natural Language Single-Turn (NL-ST) style, which employs long narrative prompts. Our study systematically evaluates the diagnostic accuracy and risk factors, including gender bias and false negative rates, using a dataset of 920 patient records in various few-shot scenarios. Results indicate that traditional clinical machine learning (ML) models generally outperform LLMs in zero-shot and few-shot settings. However, the performance gap narrows significantly when employing few-shot examples alongside effective explainable AI (XAI) methods as sources of domain knowledge. Moreover, with sufficient time and an increased number of examples, the conversational style (NC) nearly matches the performance of ML models. Most notably, LLMs demonstrate comparable or superior cost-sensitive accuracy relative to ML models. This research confirms that, with appropriate domain knowledge and tailored communication strategies, LLMs can significantly enhance diagnostic processes. The findings highlight the importance of optimizing the number of training examples and communication styles to improve accuracy and reduce biases in LLM applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06270">https://arxiv.org/abs/2405.06270</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the integration of Large Language Models (LLMs) into healthcare diagnostics. It also explores different communication styles between the user and LLMs, which aligns with your interest in using LLMs for tasks such as controlling software. Although the paper focuses more on healthcare, the general methodology might be transferable to other domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06394" target="_blank">Memory Mosaics</a></h3>
            <a href="https://arxiv.org/html/2405.06394v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06394v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, L\'eon Bottou</p>
            <p><strong>Summary:</strong> arXiv:2405.06394v1 Announce Type: new 
Abstract: Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent ways. We demonstrate these capabilities on toy examples and we also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06394">https://arxiv.org/abs/2405.06394</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not explicitly mention control of software or web browsers, it explores the capabilities of large language models in in-context learning and prediction tasks, which seem to be part of your interest in using large language models for controlling software and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06399" target="_blank">Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus</a></h3>
            
            <p><strong>Authors:</strong> Filipe Marinho Rocha, In\^es Dutra, V\'itor Santos Costa</p>
            <p><strong>Summary:</strong> arXiv:2405.06399v1 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that is currently unsolvable by any Machine Learning method, including Large Language Models (LLMs). It demands strong generalization and reasoning capabilities which are known to be weaknesses of Neural Network based systems. In this work, we propose a Program Synthesis system that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to solve ARC. We have manually defined a simple Domain Specific Language (DSL) that corresponds to a small set of object-centric abstractions relevant to ARC. This is the Background Knowledge used by ILP to create Logic Programs that provide reasoning capabilities to our system. The full system is capable of generalize to unseen tasks, since ILP can create Logic Program(s) from few examples, in the case of ARC: pairs of Input-Output grids examples for each task. These Logic Programs are able to generate Objects present in the Output grid and the combination of these can form a complete program that transforms an Input grid into an Output grid. We randomly chose some tasks from ARC that dont require more than the small number of the Object primitives we implemented and show that given only these, our system can solve tasks that require each, such different reasoning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06399">https://arxiv.org/abs/2405.06399</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper features the use of Large Language Models in solving tasks in the AI benchmark, ARC, through a Program Synthesis system. While it may not directly deal with the control of software or web browsers, it explores the broad impacts and potentials of LLMs in automating complex tasks which could illuminate your study on LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06639" target="_blank">Value Augmented Sampling for Language Model Alignment and Personalization</a></h3>
            
            <p><strong>Authors:</strong> Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, Pulkit Agrawal</p>
            <p><strong>Summary:</strong> arXiv:2405.06639v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) to cater to different human preferences, learning new skills, and unlearning harmful behavior is an important problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree Search, are performant, but impractical for LLM adaptation due to their high inference cost. On the other hand, using Reinforcement Learning (RL) for adaptation is computationally efficient, but performs worse due to the optimization challenges in co-training the value function and the policy. We present a new framework for reward optimization, Value Augmented Sampling (VAS), that can maximize different reward functions using data sampled from only the initial, frozen LLM. VAS solves for the optimal reward-maximizing policy without co-training the policy and the value function, making the optimization stable, outperforming established baselines, such as PPO and DPO, on standard benchmarks, and achieving comparable results to Best-of-128 with lower inference cost. Unlike existing RL methods that require changing the weights of the LLM, VAS does not require access to the weights of the pre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are available only as APIs. In addition, our algorithm unlocks the new capability of composing several rewards and controlling the extent of each one during deployment time, paving the road ahead for the future of aligned, personalized LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06639">https://arxiv.org/abs/2405.06639</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses Value Augmented Sampling (VAS) for Large Language Models (LLMs), an issue closely pertaining to agents based on Large Language Models. Although it doesn’t specifically mention controlling software or web browsers, the study's findings might help improve such applications by enhancing reward optimization, model alignment and personalization.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.05999" target="_blank">LLMPot: Automated LLM-based Industrial Protocol and Physical Process Emulation for ICS Honeypots</a></h3>
            <a href="https://arxiv.org/html/2405.05999v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.05999v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Christoforos Vasilatos, Dunia J. Mahboobeh, Hithem Lamri, Manaar Alam, Michail Maniatakos</p>
            <p><strong>Summary:</strong> arXiv:2405.05999v1 Announce Type: cross 
Abstract: Industrial Control Systems (ICS) are extensively used in critical infrastructures ensuring efficient, reliable, and continuous operations. However, their increasing connectivity and addition of advanced features make them vulnerable to cyber threats, potentially leading to severe disruptions in essential services. In this context, honeypots play a vital role by acting as decoy targets within ICS networks, or on the Internet, helping to detect, log, analyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS honeypots, however, is challenging due to the necessity of accurately replicating industrial protocols and device characteristics, a crucial requirement for effectively mimicking the unique operational behavior of different industrial systems. Moreover, this challenge is compounded by the significant manual effort required in also mimicking the control logic the PLC would execute, in order to capture attacker traffic aiming to disrupt critical infrastructure operations. In this paper, we propose LLMPot, a novel approach for designing honeypots in ICS networks harnessing the potency of Large Language Models (LLMs). LLMPot aims to automate and optimize the creation of realistic honeypots with vendor-agnostic configurations, and for any control logic, aiming to eliminate the manual effort and specialized knowledge traditionally required in this domain. We conducted extensive experiments focusing on a wide array of parameters, demonstrating that our LLM-based approach can effectively create honeypot devices implementing different industrial protocols and diverse control logic.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.05999">https://arxiv.org/abs/2405.05999</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the use of large language models in the design of honeypots in Industrial Control Systems. However, while it involves automation using LLMs which aligns with your interests, it does not touch specifically on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06003" target="_blank">Binary Hypothesis Testing for Softmax Models and Leverage Score Models</a></h3>
            
            <p><strong>Authors:</strong> Yeqi Gao, Yuzhou Gu, Zhao Song</p>
            <p><strong>Summary:</strong> arXiv:2405.06003v1 Announce Type: cross 
Abstract: Softmax distributions are widely used in machine learning, including Large Language Models (LLMs) where the attention unit uses softmax distributions. We abstract the attention unit as the softmax model, where given a vector input, the model produces an output drawn from the softmax distribution (which depends on the vector input). We consider the fundamental problem of binary hypothesis testing in the setting of softmax models. That is, given an unknown softmax model, which is known to be one of the two given softmax models, how many queries are needed to determine which one is the truth? We show that the sample complexity is asymptotically $O(\epsilon^{-2})$ where $\epsilon$ is a certain distance between the parameters of the models.
  Furthermore, we draw analogy between the softmax model and the leverage score model, an important tool for algorithm design in linear algebra and graph theory. The leverage score model, on a high level, is a model which, given vector input, produces an output drawn from a distribution dependent on the input. We obtain similar results for the binary hypothesis testing problem for leverage score models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06003">https://arxiv.org/abs/2405.06003</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in leveraging large language models in understanding and driving automation, as it discusses the softmax models widely used in LLMs. Even though it does not talk about controlling software or web browsers directly, understanding the underlying model of LLMs can be crucial in their effective use in agent tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06067" target="_blank">HMT: Hierarchical Memory Transformer for Long Context Language Processing</a></h3>
            <a href="https://arxiv.org/html/2405.06067v1/extracted/5580600/hmt_flow_v2.png" target="_blank"><img src="https://arxiv.org/html/2405.06067v1/extracted/5580600/hmt_flow_v2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zifan He, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong</p>
            <p><strong>Summary:</strong> arXiv:2405.06067v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLM) have been widely used in language processing applications. However, most of them restrict the context window that permits the model to attend to every token in the inputs. Previous works in recurrent models can memorize past tokens to enable unlimited context and maintain effectiveness. However, they have "flat" memory architectures, which have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we speculate that imitating brain memory hierarchy is beneficial for model memorization. We propose the Hierarchical Memory Transformer (HMT), a novel framework that enables and improves models' long-context processing ability by imitating human memorization behavior. Leveraging memory-augmented segment-level recurrence, we organize the memory hierarchy by preserving tokens from early input token segments, passing memory embeddings along the sequence, and recalling relevant information from history. Evaluating general language modeling (Wikitext-103, PG-19) and question-answering tasks (PubMedQA), we show that HMT steadily improves the long-context processing ability of context-constrained and long-context models. With an additional 0.5% - 2% of parameters, HMT can easily plug in and augment future LLMs to handle long context effectively. Our code is open-sourced on Github: https://github.com/OswaldHe/HMT-pytorch.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06067">https://arxiv.org/abs/2405.06067</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to the 'Agents based on large-language models' topic as it discusses Hierarchical Memory Transformer (HMT), a framework which improves the long-context processing ability of Large Language Models (LLM). The paper outlines how HMT can be used to augment future LLMs for handling long context more effectively which could be beneficial for controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06424" target="_blank">Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation</a></h3>
            
            <p><strong>Authors:</strong> JoonHo Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min</p>
            <p><strong>Summary:</strong> arXiv:2405.06424v1 Announce Type: cross 
Abstract: Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06424">https://arxiv.org/abs/2405.06424</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper relates to your interest of 'Agents based on large language models'. It proposes a novel way to enhance the instruction following capability of large language models, which can be crucial for tasks like controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06545" target="_blank">Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval</a></h3>
            <a href="https://arxiv.org/html/2405.06545v1/extracted/2405.06545v1/framework_overview.png" target="_blank"><img src="https://arxiv.org/html/2405.06545v1/extracted/2405.06545v1/framework_overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, Fan Mo</p>
            <p><strong>Summary:</strong> arXiv:2405.06545v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare. To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method. Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios.
  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field. Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens. Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses. Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06545">https://arxiv.org/abs/2405.06545</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it does not specifically mention software or web browser control, the paper discusses a strategy to improve large language models' output in the medical field, which falls under your interest in improving functionalities of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04532" target="_blank">QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</a></h3>
            <a href="https://arxiv.org/html/2405.04532v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.04532v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han</p>
            <p><strong>Summary:</strong> arXiv:2405.04532v2 Announce Type: replace-cross 
Abstract: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04532">https://arxiv.org/abs/2405.04532</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the development of QServe, a quantization algorithm for Large Language Model (LLM) serving. It may be particularly interesting as it deals with improving the efficiency of LLMs, which could be useful in your topic of interest concerning computer automation using LLMs.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06234" target="_blank">TS3IM: Unveiling Structural Similarity in Time Series through Image Similarity Assessment Insights</a></h3>
            
            <p><strong>Authors:</strong> Yuhan Liu, Ke Tu</p>
            <p><strong>Summary:</strong> arXiv:2405.06234v1 Announce Type: new 
Abstract: In the realm of time series analysis, accurately measuring similarity is crucial for applications such as forecasting, anomaly detection, and clustering. However, existing metrics often fail to capture the complex, multidimensional nature of time series data, limiting their effectiveness and application. This paper introduces the Structured Similarity Index Measure for Time Series (TS3IM), a novel approach inspired by the success of the Structural Similarity Index Measure (SSIM) in image analysis, tailored to address these limitations by assessing structural similarity in time series. TS3IM evaluates multiple dimensions of similarity-trend, variability, and structural integrity-offering a more nuanced and comprehensive measure. This metric represents a significant leap forward, providing a robust tool for analyzing temporal data and offering more accurate and comprehensive sequence analysis and decision support in fields such as monitoring power consumption, analyzing traffic flow, and adversarial recognition. Our extensive experimental results also show that compared with traditional methods that rely heavily on computational correlation, TS3IM is 1.87 times more similar to Dynamic Time Warping (DTW) in evaluation results and improves by more than 50% in adversarial recognition.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06234">https://arxiv.org/abs/2405.06234</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new approach (TS3IM) for measuring similarity in time series data, which is crucial for forecasting. Although it doesn't directly address deep learning methods or foundation models, it could provide valuable insights for these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06419" target="_blank">Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.06419v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06419v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianxiang Zhan, Yuanpeng He, Zhen Li, Yong Deng</p>
            <p><strong>Summary:</strong> arXiv:2405.06419v1 Announce Type: new 
Abstract: In real-world scenarios, time series forecasting often demands timeliness, making research on model backbones a perennially hot topic. To meet these performance demands, we propose a novel backbone from the perspective of information fusion. Introducing the Basic Probability Assignment (BPA) Module and the Time Evidence Fusion Network (TEFN), based on evidence theory, allows us to achieve superior performance. On the other hand, the perspective of multi-source information fusion effectively improves the accuracy of forecasting. Due to the fact that BPA is generated by fuzzy theory, TEFN also has considerable interpretability. In real data experiments, the TEFN partially achieved state-of-the-art, with low errors comparable to PatchTST, and operating efficiency surpass performance models such as Dlinear. Meanwhile, TEFN has high robustness and small error fluctuations in the random hyperparameter selection. TEFN is not a model that achieves the ultimate in single aspect, but a model that balances performance, accuracy, stability, and interpretability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06419">https://arxiv.org/abs/2405.06419</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in new deep learning methods for time series forecasting. It proposes a novel model, the Time Evidence Fusion Network (TEFN), for this purpose and provides performance evaluations of the model.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06004" target="_blank">EWMoE: An effective model for global weather forecasting with mixture-of-experts</a></h3>
            <a href="https://arxiv.org/html/2405.06004v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06004v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lihao Gan, Xin Man, Chenghong Zhang, Jie Shao</p>
            <p><strong>Summary:</strong> arXiv:2405.06004v1 Announce Type: cross 
Abstract: Weather forecasting is a crucial task for meteorologic research, with direct social and economic impacts. Recently, data-driven weather forecasting models based on deep learning have shown great potential, achieving superior performance compared with traditional numerical weather prediction methods. However, these models often require massive training data and computational resources. In this paper, we propose EWMoE, an effective model for accurate global weather forecasting, which requires significantly less training data and computational resources. Our model incorporates three key components to enhance prediction accuracy: meteorology-specific embedding, a core Mixture-of-Experts (MoE) layer, and two specific loss functions. We conduct our evaluation on the ERA5 dataset using only two years of training data. Extensive experiments demonstrate that EWMoE outperforms current models such as FourCastNet and ClimaX at all forecast time, achieving competitive performance compared with the state-of-the-art Pangu-Weather model in evaluation metrics such as Anomaly Correlation Coefficient (ACC) and Root Mean Square Error (RMSE). Additionally, ablation studies indicate that applying the MoE architecture to weather forecasting offers significant advantages in improving accuracy and resource efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06004">https://arxiv.org/abs/2405.06004</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper relates to your interest in the subtopic 'New deep learning methods for time series'. It introduces the EWMoE model, a new method based on Mixture-of-Experts for weather forecasting, a kind of time series data. While it does not specifically mention transformer-like models or multi-modal approaches, it may still offer important insights into your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06590" target="_blank">Decomposing weather forecasting into advection and convection with neural networks</a></h3>
            <a href="https://arxiv.org/html/2405.06590v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06590v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mengxuan Chen, Ziqi Yuan, Jinxiao Zhang, Runmin Dong, Haohuan Fu</p>
            <p><strong>Summary:</strong> arXiv:2405.06590v1 Announce Type: cross 
Abstract: Operational weather forecasting models have advanced for decades on both the explicit numerical solvers and the empirical physical parameterization schemes. However, the involved high computational costs and uncertainties in these existing schemes are requiring potential improvements through alternative machine learning methods. Previous works use a unified model to learn the dynamics and physics of the atmospheric model. Contrarily, we propose a simple yet effective machine learning model that learns the horizontal movement in the dynamical core and vertical movement in the physical parameterization separately. By replacing the advection with a graph attention network and the convection with a multi-layer perceptron, our model provides a new and efficient perspective to simulate the transition of variables in atmospheric models. We also assess the model's performance over a 5-day iterative forecasting. Under the same input variables and training methods, our model outperforms existing data-driven methods with a significantly-reduced number of parameters with a resolution of 5.625 deg. Overall, this work aims to contribute to the ongoing efforts that leverage machine learning techniques for improving both the accuracy and efficiency of global weather forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06590">https://arxiv.org/abs/2405.06590</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses a new machine learning method for time series analysis in the context of weather forecasting. Moreover, it proposes a unique approach by decomposing forecasting into different processes and using separate neural network models for each process.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.04620" target="_blank">Folded context condensation in Path Integral formalism for infinite context transformers</a></h3>
            <a href="https://arxiv.org/html/2405.04620v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.04620v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Won-Gi Paeng, Daesuk Kwon</p>
            <p><strong>Summary:</strong> arXiv:2405.04620v2 Announce Type: replace-cross 
Abstract: This short note is written for rapid communication of long context training and to share the idea of how to train it with low memory usage. In the note, we generalize the attention algorithm and neural network of Generative Pre-Trained Transformers and reinterpret it in Path integral formalism. First, the role of the transformer is understood as the time evolution of the token state and second, it is suggested that the all key-token states in the same time as the query-token can attend to the attention with the query token states. As a result of the repetitive time evolution, it is discussed that the token states in the past sequence meats the token states in the present sequence so that the attention between separated sequences becomes possible for maintaining infinite contextual information just by using low memory for limited size of sequence. For the experiment, the $12$ input token window size was taken and one GPU with $24$GB memory was used for the pre-training. It was confirmed that more than $150$ length context is preserved. The sampling result of the training, the code and the other details will be included in the revised version of this note later.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.04620">https://arxiv.org/abs/2405.04620</a></p>
            <p><strong>Category:</strong> hep-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't directly talk about time series, it may be of interest because it discusses a method for long context training in transformers, which can be crucial for time series predictions. Moreover, it mentions an experimental setup for maintaining infinite contextual information with low memory usage that could be applicable in time series models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.12559" target="_blank">Invariant Learning via Probability of Sufficient and Necessary Causes</a></h3>
            <a href="https://arxiv.org/html/2309.12559v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.12559v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mengyue Yang, Zhen Fang, Yonggang Zhang, Yali Du, Furui Liu, Jean-Francois Ton, Jianhong Wang, Jun Wang</p>
            <p><strong>Summary:</strong> arXiv:2309.12559v5 Announce Type: replace 
Abstract: Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. The details of the implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.12559">https://arxiv.org/abs/2309.12559</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses a new method related to causality and its implications for out-of-distribution generalization in machine learning. Causal representation learning is a significant concept for your interests, and this paper revamps the concept in terms of 'sufficiency' and 'necessity conditions.' Although it does not directly deal with large language models in causal discovery, its insights about causality might still be useful for your research.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 13, 2024 at 21:32:54</div></body></html>