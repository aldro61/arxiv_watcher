
            <html>
            <head>
                <title>Report Generated on May 15, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 15, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08017" target="_blank">Translating Expert Intuition into Quantifiable Features: Encode Investigator Domain Knowledge via LLM for Enhanced Predictive Analytics</a></h3>
            
            <p><strong>Authors:</strong> Phoebe Jing, Yijing Gao, Yuanhang Zhang, Xianlong Zeng</p>
            <p><strong>Summary:</strong> arXiv:2405.08017v1 Announce Type: new 
Abstract: In the realm of predictive analytics, the nuanced domain knowledge of investigators often remains underutilized, confined largely to subjective interpretations and ad hoc decision-making. This paper explores the potential of Large Language Models (LLMs) to bridge this gap by systematically converting investigator-derived insights into quantifiable, actionable features that enhance model performance. We present a framework that leverages LLMs' natural language understanding capabilities to encode these red flags into a structured feature set that can be readily integrated into existing predictive models. Through a series of case studies, we demonstrate how this approach not only preserves the critical human expertise within the investigative process but also scales the impact of this knowledge across various prediction tasks. The results indicate significant improvements in risk assessment and decision-making accuracy, highlighting the value of blending human experiential knowledge with advanced machine learning techniques. This study paves the way for more sophisticated, knowledge-driven analytics in fields where expert insight is paramount.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08017">https://arxiv.org/abs/2405.08017</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it includes Large Language Models (LLMs) to convert human-derived insights into quantifiable features, which is analogous to your interest in using LLMs for controlling software or web browsers and computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08448" target="_blank">Understanding the performance gap between online and offline alignment algorithms</a></h3>
            <a href="https://arxiv.org/html/2405.08448v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.08448v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, R\'emi Munos, Bernardo \'Avila Pires, Michal Valko, Yong Cheng, Will Dabney</p>
            <p><strong>Summary:</strong> arXiv:2405.08448v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08448">https://arxiv.org/abs/2405.08448</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models and their use in reinforcement learning. Though it does not directly mention automation or control over software/web browsers, it explores the performance differences between online and offline alignment algorithms which could be influential for control tasks. However, since it does not directly propose a new method for controlling software or browsers, the relevance score is 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08707" target="_blank">Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory</a></h3>
            <a href="https://arxiv.org/html/2405.08707v1/extracted/5596353/energy_logsumexp.png" target="_blank"><img src="https://arxiv.org/html/2405.08707v1/extracted/5596353/energy_logsumexp.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xueyan Niu, Bo Bai, Lei Deng, Wei Han</p>
            <p><strong>Summary:</strong> arXiv:2405.08707v1 Announce Type: new 
Abstract: Increasing the size of a Transformer model does not always lead to enhanced performance. This phenomenon cannot be explained by the empirical scaling laws. Furthermore, improved generalization ability occurs as the model memorizes the training samples. We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models. We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search. Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism. Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer. Under specific conditions, we show that the minimum achievable cross-entropy loss is bounded from below by a constant approximately equal to 1. We substantiate our theoretical results by conducting experiments with GPT-2 on various data sizes, as well as training vanilla Transformers on a dataset of 2M tokens.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08707">https://arxiv.org/abs/2405.08707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper is mostly theoretical in nature, it does provide relevant information on transformer-based language models, a type of large language models of interest to you, as well as insights into their performance dynamics and attention mechanism which can be beneficial for tasks such as controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08042" target="_blank">LLAniMAtion: LLAMA Driven Gesture Animation</a></h3>
            <a href="https://arxiv.org/html/2405.08042v1/extracted/5593075/figs/teaser.png" target="_blank"><img src="https://arxiv.org/html/2405.08042v1/extracted/5593075/figs/teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jonathan Windle, Iain Matthews, Sarah Taylor</p>
            <p><strong>Summary:</strong> arXiv:2405.08042v1 Announce Type: cross 
Abstract: Co-speech gesturing is an important modality in conversation, providing context and social cues. In character animation, appropriate and synchronised gestures add realism, and can make interactive agents more engaging. Historically, methods for automatically generating gestures were predominantly audio-driven, exploiting the prosodic and speech-related content that is encoded in the audio signal. In this paper we instead experiment with using LLM features for gesture generation that are extracted from text using LLAMA2. We compare against audio features, and explore combining the two modalities in both objective tests and a user study. Surprisingly, our results show that LLAMA2 features on their own perform significantly better than audio features and that including both modalities yields no significant difference to using LLAMA2 features in isolation. We demonstrate that the LLAMA2 based model can generate both beat and semantic gestures without any audio input, suggesting LLMs can provide rich encodings that are well suited for gesture generation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08042">https://arxiv.org/abs/2405.08042</a></p>
            <p><strong>Category:</strong> cs.HC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper discusses the use of a large language model, LLAMA2, to extract features for gesture generation in animation, indicating how LLMs are used to control software. While not directly controlling web browsers or for computer automation, it is still in the area of controlling software machines using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08755" target="_blank">Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach</a></h3>
            <a href="https://arxiv.org/html/2405.08755v1/extracted/2405.08755v1/figures/image_1.png" target="_blank"><img src="https://arxiv.org/html/2405.08755v1/extracted/2405.08755v1/figures/image_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Syed Mhamudul Hasan, Alaa M. Alotaibi, Sajedul Talukder, Abdur R. Shahid</p>
            <p><strong>Summary:</strong> arXiv:2405.08755v1 Announce Type: cross 
Abstract: With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08755">https://arxiv.org/abs/2405.08755</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems to be fairly relevant to your interest in 'Agents based on large-language models' particularly about 'computer automation using large language models.' The paper discusses the application of large language models in analysing data streams and adapting to evolving threats in the context of cybersecurity.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.00858" target="_blank">Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs</a></h3>
            <a href="https://arxiv.org/html/2403.00858v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.00858v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott</p>
            <p><strong>Summary:</strong> arXiv:2403.00858v4 Announce Type: replace 
Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.00858">https://arxiv.org/abs/2403.00858</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is of interest as it discusses the refinement of Large Language Models (LLMs) for better performance in text generation which could be potentially beneficial in controlling software and web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.05950" target="_blank">Language Models as Black-Box Optimizers for Vision-Language Models</a></h3>
            <a href="https://arxiv.org/html/2309.05950v5/extracted/2309.05950v5/figures/promptgpt.jpg" target="_blank"><img src="https://arxiv.org/html/2309.05950v5/extracted/2309.05950v5/figures/promptgpt.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, Deva Ramanan</p>
            <p><strong>Summary:</strong> arXiv:2309.05950v5 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. As such, we aim to develop a black-box approach to optimize VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or even output logits. We propose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup, our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We highlight the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search. In addition, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different VLM architectures in a black-box manner. Lastly, we apply our framework to optimize the state-of-the-art black-box VLM (DALL-E 3) for text-to-image generation, prompt inversion, and personalization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.05950">https://arxiv.org/abs/2309.05950</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines a new method of utilizing large language models for optimizing vision-language models, which is a form of software control. It does not directly deal with computer automation or browser control, but it offers a promising methodology that you might find useful in those fields.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.10799" target="_blank">Efficient Pruning of Large Language Model with Adaptive Estimation Fusion</a></h3>
            <a href="https://arxiv.org/html/2403.10799v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.10799v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi Wang</p>
            <p><strong>Summary:</strong> arXiv:2403.10799v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%, 2.0%, and 1.2% for LLaMa-7B,Vicuna-7B, Baichuan-7B, and Bloom-7b1, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.10799">https://arxiv.org/abs/2403.10799</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper addresses the optimization of large language models quite directly, introducing an adaptive method for better accuracy. The relevance to controlling software or web browsers is not addressed directly in the abstract, however efficient LLMs deployment is critical in these areas, thus a 4 score. </p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03709" target="_blank">Generating Probabilistic Scenario Programs from Natural Language</a></h3>
            <a href="https://arxiv.org/html/2405.03709v2/extracted/2405.03709v2/figures/arch.png" target="_blank"><img src="https://arxiv.org/html/2405.03709v2/extracted/2405.03709v2/figures/arch.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Karim Elmaaroufi, Devan Shanker, Ana Cismaru, Marcell Vazquez-Chanlatte, Alberto Sangiovanni-Vincentelli, Matei Zaharia, Sanjit A. Seshia</p>
            <p><strong>Summary:</strong> arXiv:2405.03709v2 Announce Type: replace-cross 
Abstract: For cyber-physical systems (CPS), including robotics and autonomous vehicles, mass deployment has been hindered by fatal errors that occur when operating in rare events. To replicate rare events such as vehicle crashes, many companies have created logging systems and employed crash reconstruction experts to meticulously recreate these valuable events in simulation. However, in these methods, "what if" questions are not easily formulated and answered. We present ScenarioNL, an AI System for creating scenario programs from natural language. Specifically, we generate these programs from police crash reports. Reports normally contain uncertainty about the exact details of the incidents which we represent through a Probabilistic Programming Language (PPL), Scenic. By using Scenic, we can clearly and concisely represent uncertainty and variation over CPS behaviors, properties, and interactions. We demonstrate how commonplace prompting techniques with the best Large Language Models (LLM) are incapable of reasoning about probabilistic scenario programs and generating code for low-resource languages such as Scenic. Our system is comprised of several LLMs chained together with several kinds of prompting strategies, a compiler, and a simulator. We evaluate our system on publicly available autonomous vehicle crash reports in California from the last five years and share insights into how we generate code that is both semantically meaningful and syntactically correct.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03709">https://arxiv.org/abs/2405.03709</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models (LLM) and their application in controlling systems. It specifically describes how LLMs can be utilized in an AI system, ScenarioNL, for creating scenario programs from natural language (in this case, police crash reports). Although it doesn't directly talk about controlling web browsers or software, the paper does touch on an application of LLM in the field of robotics and autonomous vehicles.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03963" target="_blank">ERATTA: Extreme RAG for Table To Answers with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.03963v2/extracted/2405.03963v2/images/sys.png" target="_blank"><img src="https://arxiv.org/html/2405.03963v2/extracted/2405.03963v2/images/sys.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra</p>
            <p><strong>Summary:</strong> arXiv:2405.03963v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) with retrieval augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size. Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds. One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03963">https://arxiv.org/abs/2405.03963</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents the application of large language models (LLMs) in the process of data authentication, user query routing, data retrieval, and question answering. Although it doesn't specifically discuss using LLMs to control software or web browsers, it does touch upon the concept of using LLMs for automation and complex user interaction tasks which aligns with your interest in 'Agents based on large-language models'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06067" target="_blank">HMT: Hierarchical Memory Transformer for Long Context Language Processing</a></h3>
            <a href="https://arxiv.org/html/2405.06067v2/extracted/5595014/hmt_flow_v2.png" target="_blank"><img src="https://arxiv.org/html/2405.06067v2/extracted/5595014/hmt_flow_v2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zifan He, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong</p>
            <p><strong>Summary:</strong> arXiv:2405.06067v2 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLM) have been widely used in language processing applications. However, most of them restrict the context window that permits the model to attend to every token in the inputs. Previous works in recurrent models can memorize past tokens to enable unlimited context and maintain effectiveness. However, they have "flat" memory architectures, which have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we speculate that imitating brain memory hierarchy is beneficial for model memorization. We propose the Hierarchical Memory Transformer (HMT), a novel framework that enables and improves models' long-context processing ability by imitating human memorization behavior. Leveraging memory-augmented segment-level recurrence, we organize the memory hierarchy by preserving tokens from early input token segments, passing memory embeddings along the sequence, and recalling relevant information from history. Evaluating general language modeling (Wikitext-103, PG-19) and question-answering tasks (PubMedQA), we show that HMT steadily improves the long-context processing ability of context-constrained and long-context models. With an additional 0.5% - 2% of parameters, HMT can easily plug in and augment future LLMs to handle long context effectively. Our code is open-sourced on Github: https://github.com/OswaldHe/HMT-pytorch.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06067">https://arxiv.org/abs/2405.06067</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant because it proposes the Hierarchical Memory Transformer, a novel framework that improves models' long-context processing ability. While it does not directly speak to your interest in controlling software or web browsers with large language models, it does relate to the underlying technology of these areas and could have implications for these fields. Therefore, it is scored a 4 instead of a 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06823" target="_blank">PLeak: Prompt Leaking Attacks against Large Language Model Applications</a></h3>
            <a href="https://arxiv.org/html/2405.06823v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06823v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bo Hui, Haolin Yuan, Neil Gong, Philippe Burlina, Yinzhi Cao</p>
            <p><strong>Summary:</strong> arXiv:2405.06823v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) enable a new ecosystem with many downstream applications, called LLM applications, with different natural language processing tasks. The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform. Therefore, an LLM application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an LLM application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness.
  In this paper, we design a novel, closed-box prompt leaking attack framework, called PLeak, to optimize an adversarial query such that when the attacker sends it to a target LLM application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt.
  We evaluate PLeak in both offline settings and for real-world LLM applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that PLeak can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/BHui97/PLeak.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06823">https://arxiv.org/abs/2405.06823</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is primarily about Large Language Models (LLMs) and their applications. Although it does not directly propose methods for using LLMs for controlling software or browsers, it does shed light on the potential vulnerabilities of LLM applications, such as prompt leaking, which may be of interest to anyone using large language models in the fields of software control and automation. Understanding this aspect could be instrumental in creating more secure LLM-based agents.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08793" target="_blank">A Brief Introduction to Causal Inference in Machine Learning</a></h3>
            
            <p><strong>Authors:</strong> Kyunghyun Cho</p>
            <p><strong>Summary:</strong> arXiv:2405.08793v1 Announce Type: new 
Abstract: This is a lecture note produced for DS-GA 3001.003 "Special Topics in DS - Causal Inference in Machine Learning" at the Center for Data Science, New York University in Spring, 2024. This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously. In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08793">https://arxiv.org/abs/2405.08793</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it provides an introduction to causal inference in machine learning, which aligns with your specified subtopics of causal representation learning and causal discovery. It doesn't specify the use of large language models, but it's foundational knowledge in the space of causality in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08029" target="_blank">PHUDGE: Phi-3 as Scalable Judge</a></h3>
            <a href="https://arxiv.org/html/2405.08029v1/extracted/5591943/PHUDGE_1.png" target="_blank"><img src="https://arxiv.org/html/2405.08029v1/extracted/5591943/PHUDGE_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mahesh Deshwal, Apoorva Chawla</p>
            <p><strong>Summary:</strong> arXiv:2405.08029v1 Announce Type: new 
Abstract: In this paper cum technical report, we present PHUDGE A fine tuned Phi3 model that achieved SOTA results in 4 tasks as Feedback Test, Feedback OOD, MT Human, Preference Test surpassing each and every existing model in latency and throughput. It shows very strong correlation not only with GPT4 but with Human annotators too in unseen data as well as in both absolute and relative grading tasks. We have not only addressed the usage of small LMs for cost effective production grade systems but have also shown that Causal modelling is not only slow in nature but sometimes it can hinder models learning capabilities and should be replaced by simpler tasks whenever we can to make the overall system faster and better. We show that by following systematic ML experimentation, thoughtful data augmentation and re purposing the problem itself, we can even beat 10x bigger models even with lesser training data. To the best of our knowledge, we are re the first one to experiment and showcase the usage of generalised version of Earth Movers Distance AKA Wasserstein distance by using Minkowski Distance with a penalty to control loss smoothing and can be used as a loss function instead of Cross Entropy to get stable training and better results for grading tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08029">https://arxiv.org/abs/2405.08029</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses the concept of causal modelling and the outcomes of simplifying them. It also shows how machine learning techniques were utilized and experimented to improve the performance of tasks and therefore is relevant to your interest in causal representation learning and causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08174" target="_blank">Estimating Direct and Indirect Causal Effects of Spatiotemporal Interventions in Presence of Spatial Interference</a></h3>
            <a href="https://arxiv.org/html/2405.08174v1/extracted/5594302/conf_inter_graph.png" target="_blank"><img src="https://arxiv.org/html/2405.08174v1/extracted/5594302/conf_inter_graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sahara Ali, Omar Faruque, Jianwu Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.08174v1 Announce Type: new 
Abstract: Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations. Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes. In this paper, we first formalize the concept of spatial interference in case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding. We then propose our deep learning based potential outcome model for spatiotemporal causal inference. We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time. Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data. Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference. Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08174">https://arxiv.org/abs/2405.08174</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it covers 'Causal discovery', one of your subtopics. It proposes a new deep learning method, specifically for spatiotemporal causal inference, which seems to be innovative in treating spatial interference in case of time-varying treatment assignments. Even though it doesn't directly mention using large language models, its focus on deep learning and causal effects is closely related to your areas of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08380" target="_blank">CIER: A Novel Experience Replay Approach with Causal Inference in Deep Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2405.08380v1/" target="_blank"><img src="https://arxiv.org/html/2405.08380v1/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jingwen Wang, Dehui Du, Yida Li, Yiyang Li, Yikang Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.08380v1 Announce Type: new 
Abstract: In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment. With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training. This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series. We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences. Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes. We design a module to provide feedback on the causality during DRL training. Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process. Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08380">https://arxiv.org/abs/2405.08380</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper focuses on causal inference in deep reinforcement learning and proposes a new method to identify causal factors, which could be relevant for your interest in causal discovery. However, it doesn't directly involve large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08498" target="_blank">Learning Decision Policies with Instrumental Variables through Double Machine Learning</a></h3>
            <a href="https://arxiv.org/html/2405.08498v1/extracted/5595464/causal_graph.png" target="_blank"><img src="https://arxiv.org/html/2405.08498v1/extracted/5595464/causal_graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska</p>
            <p><strong>Summary:</strong> arXiv:2405.08498v1 Announce Type: new 
Abstract: A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08498">https://arxiv.org/abs/2405.08498</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning, particularly in terms of causal discovery. It presents a novel method, DML-IV, to reduce bias in IV regression and effectively handles confounding issues commonly encountered in decision-making policies. It is not directly related to large language models, however, thus a 4 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08779" target="_blank">Jacobian Regularizer-based Neural Granger Causality</a></h3>
            <a href="https://arxiv.org/html/2405.08779v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.08779v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wanqi Zhou, Shuanghao Bai, Shujian Yu, Qibin Zhao, Badong Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.08779v1 Announce Type: new 
Abstract: With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships. However, the existing framework of neural Granger causality has several limitations. It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality. Moreover, most of them cannot grasp full-time Granger causality. To address these drawbacks, we propose a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables. Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis. Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08779">https://arxiv.org/abs/2405.08779</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'Causal discovery' as it proposes a new Jacobian Regularizer-based Neural Granger Causality method for learning causality between variables. However, it does not mention the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08699" target="_blank">Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity</a></h3>
            
            <p><strong>Authors:</strong> Wenrui Li, Wei Zhang, Qinghao Zhang, Xuegong Zhang, Xiaowo Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.08699v1 Announce Type: cross 
Abstract: Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08699">https://arxiv.org/abs/2405.08699</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Causality and Machine Learning', specifically the subtopic 'Causal discovery'. It proposes the novel KEEL method for causal discovery, which aims to tackle domain inconsistencies and high-dimensional/small-sample datasets using fuzzy knowledge and data. However, it doesn't mention the use of large language models in causal discovery, thus the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2107.10955" target="_blank">Learning Linear Polytree Structural Equation Models</a></h3>
            <a href="https://arxiv.org/html/2107.10955v4/extracted/2107.10955v4/polytree_sim1.jpg" target="_blank"><img src="https://arxiv.org/html/2107.10955v4/extracted/2107.10955v4/polytree_sim1.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xingmei Lou, Yu Hu, Xiaodong Li</p>
            <p><strong>Summary:</strong> arXiv:2107.10955v4 Announce Type: replace-cross 
Abstract: We are interested in the problem of learning the directed acyclic graph (DAG) when data are generated from a linear structural equation model (SEM) and the causal structure can be characterized by a polytree. Under the Gaussian polytree models, we study sufficient conditions on the sample sizes for the well-known Chow-Liu algorithm to exactly recover both the skeleton and the equivalence class of the polytree, which is uniquely represented by a CPDAG. On the other hand, necessary conditions on the required sample sizes for both skeleton and CPDAG recovery are also derived in terms of information-theoretic lower bounds, which match the respective sufficient conditions and thereby give a sharp characterization of the difficulty of these tasks. We also consider the problem of inverse correlation matrix estimation under the linear polytree models, and establish the estimation error bound in terms of the dimension and the total number of v-structures. We also consider an extension of group linear polytree models, in which each node represents a group of variables. Our theoretical findings are illustrated by comprehensive numerical simulations, and experiments on benchmark data also demonstrate the robustness of polytree learning when the true graphical structures can only be approximated by polytrees.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2107.10955">https://arxiv.org/abs/2107.10955</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Causality and Machine Learning'. It comprehensively deals with learning the directed acyclic graph (DAG), a fundamental aspect of causal discovery. It proposes a strategy under the Gaussian polytree models and provides a robust statistical analysis in terms of information-theoretic lower bounds. This matches your interest in 'Causal discovery'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.17483" target="_blank">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</a></h3>
            <a href="https://arxiv.org/html/2404.17483v3/" target="_blank"><img src="https://arxiv.org/html/2404.17483v3/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yoichi Chikahara, Kansei Ushiyama</p>
            <p><strong>Summary:</strong> arXiv:2404.17483v3 Announce Type: replace-cross 
Abstract: There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.17483">https://arxiv.org/abs/2404.17483</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses heterogeneous treatment effect estimation, an area of causal inference. The proposed method aims to solve a challenging problem in causal representation learning related to high-dimensional heterogeneous treatment effect estimation, which aligns with your interest in causal discovery. However, it does not directly mention the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.01463" target="_blank">Dynamic Local Average Treatment Effects</a></h3>
            
            <p><strong>Authors:</strong> Ravi B. Sojitra, Vasilis Syrgkanis</p>
            <p><strong>Summary:</strong> arXiv:2405.01463v2 Announce Type: replace-cross 
Abstract: We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment contrasts for the respective complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify Dynamic LATEs that correspond to treating at single time steps. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify Dynamic LATEs for treating in multiple time periods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.01463">https://arxiv.org/abs/2405.01463</a></p>
            <p><strong>Category:</strong> econ.EM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper fits under your interest in causality in machine learning. Though it does not directly refer to causal representation learning or causal discovery, it discusses Dynamic Local Average Treatment Effects (LATEs), which have applications in digital recommendations and adaptive medical trials that could be classified under causal discovery in machine learning.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08790" target="_blank">Kolmogorov-Arnold Networks (KANs) for Time Series Analysis</a></h3>
            <a href="https://arxiv.org/html/2405.08790v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.08790v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, M\`arius Caus</p>
            <p><strong>Summary:</strong> arXiv:2405.08790v1 Announce Type: cross 
Abstract: This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08790">https://arxiv.org/abs/2405.08790</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in time series and deep learning. It introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, which fits your request for new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.03885" target="_blank">MOMENT: A Family of Open Time-series Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2402.03885v2/" target="_blank"><img src="https://arxiv.org/html/2402.03885v2/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, Artur Dubrawski</p>
            <p><strong>Summary:</strong> arXiv:2402.03885v2 Announce Type: replace 
Abstract: We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on Huggingface.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.03885">https://arxiv.org/abs/2402.03885</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper covers several of your interests under time-series. It introduces a new model, MOMENT, for time series analysis, touches on the challenges in training models on time series data due to the absence of large and diverse public repository and also the complexities involved in dealing with diverse time series characteristics. In response, they have compiled a collection of time series called the Time Series Pile. This can be related to your interest in new foundation models for time series, new deep learning methods for time series, and datasets to train foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08293" target="_blank">Airport Delay Prediction with Temporal Fusion Transformers</a></h3>
            
            <p><strong>Authors:</strong> Ke Liu, Kaijing Ding, Xi Cheng, Jianan Chen, Siyuan Feng, Hui Lin, Jilin Song, Chen Zhu</p>
            <p><strong>Summary:</strong> arXiv:2405.08293v1 Announce Type: new 
Abstract: Since flight delay hurts passengers, airlines, and airports, its prediction becomes crucial for the decision-making of all stakeholders in the aviation industry and thus has been attempted by various previous research. However, previous delay predictions are often categorical and at a highly aggregated level. To improve that, this study proposes to apply the novel Temporal Fusion Transformer model and predict numerical airport arrival delays at quarter hour level for U.S. top 30 airports. Inputs to our model include airport demand and capacity forecasts, historic airport operation efficiency information, airport wind and visibility conditions, as well as enroute weather and traffic conditions. The results show that our model achieves satisfactory performance measured by small prediction errors on the test set. In addition, the interpretability analysis of the model outputs identifies the important input factors for delay prediction.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08293">https://arxiv.org/abs/2405.08293</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it applies a new transformer-like model, i.e., Temporal Fusion Transformer, for time series forecasting in the aviation industry.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08440" target="_blank">DGCformer: Deep Graph Clustering Transformer for Multivariate Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.08440v1/1.1.png" target="_blank"><img src="https://arxiv.org/html/2405.08440v1/1.1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qinshuo Liu, Yanwen Fang, Pengtao Jiang, Guodong Li</p>
            <p><strong>Summary:</strong> arXiv:2405.08440v1 Announce Type: new 
Abstract: Multivariate time series forecasting tasks are usually conducted in a channel-dependent (CD) way since it can incorporate more variable-relevant information. However, it may also involve a lot of irrelevant variables, and this even leads to worse performance than the channel-independent (CI) strategy. This paper combines the strengths of both strategies and proposes the Deep Graph Clustering Transformer (DGCformer) for multivariate time series forecasting. Specifically, it first groups these relevant variables by a graph convolutional network integrated with an autoencoder, and a former-latter masked self-attention mechanism is then considered with the CD strategy being applied to each group of variables while the CI one for different groups. Extensive experimental results on eight datasets demonstrate the superiority of our method against state-of-the-art models, and our code will be publicly available upon acceptance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08440">https://arxiv.org/abs/2405.08440</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in time-series forecasting as it presents the Deep Graph Clustering Transformer (DGCformer), a new model for multivariate time series forecasting. While it doesn't explicitly mention being a foundation model or multimodal, it introduces a new transformer-like model, meeting one of your subinterests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.05499" target="_blank">Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.05499v2/" target="_blank"><img src="https://arxiv.org/html/2405.05499v2/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Feifei Li, Suhan Guo, Feng Han, Jian Zhao, Furao Shen</p>
            <p><strong>Summary:</strong> arXiv:2405.05499v2 Announce Type: replace 
Abstract: Accurate forecasting of long-term time series has important applications for decision making and planning. However, it remains challenging to capture the long-term dependencies in time series data. To better extract long-term dependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a method that utilizes a shallow dilated convolution architecture to capture the period and trend characteristics of long time series. We design different convolution blocks with exponentially growing dilations and varying kernel sizes to sample time series data at different scales. Furthermore, we utilize traditional autoregressive model to capture the linear relationships within the data. To validate the effectiveness of the proposed approach, we conduct experiments on eight challenging long-term time series forecasting benchmark datasets. The experimental results show that our approach outperforms the prior state-of-the-art approaches and shows significant inference speed improvements compared to several strong baseline methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.05499">https://arxiv.org/abs/2405.05499</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it introduces a new deep learning method (Multi-Scale Dilated Convolution Network) for time series forecasting. It however, doesn't mention explicitly about foundation models or transformer-like models or multimodality in its approach.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2303.12797" target="_blank">An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters</a></h3>
            <a href="https://arxiv.org/html/2303.12797v2/extracted/2303.12797v2/data/performance_ratio.png" target="_blank"><img src="https://arxiv.org/html/2303.12797v2/extracted/2303.12797v2/data/performance_ratio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Julie Keisler (EDF R\&D OSIRIS, EDF R\&D, CRIStAL, CRIStAL), El-Ghazali Talbi (CRIStAL, CRIStAL), Sandra Claudel (EDF R\&D OSIRIS, EDF R\&D), Gilles Cabriel (EDF R\&D OSIRIS, EDF R\&D)</p>
            <p><strong>Summary:</strong> arXiv:2303.12797v2 Announce Type: replace-cross 
Abstract: In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2303.12797">https://arxiv.org/abs/2303.12797</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents an algorithmic framework for optimizing deep neural networks, which includes application to time series. Though it does not explicitly mention new deep learning methods for time series or foundation models for time series, it suggests a flexible search space including self-attention, reminiscent of transformer-like models. It also demonstrated its effectiveness using time series prediction benchmark.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 15, 2024 at 21:36:56</div></body></html>