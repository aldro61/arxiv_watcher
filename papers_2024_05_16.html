
            <html>
            <head>
                <title>Report Generated on May 16, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 16, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08842" target="_blank">Automated Deep Learning for Load Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Julie Keisler (CRIStAL, EDF R\&D OSIRIS, EDF R\&D), Sandra Claudel, Gilles Cabriel, Margaux Br\'eg\`ere</p>
            <p><strong>Summary:</strong> arXiv:2405.08842v1 Announce Type: new 
Abstract: Accurate forecasting of electricity consumption is essential to ensure the performance and stability of the grid, especially as the use of renewable energy increases. Forecasting electricity is challenging because it depends on many external factors, such as weather and calendar variables. While regression-based models are currently effective, the emergence of new explanatory variables and the need to refine the temporality of the signals to be forecasted is encouraging the exploration of novel methodologies, in particular deep learning models. However, Deep Neural Networks (DNNs) struggle with this task due to the lack of data points and the different types of explanatory variables (e.g. integer, float, or categorical). In this paper, we explain why and how we used Automated Deep Learning (AutoDL) to find performing DNNs for load forecasting. We ended up creating an AutoDL framework called EnergyDragon by extending the DRAGON package and applying it to load forecasting. EnergyDragon automatically selects the features embedded in the DNN training in an innovative way and optimizes the architecture and the hyperparameters of the networks. We demonstrate on the French load signal that EnergyDragon can find original DNNs that outperform state-of-the-art load forecasting methods as well as other AutoDL approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08842">https://arxiv.org/abs/2405.08842</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is focused on time series forecasting using a new deep learning methodology. Although it's specifically about load forecasting and uses a novel automated deep learning approach rather than a new foundation or multimodal model, it still aligns fairly well with your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08967" target="_blank">Perturbation-based Learning for Recurrent Neural Networks</a></h3>
            <a href="https://arxiv.org/html/2405.08967v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.08967v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jesus Garcia Fernandez, Sander Keemink, Marcel van Gerven</p>
            <p><strong>Summary:</strong> arXiv:2405.08967v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) hold immense potential for computations due to their Turing completeness and sequential processing capabilities, yet existing methods for their training encounter efficiency challenges. Backpropagation through time (BPTT), the prevailing method, extends the backpropagation (BP) algorithm by unrolling the RNN over time. However, this approach suffers from significant drawbacks, including the need to interleave forward and backward phases and store exact gradient information. Furthermore, BPTT has been shown to struggle with propagating gradient information for long sequences, leading to vanishing gradients. An alternative strategy to using gradient-based methods like BPTT involves stochastically approximating gradients through perturbation-based methods. This learning approach is exceptionally simple, necessitating only forward passes in the network and a global reinforcement signal as feedback. Despite its simplicity, the random nature of its updates typically leads to inefficient optimization, limiting its effectiveness in training neural networks. In this study, we present a new approach to perturbation-based learning in RNNs whose performance is competitive with BPTT, while maintaining the inherent advantages over gradient-based learning. To this end, we extend the recently introduced activity-based node perturbation (ANP) method to operate in the time domain, leading to more efficient learning and generalization. Subsequently, we conduct a range of experiments to validate our approach. Our results show similar performance, convergence time and scalability when compared to BPTT, strongly outperforming standard node perturbation and weight perturbation methods. These findings suggest that perturbation-based learning methods offer a versatile alternative to gradient-based methods for training RNNs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08967">https://arxiv.org/abs/2405.08967</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper presents a new learning method for recurrent neural networks, which are often used in time series forecasting. While it doesn't directly address every subtopic of your interest, it is highly relevant due to its contribution to improving the efficiency of training RNNs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09061" target="_blank">Improving Transformers using Faithful Positional Encoding</a></h3>
            <a href="https://arxiv.org/html/2405.09061v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.09061v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tsuyoshi Id\'e, Jokin Labaien, Pin-Yu Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.09061v1 Announce Type: new 
Abstract: We propose a new positional encoding method for a neural network architecture called the Transformer. Unlike the standard sinusoidal positional encoding, our approach is based on solid mathematical grounds and has a guarantee of not losing information about the positional order of the input sequence. We show that the new encoding approach systematically improves the prediction performance in the time-series classification task.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09061">https://arxiv.org/abs/2405.09061</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'time-series' tag because it deals with positional encoding on transformer-like models, with specific application and performance improvement on time-series classification tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09308" target="_blank">TimeX++: Learning Time-Series Explanations with Information Bottleneck</a></h3>
            <a href="https://arxiv.org/html/2405.09308v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.09308v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zichuan Liu, Tianchun Wang, Jimeng Shi, Xu Zheng, Zhuomin Chen, Lei Song, Wenqian Dong, Jayantha Obeysekera, Farhad Shirani, Dongsheng Luo</p>
            <p><strong>Summary:</strong> arXiv:2405.09308v1 Announce Type: new 
Abstract: Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/TimeXplusplus}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09308">https://arxiv.org/abs/2405.09308</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new explanation framework, TimeX++, for time series data which builds upon information bottleneck (IB) concept, hence proposes a new method for time series. They also evaluate the method on synthetic and real-world datasets.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.12874" target="_blank">Easy attention: A simple attention mechanism for temporal predictions with transformers</a></h3>
            <a href="https://arxiv.org/html/2308.12874v3/extracted/5595327/Sine/Evo_self_combine_a11a22a33sine_50range_012init_3n_3in_3out_1h_3d_8batch_100000epoch.jpg" target="_blank"><img src="https://arxiv.org/html/2308.12874v3/extracted/5595327/Sine/Evo_self_combine_a11a22a33sine_50range_012init_3n_3in_3out_1h_3d_8batch_100000epoch.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Marcial Sanchis-Agudo, Yuning Wang, Roger Arnau, Luca Guastoni, Jasmin Lim, Karthik Duraisamy, Ricardo Vinuesa</p>
            <p><strong>Summary:</strong> arXiv:2308.12874v3 Announce Type: replace 
Abstract: To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention which we demonstrate in time-series reconstruction and prediction. While the standard self attention only makes use of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through the singular-value decomposition (SVD) on the softmax attention score, we further observe that self attention compresses the contributions from both queries and keys in the space spanned by the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than self attention or the widely-used long short-term memory (LSTM) network. We show the improved performance of the easy-attention method in the Lorenz system, a turbulence shear flow and a model of a nuclear reactor.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.12874">https://arxiv.org/abs/2308.12874</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a novel attention mechanism for time series prediction, which aligns with your interest in new transformer-like models for time series. However, it does not mention the specific aspect of forecasting, so the score is not full.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15419" target="_blank">Using Deep Learning to Identify Initial Error Sensitivity for Interpretable ENSO Forecasts</a></h3>
            
            <p><strong>Authors:</strong> Kinya Toride, Matthew Newman, Andrew Hoell, Antonietta Capotondi, Jakob Schl\"or, Dillon Amaya</p>
            <p><strong>Summary:</strong> arXiv:2404.15419v3 Announce Type: replace-cross 
Abstract: We introduce an interpretable-by-design method, optimized model-analog, that integrates deep learning with model-analog forecasting, a straightforward yet effective approach that generates forecasts from similar initial climate states in a repository of model simulations. This hybrid framework employs a convolutional neural network to estimate state-dependent weights to identify initial analog states that lead to shadowing target trajectories. The advantage of our method lies in its inherent interpretability, offering insights into initial-error-sensitive regions through estimated weights and the ability to trace the physically-based evolution of the system through analog forecasting. We evaluate our approach using the Community Earth System Model Version 2 Large Ensemble to forecast the El Ni\~no-Southern Oscillation (ENSO) on a seasonal-to-annual time scale. Results show a 10% improvement in forecasting equatorial Pacific sea surface temperature anomalies at 9-12 months leads compared to the original (unweighted) model-analog technique. Furthermore, our model demonstrates improvements in boreal winter and spring initialization when evaluated against a reanalysis dataset. Our approach reveals state-dependent regional sensitivity linked to various seasonally varying physical processes, including the Pacific Meridional Modes, equatorial recharge oscillator, and stochastic wind forcing. Additionally, disparities emerge in the sensitivity associated with El Ni\~no versus La Ni\~na events. El Ni\~no forecasts are more sensitive to initial uncertainty in tropical Pacific sea surface temperatures, while La Ni\~na forecasts are more sensitive to initial uncertainty in tropical Pacific zonal wind stress. This approach has broad implications for forecasting diverse climate phenomena, including regional temperature and precipitation, which are challenging for the original model-analog approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15419">https://arxiv.org/abs/2404.15419</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new model that combines deep learning and model-analog forecasting for predicting time series data related to climate phenomena. It is particularly relevant to your interest in new deep learning methods for time series and forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09220" target="_blank">ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.09220v1/extracted/5597870/Fig/C1/Graph.png" target="_blank"><img src="https://arxiv.org/html/2405.09220v1/extracted/5597870/Fig/C1/Graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.09220v1 Announce Type: new 
Abstract: In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks." Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis. Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent. Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09220">https://arxiv.org/abs/2405.09220</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the capabilities of transformer-based language models, which falls under your interest in the use of large language models for tasks like software control. However, the work focuses on planning in networks, so it might not directly match your software control sub-interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08839" target="_blank">PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.08839v1/extracted/5594677/sql_generation.png" target="_blank"><img src="https://arxiv.org/html/2405.08839v1/extracted/5594677/sql_generation.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Satya K Gundabathula, Sriram R Kolar</p>
            <p><strong>Summary:</strong> arXiv:2405.08839v1 Announce Type: cross 
Abstract: This paper presents our approach to the EHRSQL-2024 shared task, which aims to develop a reliable Text-to-SQL system for electronic health records. We propose two approaches that leverage large language models (LLMs) for prompting and fine-tuning to generate EHRSQL queries. In both techniques, we concentrate on bridging the gap between the real-world knowledge on which LLMs are trained and the domain specific knowledge required for the task. The paper provides the results of each approach individually, demonstrating that they achieve high execution accuracy. Additionally, we show that an ensemble approach further enhances generation reliability by reducing errors. This approach secured us 2nd place in the shared task competition. The methodologies outlined in this paper are designed to be transferable to domain-specific Text-to-SQL problems that emphasize both accuracy and reliability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08839">https://arxiv.org/abs/2405.08839</a></p>
            <p><strong>Category:</strong> cs.DB</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in 'Agents based on large language models'. It discusses how large language models can be used for text-to-SQL tasks which is a form of software control. Although not directly focused on web browsers, the methodologies used could potentially be applicable to broader areas of computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08888" target="_blank">Large Language Models for Human-Machine Collaborative Particle Accelerator Tuning through Natural Language</a></h3>
            
            <p><strong>Authors:</strong> Jan Kaiser, Annika Eichler, Anne Lauscher</p>
            <p><strong>Summary:</strong> arXiv:2405.08888v1 Announce Type: cross 
Abstract: Autonomous tuning of particle accelerators is an active and challenging field of research with the goal of enabling novel accelerator technologies cutting-edge high-impact applications, such as physics discovery, cancer research and material sciences. A key challenge with autonomous accelerator tuning remains that the most capable algorithms require an expert in optimisation, machine learning or a similar field to implement the algorithm for every new tuning task. In this work, we propose the use of large language models (LLMs) to tune particle accelerators. We demonstrate on a proof-of-principle example the ability of LLMs to successfully and autonomously tune a particle accelerator subsystem based on nothing more than a natural language prompt from the operator, and compare the performance of our LLM-based solution to state-of-the-art optimisation algorithms, such as Bayesian optimisation (BO) and reinforcement learning-trained optimisation (RLO). In doing so, we also show how LLMs can perform numerical optimisation of a highly non-linear real-world objective function. Ultimately, this work represents yet another complex task that LLMs are capable of solving and promises to help accelerate the deployment of autonomous tuning algorithms to the day-to-day operations of particle accelerators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08888">https://arxiv.org/abs/2405.08888</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research paper is relevant to the 'Agents based on large-language models' topic you are interested in. The paper explores how large language models can autonomously tune a particle accelerator using natural language input from the operator, which falls under 'Using large language models to control software'. However, it is not about controlling web browsers or general computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08989" target="_blank">What is it for a Machine Learning Model to Have a Capability?</a></h3>
            
            <p><strong>Authors:</strong> Jacqueline Harding, Nathaniel Sharadin</p>
            <p><strong>Summary:</strong> arXiv:2405.08989v1 Announce Type: cross 
Abstract: What can contemporary machine learning (ML) models do? Given the proliferation of ML models in society, answering this question matters to a variety of stakeholders, both public and private. The evaluation of models' capabilities is rapidly emerging as a key subfield of modern ML, buoyed by regulatory attention and government grants. Despite this, the notion of an ML model possessing a capability has not been interrogated: what are we saying when we say that a model is able to do something? And what sorts of evidence bear upon this question? In this paper, we aim to answer these questions, using the capabilities of large language models (LLMs) as a running example. Drawing on the large philosophical literature on abilities, we develop an account of ML models' capabilities which can be usefully applied to the nascent science of model evaluation. Our core proposal is a conditional analysis of model abilities (CAMA): crudely, a machine learning model has a capability to X just when it would reliably succeed at doing X if it 'tried'. The main contribution of the paper is making this proposal precise in the context of ML, resulting in an operationalisation of CAMA applicable to LLMs. We then put CAMA to work, showing that it can help make sense of various features of ML model evaluation practice, as well as suggest procedures for performing fair inter-model comparisons.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08989">https://arxiv.org/abs/2405.08989</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper pertains to the capabilities and evaluation mechanisms of large language models (LLMs), which matches with your interest about agents based on large language models. It mainly discusses the potential abilities of LLMs but doesn't necessarily delve into specific applications like controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.15047" target="_blank">Implicit meta-learning may lead language models to trust more reliable sources</a></h3>
            <a href="https://arxiv.org/html/2310.15047v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.15047v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dmitrii Krasheninnikov, Egor Krasheninnikov, Bruno Mlodozeniec, Tegan Maharaj, David Krueger</p>
            <p><strong>Summary:</strong> arXiv:2310.15047v3 Announce Type: replace 
Abstract: We demonstrate that LLMs may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings ("tags") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to implicit meta-learning (IML): in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about capabilities, risks, and controllability of future AI systems. Our code can be found at https://github.com/krasheninnikov/internalization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.15047">https://arxiv.org/abs/2310.15047</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Since this paper discusses implicit meta-learning in large language models, it could provide useful insight into how such models control software or carry out computer automation. Even though it does not directly propose a new method, it presents an intriguing adjustment in the approach to using these agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.06353" target="_blank">Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes</a></h3>
            <a href="https://arxiv.org/html/2312.06353v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.06353v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng</p>
            <p><strong>Summary:</strong> arXiv:2312.06353v4 Announce Type: replace 
Abstract: Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioritizing perturbations with greater impact on model accuracy. Experiments across six scenarios with various LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in both communication efficiency and new task generalization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.06353">https://arxiv.org/abs/2312.06353</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses large language models and their optimization for fine-tuning responsiveness to instructions, which is vital for creating LLM-based agents. However, it does not explicitly talk about controlling software, web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.07553" target="_blank">Safe Reinforcement Learning with Free-form Natural Language Constraints and Pre-Trained Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.07553v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.07553v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xingzhou Lou, Junge Zhang, Ziyan Wang, Kaiqi Huang, Yali Du</p>
            <p><strong>Summary:</strong> arXiv:2401.07553v3 Announce Type: replace 
Abstract: Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents' comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.07553">https://arxiv.org/abs/2401.07553</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of pre-trained language models in reinforcement learning to comprehend language constraints, which aligns with your interest in agents that use large language models for tasks such as software control, automation and web browsers control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04291" target="_blank">BiLLM: Pushing the Limit of Post-Training Quantization for LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.04291v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04291v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan Qi</p>
            <p><strong>Summary:</strong> arXiv:2402.04291v2 Announce Type: replace 
Abstract: Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency. Our code is available at https://github.com/Aaronhuang-778/BiLLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04291">https://arxiv.org/abs/2402.04291</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper titled 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs' is relevant to your interests in 'Agents based on large-language models'. It describes a compression method for large language models, which could be important when using these models for controlling software or web browsers due to memory considerations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06270" target="_blank">XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</a></h3>
            <a href="https://arxiv.org/html/2405.06270v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.06270v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</p>
            <p><strong>Summary:</strong> arXiv:2405.06270v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs) into healthcare diagnostics offers a promising avenue for clinical decision-making. This study outlines the development of a novel method for zero-shot/few-shot in-context learning (ICL) by integrating medical domain knowledge using a multi-layered structured prompt. We also explore the efficacy of two communication styles between the user and LLMs: the Numerical Conversational (NC) style, which processes data incrementally, and the Natural Language Single-Turn (NL-ST) style, which employs long narrative prompts.
  Our study systematically evaluates the diagnostic accuracy and risk factors, including gender bias and false negative rates, using a dataset of 920 patient records in various few-shot scenarios. Results indicate that traditional clinical machine learning (ML) models generally outperform LLMs in zero-shot and few-shot settings. However, the performance gap narrows significantly when employing few-shot examples alongside effective explainable AI (XAI) methods as sources of domain knowledge. Moreover, with sufficient time and an increased number of examples, the conversational style (NC) nearly matches the performance of ML models. Most notably, LLMs demonstrate comparable or superior cost-sensitive accuracy relative to ML models.
  This research confirms that, with appropriate domain knowledge and tailored communication strategies, LLMs can significantly enhance diagnostic processes. The findings highlight the importance of optimizing the number of training examples and communication styles to improve accuracy and reduce biases in LLM applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06270">https://arxiv.org/abs/2405.06270</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper explores the application of Large Language Models (LLMs) in medical diagnostics and uses them for decision-making, which aligns with your interest in agents based on large-language models. However, it does not specifically cover the subtopics of using LLMs to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.03825" target="_blank">"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2308.03825v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.03825v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</p>
            <p><strong>Summary:</strong> arXiv:2308.03825v2 Announce Type: replace-cross 
Abstract: The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.03825">https://arxiv.org/abs/2308.03825</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper corresponds to your interest in large language models and their potential uses and challenges, particularly in terms of security. Although the paper does not specifically address using large language models for computer automation, it does provide valuable information on the misuse of LLMs, which could be an important aspect to consider when application these models to software or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.01974" target="_blank">Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers</a></h3>
            <a href="https://arxiv.org/html/2401.01974v2/extracted/5594333/figures/refcoco_skiers_blur.png" target="_blank"><img src="https://arxiv.org/html/2401.01974v2/extracted/5594333/figures/refcoco_skiers_blur.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aleksandar Stani\'c, Sergi Caelles, Michael Tschannen</p>
            <p><strong>Summary:</strong> arXiv:2401.01974v2 Announce Type: replace-cross 
Abstract: Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small number of labeled examples to automatically generate in-context examples, thereby avoiding human-created in-context examples. On a number of visual reasoning tasks, we show that our framework leads to consistent gains in performance, makes LLMs as controllers setup more robust, and removes the need for human engineering of in-context examples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.01974">https://arxiv.org/abs/2401.01974</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a solution using Large Language Models (LLMs) as controllers for visual reasoning tasks. This falls under your interest in using large language models for computer automation, specifically in cases where the task involves controlling or orchestrating various tools.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.10225" target="_blank">ChatQA: Surpassing GPT-4 on Conversational QA and RAG</a></h3>
            <a href="https://arxiv.org/html/2401.10225v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.10225v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro</p>
            <p><strong>Summary:</strong> arXiv:2401.10225v3 Announce Type: replace-cross 
Abstract: In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.10225">https://arxiv.org/abs/2401.10225</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines the design of ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). These methods could potentially be applied to automate tasks in controlling software or web browsers, as well as other computer-oriented tasks. While it does not explicitly focus on the control of web browsers or other agent-based tasks, it provides valuable insights into the capabilities of large language models, which could be of great interest given your specified subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01766" target="_blank">LLM Voting: Human Choices and AI Collective Decision Making</a></h3>
            <a href="https://arxiv.org/html/2402.01766v2/extracted/5598465/figures/overview_simple.png" target="_blank"><img src="https://arxiv.org/html/2402.01766v2/extracted/5598465/figures/overview_simple.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen, Dirk Helbing</p>
            <p><strong>Summary:</strong> arXiv:2402.01766v2 Announce Type: replace-cross 
Abstract: This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and a corresponding experiment with LLM agents. We observed that the methods used for voting input and the presentation of choices influence LLM voting behavior. We discovered that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the importance of cautious integration of LLMs into democratic processes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01766">https://arxiv.org/abs/2402.01766</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper applies large language models (LLMs) to informative decision-making processes, which resonates with your interest in agent-based applications of LLMs. However, it doesn't directly address the subtopics of controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18532" target="_blank">MileBench: Benchmarking MLLMs in Long Context</a></h3>
            <a href="https://arxiv.org/html/2404.18532v2/extracted/5597368/logo.png" target="_blank"><img src="https://arxiv.org/html/2404.18532v2/extracted/5597368/logo.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, Benyou Wang</p>
            <p><strong>Summary:</strong> arXiv:2404.18532v2 Announce Type: replace-cross 
Abstract: Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18532">https://arxiv.org/abs/2404.18532</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper does not directly align with your outlined interests in software control and browser control using LLMs, but it will give you an overview of the performance capabilities of Multimodal Large Language Models in long-context situations, which might be useful for designing and understanding the potential of LLM-agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.02213" target="_blank">Automatic Programming: Large Language Models and Beyond</a></h3>
            <a href="https://arxiv.org/html/2405.02213v2/extracted/5598233/figures/constraint.png" target="_blank"><img src="https://arxiv.org/html/2405.02213v2/extracted/5598233/figures/constraint.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michael R. Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, Patanamon Thongtanunam</p>
            <p><strong>Summary:</strong> arXiv:2405.02213v2 Announce Type: replace-cross 
Abstract: Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.02213">https://arxiv.org/abs/2405.02213</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the usage of Large Language Models (LLMs) in the process of Automatic Programming, which falls under the category of software control through LLMs. It further touches upon issues of code quality, programming responsibilities, and the potential shifts in work roles to accommodate LLMs in future - all of which tie back into the broader context of computer automation utilizing LLMs. However, it doesn't directly propose a new method, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06671" target="_blank">Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling</a></h3>
            <a href="https://arxiv.org/html/2405.06671v2/extracted/5598469/Images/ann_example.png" target="_blank"><img src="https://arxiv.org/html/2405.06671v2/extracted/5598469/Images/ann_example.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Subhendu Khatuya, Rajdeep Mukherjee, Akash Ghosh, Manjunath Hegde, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal</p>
            <p><strong>Summary:</strong> arXiv:2405.06671v2 Announce Type: replace-cross 
Abstract: We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification problem using a generative paradigm through instruction tuning of Large Language Models (LLMs). To this end, we leverage metric metadata information to frame our target outputs while proposing a parameter efficient solution for the task using LoRA. We perform experiments on two recently released financial numeric labeling datasets. Our proposed model, FLAN-FinXC, achieves new state-of-the-art performances on both the datasets, outperforming several strong baselines. We explain the better scores of our proposed model by demonstrating its capability for zero-shot as well as the least frequently occurring tags. Also, even when we fail to predict the XBRL tags correctly, our generated output has substantial overlap with the ground-truth in majority of the cases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06671">https://arxiv.org/abs/2405.06671</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it does not direct control on the software or web, this paper discusses instruction tuning of Large Language models for the task of numeral annotation in financial documents which aligns with your interest in automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.07896" target="_blank">Almanac Copilot: Towards Autonomous Electronic Health Record Navigation</a></h3>
            <a href="https://arxiv.org/html/2405.07896v2/extracted/5596816/figures/main_fig.png" target="_blank"><img src="https://arxiv.org/html/2405.07896v2/extracted/5596816/figures/main_fig.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cyril Zakka, Joseph Cho, Gracia Fahed, Rohan Shad, Michael Moor, Robyn Fong, Dhamanpreet Kaur, Vishnu Ravi, Oliver Aalami, Roxana Daneshjou, Akshay Chaudhari, William Hiesinger</p>
            <p><strong>Summary:</strong> arXiv:2405.07896v2 Announce Type: replace-cross 
Abstract: Clinicians spend large amounts of time on clinical documentation, and inefficiencies impact quality of care and increase clinician burnout. Despite the promise of electronic medical records (EMR), the transition from paper-based records has been negatively associated with clinician wellness, in part due to poor user experience, increased burden of documentation, and alert fatigue. In this study, we present Almanac Copilot, an autonomous agent capable of assisting clinicians with EMR-specific tasks such as information retrieval and order placement. On EHR-QA, a synthetic evaluation dataset of 300 common EHR queries based on real patient data, Almanac Copilot obtains a successful task completion rate of 74% (n = 221 tasks) with a mean score of 2.45 over 3 (95% CI:2.34-2.56). By automating routine tasks and streamlining the documentation process, our findings highlight the significant potential of autonomous agents to mitigate the cognitive load imposed on clinicians by current EMR systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.07896">https://arxiv.org/abs/2405.07896</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might interest you as it talks about using an autonomous agent for specific tasks in electronic health record systems. However, it's more about application rather than proposing new methods.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01454" target="_blank">Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach</a></h3>
            <a href="https://arxiv.org/html/2402.01454v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.01454v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</p>
            <p><strong>Summary:</strong> arXiv:2402.01454v2 Announce Type: replace 
Abstract: In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01454">https://arxiv.org/abs/2402.01454</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your interest in causality and machine learning, specifically in the subtopic of using large language models in causal discovery. It introduces a new methodology for causal inference which is a key interest of yours.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09493" target="_blank">Constrained Learning for Causal Inference and Semiparametric Statistics</a></h3>
            
            <p><strong>Authors:</strong> Tiffany Tianhui Cai, Yuri Fonseca, Kaiwen Hou, Hongseok Namkoong</p>
            <p><strong>Summary:</strong> arXiv:2405.09493v1 Announce Type: cross 
Abstract: Causal estimation (e.g. of the average treatment effect) requires estimating complex nuisance parameters (e.g. outcome models). To adjust for errors in nuisance parameter estimation, we present a novel correction method that solves for the best plug-in estimator under the constraint that the first-order error of the estimator with respect to the nuisance parameter estimate is zero. Our constrained learning framework provides a unifying perspective to prominent first-order correction approaches including debiasing (a.k.a. augmented inverse probability weighting) and targeting (a.k.a. targeted maximum likelihood estimation). Our semiparametric inference approach, which we call the "C-Learner", can be implemented with modern machine learning methods such as neural networks and tree ensembles, and enjoys standard guarantees like semiparametric efficiency and double robustness. Empirically, we demonstrate our approach on several datasets, including those with text features that require fine-tuning language models. We observe the C-Learner matches or outperforms other asymptotically optimal estimators, with better performance in settings with less estimated overlap.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09493">https://arxiv.org/abs/2405.09493</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper gives direct attention to causal estimation. It presents a new correction method for better causal inference and can be implemented with modern machine learning methods like neural networks. However, it does not specifically address causal discovery or use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09516" target="_blank">Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis</a></h3>
            <a href="https://arxiv.org/html/2405.09516v1/extracted/5598714/figures/fig1.png" target="_blank"><img src="https://arxiv.org/html/2405.09516v1/extracted/5598714/figures/fig1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daniel Csillag, Claudio Jos\'e Struchiner, Guilherme Tegoni Goedert</p>
            <p><strong>Summary:</strong> arXiv:2405.09516v1 Announce Type: cross 
Abstract: Many algorithms have been recently proposed for causal machine learning. Yet, there is little to no theory on their quality, especially considering finite samples. In this work, we propose a theory based on generalization bounds that provides such guarantees. By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited. Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity. We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09516">https://arxiv.org/abs/2405.09516</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in causality as it covers causal machine learning and thus can provide new insights and techniques for causal representation learning and discovery. It talks about proposing a theory for the quality of causal ML algorithms which might hint at new methods as well.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08029" target="_blank">PHUDGE: Phi-3 as Scalable Judge</a></h3>
            <a href="https://arxiv.org/html/2405.08029v2/extracted/5597429/PHUDGE_1.png" target="_blank"><img src="https://arxiv.org/html/2405.08029v2/extracted/5597429/PHUDGE_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mahesh Deshwal, Apoorva Chawla</p>
            <p><strong>Summary:</strong> arXiv:2405.08029v2 Announce Type: replace 
Abstract: In this paper cum technical report, we present PHUDGE A fine tuned Phi3 model that achieved SOTA results in 4 tasks as Feedback Test, Feedback OOD, MT Human, Preference Test surpassing each and every existing model in latency and throughput. It shows very strong correlation not only with GPT4 but with Human annotators too in unseen data as well as in both absolute and relative grading tasks. We have not only addressed the usage of small LMs for cost effective production grade systems but have also shown that Causal modelling is not only slow in nature but sometimes it can hinder models learning capabilities and should be replaced by simpler tasks whenever we can to make the overall system faster and better. We show that by following systematic ML experimentation, thoughtful data augmentation and re purposing the problem itself, we can even beat 10x bigger models even with lesser training data. To the best of our knowledge, we are re the first one to experiment and showcase the usage of generalised version of Earth Movers Distance AKA Wasserstein distance by using Minkowski Distance with a penalty to control loss smoothing and can be used as a loss function instead of Cross Entropy to get stable training and better results for grading tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08029">https://arxiv.org/abs/2405.08029</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> While the paper might not purely focus on causality, it does indeed mention the role of causal modeling and seems to propose alternative methods, thereby offering insights for your interest in 'Causal discovery' and 'Causal representation learning'. The paper also sheds light on advances related to large language models but is not explicit about their use in control systems or agent-based settings.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08498" target="_blank">Learning Decision Policies with Instrumental Variables through Double Machine Learning</a></h3>
            <a href="https://arxiv.org/html/2405.08498v2/extracted/5598147/causal_graph.png" target="_blank"><img src="https://arxiv.org/html/2405.08498v2/extracted/5598147/causal_graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska</p>
            <p><strong>Summary:</strong> arXiv:2405.08498v2 Announce Type: replace 
Abstract: A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08498">https://arxiv.org/abs/2405.08498</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper pertains to learning decision-making policies in data-rich settings, dealing with issues related to spurious correlations. It proposes a method - DML-IV, which falls under causal discovery. Although it does not directly mention large language models, the principles can be potentially utilized in the similar setups.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 16, 2024 at 21:33:53</div></body></html>