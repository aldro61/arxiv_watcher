
            <html>
            <head>
                <title>Report Generated on May 20, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 20, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10597" target="_blank">UniCL: A Universal Contrastive Learning Framework for Large Time Series Models</a></h3>
            <a href="https://arxiv.org/html/2405.10597v1/extracted/5602222/figures/overview.png" target="_blank"><img src="https://arxiv.org/html/2405.10597v1/extracted/5602222/figures/overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiawei Li, Jingshu Peng, Haoyang Li, Lei Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.10597v1 Announce Type: new 
Abstract: Time-series analysis plays a pivotal role across a range of critical applications, from finance to healthcare, which involves various tasks, such as forecasting and classification. To handle the inherent complexities of time-series data, such as high dimensionality and noise, traditional supervised learning methods first annotate extensive labels for time-series data in each task, which is very costly and impractical in real-world applications. In contrast, pre-trained foundation models offer a promising alternative by leveraging unlabeled data to capture general time series patterns, which can then be fine-tuned for specific tasks. However, existing approaches to pre-training such models typically suffer from high-bias and low-generality issues due to the use of predefined and rigid augmentation operations and domain-specific data training. To overcome these limitations, this paper introduces UniCL, a universal and scalable contrastive learning framework designed for pretraining time-series foundation models across cross-domain datasets. Specifically, we propose a unified and trainable time-series augmentation operation to generate pattern-preserved, diverse, and low-bias time-series data by leveraging spectral information. Besides, we introduce a scalable augmentation algorithm capable of handling datasets with varying lengths, facilitating cross-domain pretraining. Extensive experiments on two benchmark datasets across eleven domains validate the effectiveness of UniCL, demonstrating its high generalization on time-series analysis across various fields.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10597">https://arxiv.org/abs/2405.10597</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The abstract of this paper aligns well with your interest in new deep learning methods for time series and foundation models for time series. The authors are introducing UniCL, a new contrastive learning framework for pretraining time-series foundation models, which may be of significance to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15772" target="_blank">Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2404.15772v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15772v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aobo Liang, Xingguo Jiang, Yan Sun, Xiaohou Shi, Ke Li</p>
            <p><strong>Summary:</strong> arXiv:2404.15772v2 Announce Type: replace 
Abstract: Long-term time series forecasting (LTSF) provides longer insights into future trends and patterns. Over the past few years, deep learning models especially Transformers have achieved advanced performance in LTSF tasks. However, LTSF faces inherent challenges such as long-term dependencies capturing and sparse semantic characteristics. Recently, a new state space model (SSM) named Mamba is proposed. With the selective capability on input data and the hardware-aware parallel computing algorithm, Mamba has shown great potential in balancing predicting performance and computational efficiency compared to Transformers. To enhance Mamba's ability to preserve historical information in a longer range, we design a novel Mamba+ block by adding a forget gate inside Mamba to selectively combine the new features with the historical features in a complementary manner. Furthermore, we apply Mamba+ both forward and backward and propose Bi-Mamba+, aiming to promote the model's ability to capture interactions among time series elements. Additionally, multivariate time series data in different scenarios may exhibit varying emphasis on intra- or inter-series dependencies. Therefore, we propose a series-relation-aware decider that controls the utilization of channel-independent or channel-mixing tokenization strategy for specific datasets. Extensive experiments on 8 real-world datasets show that our model achieves more accurate predictions compared with state-of-the-art methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15772">https://arxiv.org/abs/2404.15772</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper seems very relevant to your interest. It proposes a new extension (Bi-Mamba+) to a deep learning model for time series forecasting, which could be said to be a new method for time series forecasting and mentions 'Transformers', possibly indicating it's a new transformer-like model for time series. Also, as it's tested on various datasets, it could provide information on datasets to train foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10608" target="_blank">ECATS: Explainable-by-design concept-based anomaly detection for time series</a></h3>
            <a href="https://arxiv.org/html/2405.10608v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.10608v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Irene Ferfoglia, Gaia Saveri, Laura Nenzi, Luca Bortolussi</p>
            <p><strong>Summary:</strong> arXiv:2405.10608v1 Announce Type: new 
Abstract: Deep learning methods for time series have already reached excellent performances in both prediction and classification tasks, including anomaly detection. However, the complexity inherent in Cyber Physical Systems (CPS) creates a challenge when it comes to explainability methods. To overcome this inherent lack of interpretability, we propose ECATS, a concept-based neuro-symbolic architecture where concepts are represented as Signal Temporal Logic (STL) formulae. Leveraging kernel-based methods for STL, concept embeddings are learnt in an unsupervised manner through a cross-attention mechanism. The network makes class predictions through these concept embeddings, allowing for a meaningful explanation to be naturally extracted for each input. Our preliminary experiments with a simple CPS-based dataset show that our model is able to achieve great classification performance while ensuring local interpretability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10608">https://arxiv.org/abs/2405.10608</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be of interest based on your request about new deep learning methods for time series. ECATS discusses a concept-based neuro-symbolic architecture for time series, which clearly denotes innovation in the methodological approach. However, it does not directly address forecasting or transformer-like models, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10800" target="_blank">Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.10800v1/extracted/5602515/images/intro.jpg" target="_blank"><img src="https://arxiv.org/html/2405.10800v1/extracted/5602515/images/intro.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, Xuan Song</p>
            <p><strong>Summary:</strong> arXiv:2405.10800v1 Announce Type: new 
Abstract: Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at https://github.com/XDZhelheim/HimNet.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10800">https://arxiv.org/abs/2405.10800</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes a novel approach for time series forecasting which utilizes meta-parameter learning. It captures and leverages spatiotemporal heterogeneity, which is closely tied to your interest in new methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10877" target="_blank">WEITS: A Wavelet-enhanced residual framework for interpretable time series forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.10877v1/extracted/5603220/Main_Sturcture.png" target="_blank"><img src="https://arxiv.org/html/2405.10877v1/extracted/5603220/Main_Sturcture.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ziyou Guo, Yan Sun, Tieru Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.10877v1 Announce Type: new 
Abstract: Time series (TS) forecasting has been an unprecedentedly popular problem in recent years, with ubiquitous applications in both scientific and business fields. Various approaches have been introduced to time series analysis, including both statistical approaches and deep neural networks. Although neural network approaches have illustrated stronger ability of representation than statistical methods, they struggle to provide sufficient interpretablility, and can be too complicated to optimize. In this paper, we present WEITS, a frequency-aware deep learning framework that is highly interpretable and computationally efficient. Through multi-level wavelet decomposition, WEITS novelly infuses frequency analysis into a highly deep learning framework. Combined with a forward-backward residual architecture, it enjoys both high representation capability and statistical interpretability. Extensive experiments on real-world datasets have demonstrated competitive performance of our model, along with its additional advantage of high computation efficiency. Furthermore, WEITS provides a general framework that can always seamlessly integrate with state-of-the-art approaches for time series forecast.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10877">https://arxiv.org/abs/2405.10877</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new deep learning method named WEITS for time series forecasting. It uses multi-level wavelet decomposition to increase interpretability and computational efficiency, which is relevant to your interests in new deep learning methods and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.14192" target="_blank">How Can Large Language Models Understand Spatial-Temporal Data?</a></h3>
            <a href="https://arxiv.org/html/2401.14192v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.14192v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, Yanming Shen</p>
            <p><strong>Summary:</strong> arXiv:2401.14192v2 Announce Type: replace 
Abstract: While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting. Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.14192">https://arxiv.org/abs/2401.14192</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The presented paper introduces a method that enables LLMs for spatial-temporal forecasting, which is synonymous with time series forecasting. Hence, it is relevant for your interest in novel deep learning methods and foundational models for time series, as well as your interest in large-scale language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.07836" target="_blank">Forecasting with Hyper-Trees</a></h3>
            
            <p><strong>Authors:</strong> Alexander M\"arz, Kashif Rasul</p>
            <p><strong>Summary:</strong> arXiv:2405.07836v2 Announce Type: replace 
Abstract: This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the issue of parameter non-stationarity and enable tree-based forecasts to extend beyond their training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to extend the application of gradient boosted decision trees outside their conventional use in time series modeling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.07836">https://arxiv.org/abs/2405.07836</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces Hyper-Trees, a new method for forecasting in time series data. Additionally, it extends the application of existing models, which falls under your interest in new deep learning methods and foundation models for time series.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10853" target="_blank">The Future of Large Language Model Pre-training is Federated</a></h3>
            <a href="https://arxiv.org/html/2405.10853v1/extracted/5601520/plots/flower_logo.png" target="_blank"><img src="https://arxiv.org/html/2405.10853v1/extracted/5601520/plots/flower_logo.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, Nicholas D. Lane</p>
            <p><strong>Summary:</strong> arXiv:2405.10853v1 Announce Type: new 
Abstract: Generative pre-trained large language models (LLMs) have demonstrated impressive performance over a wide range of tasks, thanks to the unprecedented amount of data they have been trained on. As established scaling laws indicate, LLMs' future performance improvement depends on the amount of computing and data sources we can leverage for pre-training. Federated learning (FL) has the potential to unleash the majority of the planet's data and computational resources, which are underutilized by the data-center-focused training methodology of current LLM practice. Our work presents a robust, flexible, reproducible FL approach that enables large-scale collaboration across institutions to train LLMs. This would mobilize more computational and data resources while matching or potentially exceeding centralized performance. We further show the effectiveness of the federated training scales with model size and present our approach for training a billion-scale federated LLM using limited resources. This will help data-rich actors to become the protagonists of LLMs pre-training instead of leaving the stage to compute-rich actors alone.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10853">https://arxiv.org/abs/2405.10853</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method to train large language models in a Federated Learning setting, which allows the collaboration across institutions. This indicates the possibility of using large language models to control software, potentially web browsers, and improve computer automation. However, the paper does not outline specific uses of the large language models for controlling software or web browsers, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10938" target="_blank">Observational Scaling Laws and the Predictability of Language Model Performance</a></h3>
            <a href="https://arxiv.org/html/2405.10938v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.10938v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yangjun Ruan, Chris J. Maddison, Tatsunori Hashimoto</p>
            <p><strong>Summary:</strong> arXiv:2405.10938v1 Announce Type: new 
Abstract: Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~80 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10938">https://arxiv.org/abs/2405.10938</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not address tasks such as controlling software or web browsers, it provides critical insights about the performance predictability of large language models like GPT-4, which can be valuable for designing agents based on these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10443" target="_blank">Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation</a></h3>
            <a href="https://arxiv.org/html/2405.10443v1/extracted/5599495/Figures/EncoderMask.png" target="_blank"><img src="https://arxiv.org/html/2405.10443v1/extracted/5599495/Figures/EncoderMask.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Matthew Raffel, Victor Agostinelli, Lizhong Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.10443v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as an unnecessarily expanded training set, computational inefficiency from dumping the KV cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, we propose a new paradigm in fine-tuning LLMs for simultaneous translation, called SimulMask. It utilizes a novel attention mask technique that models simultaneous translation during fine-tuning by masking attention connections under a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on three language pairs when averaged across four different latency regimes while reducing the computational cost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10443">https://arxiv.org/abs/2405.10443</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not specifically mention controlling software or browsers with a large language model, it does present a new paradigm in fine-tuning LLMs for simultaneous translation. If you're interested in the use of LLMs and their improvement, specifically in the context of real-time applications, then this paper could be very relevant.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.10621" target="_blank">Large language models can accurately predict searcher preferences</a></h3>
            <a href="https://arxiv.org/html/2309.10621v3/extracted/5601627/pareto.png" target="_blank"><img src="https://arxiv.org/html/2309.10621v3/extracted/5601627/pareto.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra</p>
            <p><strong>Summary:</strong> arXiv:2309.10621v3 Announce Type: replace-cross 
Abstract: Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that can be derived, and develops an large language model prompt that agrees with that data.
  We present ideas and observations from deploying language models for large-scale relevance labelling at Bing, and illustrate with data from TREC. We have found large language models can be effective, with accuracy as good as human labellers and similar capability to pick the hardest queries, best runs, and best groups. Systematic changes to the prompts make a difference in accuracy, but so too do simple paraphrases. To measure agreement with real searchers needs high-quality "gold" labels, but with these we find that models produce better labels than third-party workers, for a fraction of the cost, and these labels let us train notably better rankers.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.10621">https://arxiv.org/abs/2309.10621</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models in labeling data within a search system, which may be relevant to your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.15817" target="_blank">Identifying the Risks of LM Agents with an LM-Emulated Sandbox</a></h3>
            
            <p><strong>Authors:</strong> Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto</p>
            <p><strong>Summary:</strong> arXiv:2309.15817v2 Announce Type: replace-cross 
Abstract: Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.15817">https://arxiv.org/abs/2309.15817</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is mainly focused on the risks associated with using large language models to control machines (LM agents), which is at the core of your interests in this category. It discusses the functions, tools, and potential risks of LM agents, which aligns with your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.12321" target="_blank">Bypassing the Safety Training of Open-Source LLMs with Priming Attacks</a></h3>
            <a href="https://arxiv.org/html/2312.12321v2/extracted/5602110/images/llm_attack_final_bold.png" target="_blank"><img src="https://arxiv.org/html/2312.12321v2/extracted/5602110/images/llm_attack_final_bold.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh</p>
            <p><strong>Summary:</strong> arXiv:2312.12321v2 Announce Type: replace-cross 
Abstract: With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training. In this paper, we investigate the fragility of SOTA open-source LLMs under simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training. Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\times$ compared to baselines. Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.12321">https://arxiv.org/abs/2312.12321</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models' as it investigates the safety training aspect of these models, which is crucial for their effective implementation in controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.07927" target="_blank">Are self-explanations from Large Language Models faithful?</a></h3>
            
            <p><strong>Authors:</strong> Andreas Madsen, Sarath Chandar, Siva Reddy</p>
            <p><strong>Summary:</strong> arXiv:2401.07927v4 Announce Type: replace-cross 
Abstract: Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.07927">https://arxiv.org/abs/2401.07927</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses Large Language Models (LLMs) in detail, exploring their self-explanation abilities and faithfulness. Although it isn't specifically about controlling software or web browsers using LLMs, the analysis of how LLMs function and the proposal for self-consistency checks could be important for any application seeking to use LLMs for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19021" target="_blank">IDGenRec: LLM-RecSys Alignment with Textual ID Learning</a></h3>
            <a href="https://arxiv.org/html/2403.19021v2/extracted/5602001/images/id_generator.png" target="_blank"><img src="https://arxiv.org/html/2403.19021v2/extracted/5602001/images/id_generator.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.19021v2 Announce Type: replace-cross 
Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19021">https://arxiv.org/abs/2403.19021</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper discusses aspects of large language models, it primarily focuses on their use for recommendation systems, rather than controlling software or web browsers. However, it may still be relevant as it explores the potential of training foundation models (a subtopic of your interest) and their applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10292" target="_blank">Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2405.10292v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.10292v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine</p>
            <p><strong>Summary:</strong> arXiv:2405.10292v2 Announce Type: replace-cross 
Abstract: Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10292">https://arxiv.org/abs/2405.10292</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it explores how large vision-language models can be fine-tuned with reinforcement learning to act as decision-making agents in interactive environments. Although the paper doesn't specifically address controlling software or web browsers, it does discuss goal-directed tasks and could provide a useful framework for these applications.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10512" target="_blank">In-context Contrastive Learning for Event Causality Identification</a></h3>
            <a href="https://arxiv.org/html/2405.10512v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.10512v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chao Liang, Wei Xiang, Bang Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.10512v1 Announce Type: cross 
Abstract: Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an In-Context Contrastive Learning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10512">https://arxiv.org/abs/2405.10512</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper presents a novel In-Context Contrastive Learning model to improve Event Causality Identification which aligns with your interest in causal discovery. Although it doesn't specifically mention the use of large language models in causal discovery, it does relate to machine learning and causality.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03342" target="_blank">Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning</a></h3>
            <a href="https://arxiv.org/html/2405.03342v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03342v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Weilin Chen, Ruichu Cai, Zeqin Yang, Jie Qiao, Yuguang Yan, Zijian Li, Zhifeng Hao</p>
            <p><strong>Summary:</strong> arXiv:2405.03342v2 Announce Type: replace 
Abstract: Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03342">https://arxiv.org/abs/2405.03342</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interest in causality and machine learning, as it proposes a new method for causal effect estimation under networked interference. Although it doesn't specifically involve large language models in causal discovery, it still seems to be within the sphere of your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.05794" target="_blank">An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits</a></h3>
            <a href="https://arxiv.org/html/2311.05794v2/extracted/5601647/figures/width_plot_dist_Bern.png" target="_blank"><img src="https://arxiv.org/html/2311.05794v2/extracted/5601647/figures/width_plot_dist_Bern.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Biyonka Liang, Iavor Bojinov</p>
            <p><strong>Summary:</strong> arXiv:2311.05794v2 Announce Type: replace-cross 
Abstract: In multi-armed bandit (MAB) experiments, it is often advantageous to continuously produce inference on the average treatment effect (ATE) between arms as new data arrive and determine a data-driven stopping time for the experiment. We develop the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit experiments that produces powerful and anytime-valid inference on the ATE for \emph{any} bandit algorithm of the experimenter's choice, even those without probabilistic treatment assignment. Intuitively, the MAD "mixes" any bandit algorithm of the experimenter's choice with a Bernoulli design through a tuning parameter $\delta_t$, where $\delta_t$ is a deterministic sequence that decreases the priority placed on the Bernoulli design as the sample size grows. We prove that for $\delta_t = \omega\left(t^{-1/4}\right)$, the MAD generates anytime-valid asymptotic confidence sequences that are guaranteed to shrink around the true ATE. Hence, the experimenter is guaranteed to detect a true non-zero treatment effect in finite time. Additionally, we prove that the regret of the MAD approaches that of its underlying bandit algorithm over time, and hence, incurs a relatively small loss in regret in return for powerful inferential guarantees. Finally, we conduct an extensive simulation study exhibiting that the MAD achieves finite-sample anytime validity and high power without significant losses in finite-sample reward.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.05794">https://arxiv.org/abs/2311.05794</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 3.5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses about the average treatment effect (ATE) in multi-armed bandit experiments and proposes a new experimental design which is relevant to your interest in causal discovery. However, it doesn't directly fit under the sub-topics you outlined for Causality and Machine Learning, as it doesn't speak specifically to causal representation learning or the usage of large language models in causal discovery.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 20, 2024 at 21:33:37</div></body></html>