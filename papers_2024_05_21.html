
            <html>
            <head>
                <title>Report Generated on May 21, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 21, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11157" target="_blank">Towards Modular LLMs by Building and Reusing a Library of LoRAs</a></h3>
            <a href="https://arxiv.org/html/2405.11157v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11157v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, Alessandro Sordoni</p>
            <p><strong>Summary:</strong> arXiv:2405.11157v1 Announce Type: new 
Abstract: The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11157">https://arxiv.org/abs/2405.11157</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses advancements in large language models (LLMs) focusing on their performance across different tasks. The focus of the paper is on adaptability and versatility, which applies directly to using LLMs for software control and computer automation. It does not specify web browsers or software control, but the idea of improving LLM to handle new tasks is parallel to your query.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.10999" target="_blank">Large Language Models for Tuning Evolution Strategies</a></h3>
            <a href="https://arxiv.org/html/2405.10999v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.10999v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Oliver Kramer</p>
            <p><strong>Summary:</strong> arXiv:2405.10999v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit world knowledge and inference capabilities, making them powerful tools for various applications. This paper proposes a feedback loop mechanism that leverages these capabilities to tune Evolution Strategies (ES) parameters effectively. The mechanism involves a structured process of providing programming instructions, executing the corresponding code, and conducting thorough analysis. This process is specifically designed for the optimization of ES parameters. The method operates through an iterative cycle, ensuring continuous refinement of the ES parameters. First, LLMs process the instructions to generate or modify the code. The code is then executed, and the results are meticulously logged. Subsequent analysis of these results provides insights that drive further improvements. An experiment on tuning the learning rates of ES using the LLaMA3 model demonstrate the feasibility of this approach. This research illustrates how LLMs can be harnessed to improve ES algorithms' performance and suggests broader applications for similar feedback loop mechanisms in various domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.10999">https://arxiv.org/abs/2405.10999</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agent-based large language models. It explores how large language models can be used to improve Evolution Strategies algorithms, which is a form of software control that falls under your specified interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11002" target="_blank">Large Language Models in Wireless Application Design: In-Context Learning-enhanced Automatic Network Intrusion Detection</a></h3>
            <a href="https://arxiv.org/html/2405.11002v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11002v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Han Zhang, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci</p>
            <p><strong>Summary:</strong> arXiv:2405.11002v1 Announce Type: new 
Abstract: Large language models (LLMs), especially generative pre-trained transformers (GPTs), have recently demonstrated outstanding ability in information comprehension and problem-solving. This has motivated many studies in applying LLMs to wireless communication networks. In this paper, we propose a pre-trained LLM-empowered framework to perform fully automatic network intrusion detection. Three in-context learning methods are designed and compared to enhance the performance of LLMs. With experiments on a real network intrusion detection dataset, in-context learning proves to be highly beneficial in improving the task processing performance in a way that no further training or fine-tuning of LLMs is required. We show that for GPT-4, testing accuracy and F1-Score can be improved by 90%. Moreover, pre-trained LLMs demonstrate big potential in performing wireless communication-related tasks. Specifically, the proposed framework can reach an accuracy and F1-Score of over 95% on different types of attacks with GPT-4 using only 10 in-context learning examples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11002">https://arxiv.org/abs/2405.11002</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it explores the use of large language models (LLMs), specifically generative pre-trained transformers (GPTs), for automatic network intrusion detection. It doesn't directly relate to your subtopics of using LLMs to control software or web browsers, or computer automation, but it does contribute to the broader understanding of implementing LLMs in practical applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11029" target="_blank">Generative Artificial Intelligence: A Systematic Review and Applications</a></h3>
            <a href="https://arxiv.org/html/2405.11029v1/extracted/5603534/Images/gans/arc-gan-v2.png" target="_blank"><img src="https://arxiv.org/html/2405.11029v1/extracted/5603534/Images/gans/arc-gan-v2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sandeep Singh Sengar, Affan Bin Hasan, Sanjay Kumar, Fiona Carroll</p>
            <p><strong>Summary:</strong> arXiv:2405.11029v1 Announce Type: new 
Abstract: In recent years, the study of artificial intelligence (AI) has undergone a paradigm shift. This has been propelled by the groundbreaking capabilities of generative models both in supervised and unsupervised learning scenarios. Generative AI has shown state-of-the-art performance in solving perplexing real-world conundrums in fields such as image translation, medical diagnostics, textual imagery fusion, natural language processing, and beyond. This paper documents the systematic review and analysis of recent advancements and techniques in Generative AI with a detailed discussion of their applications including application-specific models. Indeed, the major impact that generative AI has made to date, has been in language generation with the development of large language models, in the field of image translation and several other interdisciplinary applications of generative AI. Moreover, the primary contribution of this paper lies in its coherent synthesis of the latest advancements in these areas, seamlessly weaving together contemporary breakthroughs in the field. Particularly, how it shares an exploration of the future trajectory for generative AI. In conclusion, the paper ends with a discussion of Responsible AI principles, and the necessary ethical considerations for the sustainability and growth of these generative models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11029">https://arxiv.org/abs/2405.11029</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant as it discusses large language models and their applications, which aligns with your interest in agents based on large language models. Although it does not directly mention controlling software or web browsers, it still offers valuable insights on the advancements and future trajectory of generative AI and large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11226" target="_blank">The Power of Active Multi-Task Learning in Reinforcement Learning from Human Feedback</a></h3>
            
            <p><strong>Authors:</strong> Ruitao Chen, Liwei Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.11226v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has contributed to performance improvements in large language models. To tackle its reliance on substantial amounts of human-labeled data, a successful approach is multi-task representation learning, which involves learning a high-quality, low-dimensional representation from a wide range of source tasks. In this paper, we formulate RLHF as the contextual dueling bandit problem and assume a common linear representation. We demonstrate that the sample complexity of source tasks in multi-task RLHF can be reduced by considering task relevance and allocating different sample sizes to source tasks with varying task relevance. We further propose an algorithm to estimate task relevance by a small number of additional data and then learn a policy. We prove that to achieve $\varepsilon-$optimal, the sample complexity of the source tasks can be significantly reduced compared to uniform sampling. Additionally, the sample complexity of the target task is only linear in the dimension of the latent space, thanks to representation learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11226">https://arxiv.org/abs/2405.11226</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large language models. It discusses reinforcement learning from human feedback, an approach that has been employed in large language models. The paper proposes an algorithm to estimate task relevance, which can be interpreted as a new method for controlling software using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11344" target="_blank">Improved Content Understanding With Effective Use of Multi-task Contrastive Learning</a></h3>
            
            <p><strong>Authors:</strong> Akanksha Bindal, Sudarshan Ramanujam, Dave Golland, TJ Hazen, Tina Jiang, Fengyu Zhang, Peng Yan</p>
            <p><strong>Summary:</strong> arXiv:2405.11344v1 Announce Type: new 
Abstract: In enhancing LinkedIn core content recommendation models, a significant challenge lies in improving their semantic understanding capabilities. This paper addresses the problem by leveraging multi-task learning, a method that has shown promise in various domains. We fine-tune a pre-trained, transformer-based LLM using multi-task contrastive learning with data from a diverse set of semantic labeling tasks. We observe positive transfer, leading to superior performance across all tasks when compared to training independently on each. Our model outperforms the baseline on zero shot learning and offers improved multilingual support, highlighting its potential for broader application. The specialized content embeddings produced by our model outperform generalized embeddings offered by OpenAI on Linkedin dataset and tasks. This work provides a robust foundation for vertical teams across LinkedIn to customize and fine-tune the LLM to their specific applications. Our work offers insights and best practices for the field to build on.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11344">https://arxiv.org/abs/2405.11344</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses about leveraging a pre-trained, transformer-based Large Language Model (LLM) using multi-task contrastive learning for LinkedIn's content recommendation models, which can relate to using large language models to control software. However, it does not directly discuss about controlling web browsers or computer automation using LLM.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11704" target="_blank">Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks</a></h3>
            
            <p><strong>Authors:</strong> Taiyuan Mei, Yun Zi, Xiaohan Cheng, Zijun Gao, Qi Wang, Haowei Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.11704v1 Announce Type: new 
Abstract: The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11704">https://arxiv.org/abs/2405.11704</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the efficiency optimization of large-scale language models which fits your interest in large language models. However, it does not focus directly on controlling software or web browsers, nor on automation using these models, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11880" target="_blank">Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.11880v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11880v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siyu Lou, Yuntian Chen, Xiaodan Liang, Liang Lin, Quanshi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.11880v1 Announce Type: new 
Abstract: In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation. These effects are formulated as non-linear interactions between tokens/words encoded by the LLM. Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns. Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects. Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11880">https://arxiv.org/abs/2405.11880</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research touches upon large language models (LLMs) and their effects and interactions which relate to your interest in agents based on large-language models. However, it doesn't specifically reference control software, web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11930" target="_blank">Data Contamination Calibration for Black-box LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.11930v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11930v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wentao Ye, Jiaqi Hu, Liyao Li, Haobo Wang, Gang Chen, Junbo Zhao</p>
            <p><strong>Summary:</strong> arXiv:2405.11930v1 Announce Type: new 
Abstract: The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11930">https://arxiv.org/abs/2405.11930</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses Large Language Models (LLMs) and proposes a new method to calibrate the contamination in data used for their training, which could have implications on how these LLMs are employed to control software or automate tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11070" target="_blank">Jill Watson: A Virtual Teaching Assistant powered by ChatGPT</a></h3>
            <a href="https://arxiv.org/html/2405.11070v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11070v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Karan Taneja, Pratyusha Maiti, Sandeep Kakar, Pranav Guruprasad, Sanjeev Rao, Ashok K. Goel</p>
            <p><strong>Summary:</strong> arXiv:2405.11070v1 Announce Type: cross 
Abstract: Conversational AI agents often require extensive datasets for training that are not publicly released, are limited to social chit-chat or handling a specific domain, and may not be easily extended to accommodate the latest advances in AI technologies. This paper introduces Jill Watson, a conversational Virtual Teaching Assistant (VTA) leveraging the capabilities of ChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a modular design to allow the integration of new APIs using a skill-based architecture inspired by XiaoIce. Jill Watson is also well-suited for intelligent textbooks as it can process and converse using multiple large documents. We exclusively utilize publicly available resources for reproducibility and extensibility. Comparative analysis shows that our system outperforms the legacy knowledge-based Jill Watson as well as the OpenAI Assistants service. We employ many safety measures that reduce instances of hallucinations and toxicity. The paper also includes real-world examples from a classroom setting that demonstrate different features of Jill Watson and its effectiveness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11070">https://arxiv.org/abs/2405.11070</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is about Jill Watson, a virtual teaching assistant built on top of GPT, a large language model. It aligns well with your interest in agent-based applications of large language models, specifically those used to control software. However, it does not address controlling web browsers or other more specific aspects of automation, hence a score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11083" target="_blank">Prompt Exploration with Prompt Regression</a></h3>
            <a href="https://arxiv.org/html/2405.11083v1/extracted/5603673/figures/PEPR_overview_v2_cropped.png" target="_blank"><img src="https://arxiv.org/html/2405.11083v1/extracted/5603673/figures/PEPR_overview_v2_cropped.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Michael Feffer, Ronald Xu, Yuekai Sun, Mikhail Yurochkin</p>
            <p><strong>Summary:</strong> arXiv:2405.11083v1 Announce Type: cross 
Abstract: In the advent of democratized usage of large language models (LLMs), there is a growing desire to systematize LLM prompt creation and selection processes beyond iterative trial-and-error. Prior works majorly focus on searching the space of prompts without accounting for relations between prompt variations.
  Here we propose a framework, Prompt Exploration with Prompt Regression (PEPR), to predict the effect of prompt combinations given results for individual prompt elements as well as a simple method to select an effective prompt for a given use-case. We evaluate our approach with open-source LLMs of different sizes on several different tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11083">https://arxiv.org/abs/2405.11083</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a framework dealing with the use of large language models (LLMs), which is aligned with your interest in agents based on large-language models. However, it does not specifically mention controlling software or web browsers with LLMs, hence not a full score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11106" target="_blank">LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions</a></h3>
            
            <p><strong>Authors:</strong> Chuanneng Sun, Songjun Huang, Dario Pompili</p>
            <p><strong>Summary:</strong> arXiv:2405.11106v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11106">https://arxiv.org/abs/2405.11106</a></p>
            <p><strong>Category:</strong> cs.MA</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on the area of Large Language Models (LLMs) being used as agents in Reinforcement Learning (RL) scenarios. It describes current and future directions of LLMs used as agents, which aligns with your focus on agents controlled by large language models. However, it doesn't specifically mention software or web browser control, which is why it gets a score of 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11120" target="_blank">Latent State Estimation Helps UI Agents to Reason</a></h3>
            <a href="https://arxiv.org/html/2405.11120v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11120v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William E Bishop, Alice Li, Christopher Rawles, Oriana Riva</p>
            <p><strong>Summary:</strong> arXiv:2405.11120v1 Announce Type: cross 
Abstract: A common problem for agents operating in real-world environments is that the response of an environment to their actions may be non-deterministic and observed through noise. This renders environmental state and progress towards completing a task latent. Despite recent impressive demonstrations of LLM's reasoning abilities on various benchmarks, whether LLMs can build estimates of latent state and leverage them for reasoning has not been explicitly studied. We investigate this problem in the real-world domain of autonomous UI agents. We establish that appropriately prompting LLMs in a zero-shot manner can be formally understood as forming point estimates of latent state in a textual space. In the context of autonomous UI agents we then show that LLMs used in this manner are more than $76\%$ accurate at inferring various aspects of latent state, such as performed (vs. commanded) actions and task progression. Using both public and internal benchmarks and three reasoning methods (zero-shot, CoT-SC & ReAct), we show that LLM-powered agents that explicitly estimate and reason about latent state are able to successfully complete up to 1.6x more tasks than those that do not.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11120">https://arxiv.org/abs/2405.11120</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is relevant as it discusses the use of large language models (LLMs) in the real-world domain of autonomous UI agents. The estimation of latent state and reasoning provides a new technique for controlling software automation, which aligns with your interest. However, it does not specifically touch on controlling web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11143" target="_blank">OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</a></h3>
            <a href="https://arxiv.org/html/2405.11143v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11143v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jian Hu, Xibin Wu, Weixun Wang,  Xianyu, Dehao Zhang, Yu Cao</p>
            <p><strong>Summary:</strong> arXiv:2405.11143v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11143">https://arxiv.org/abs/2405.11143</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a framework for efficient scaling of reinforcement learning from human feedback (RLHF) for training large language models, which corresponds to your interest in agents based on large-language models. However, it doesn't specifically mention controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11277" target="_blank">Action Controlled Paraphrasing</a></h3>
            <a href="https://arxiv.org/html/2405.11277v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11277v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ning Shi, Zijun Wu, Lili Mou</p>
            <p><strong>Summary:</strong> arXiv:2405.11277v1 Announce Type: cross 
Abstract: Recent studies have demonstrated the potential to control paraphrase generation, such as through syntax, which has broad applications in various downstream tasks. However, these methods often require detailed parse trees or syntactic exemplars, which are not user-friendly. Furthermore, an inference gap exists, as control specifications are only available during training but not inference. In this work, we propose a new setup for controlled paraphrasing. Specifically, we represent user-intended actions as action tokens, allowing embedding and concatenating them with text embeddings, thus flowing together to a self-attention encoder for representation fusion. To address the inference gap, we introduce an optional action token as a placeholder that encourages the model to determine the appropriate action when control specifications are inaccessible. Experimental results show that our method successfully enables specific action-controlled paraphrasing and preserves the same or even better performance compared to conventional uncontrolled methods when actions are not given. Our findings thus promote the concept of optional action control for a more user-centered design via representation learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11277">https://arxiv.org/abs/2405.11277</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper, 'Action Controlled Paraphrasing' is relevant to the 'llm-agents' tag as it's about controlled paraphrasing using token action embeddings which can be related to controlling software or automating processes using large language models. However, it does not specifically mention software or web browser management.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11299" target="_blank">The CAP Principle for LLM Serving</a></h3>
            <a href="https://arxiv.org/html/2405.11299v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11299v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, Xusheng Chen, Yizhou Shan</p>
            <p><strong>Summary:</strong> arXiv:2405.11299v1 Announce Type: cross 
Abstract: We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11299">https://arxiv.org/abs/2405.11299</a></p>
            <p><strong>Category:</strong> cs.DB</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your third interest particularly in the aspect of large language models. It discusses the dynamics and optimizations of large language models (LLMs) in serving contexts which could be significant in tasks such as controlling software or achieving computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11422" target="_blank">Large Language Models are Biased Reinforcement Learners</a></h3>
            <a href="https://arxiv.org/html/2405.11422v1/extracted/5604969/figures/Figure_1.png" target="_blank"><img src="https://arxiv.org/html/2405.11422v1/extracted/5604969/figures/Figure_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William M. Hayes, Nicolas Yax, Stefano Palminteri</p>
            <p><strong>Summary:</strong> arXiv:2405.11422v1 Announce Type: cross 
Abstract: In-context learning enables large language models (LLMs) to perform a variety of tasks, including learning to make reward-maximizing choices in simple bandit tasks. Given their potential use as (autonomous) decision-making agents, it is important to understand how these models perform such reinforcement learning (RL) tasks and the extent to which they are susceptible to biases. Motivated by the fact that, in humans, it has been widely documented that the value of an outcome depends on how it compares to other local outcomes, the present study focuses on whether similar value encoding biases apply to how LLMs encode rewarding outcomes. Results from experiments with multiple bandit tasks and models show that LLMs exhibit behavioral signatures of a relative value bias. Adding explicit outcome comparisons to the prompt produces opposing effects on performance, enhancing maximization in trained choice sets but impairing generalization to new choice sets. Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage. Lastly, we present preliminary evidence that the observed biases are not limited to fine-tuned LLMs, and that relative value processing is detectable in the final hidden layer activations of a raw, pretrained model. These findings have important implications for the use of LLMs in decision-making applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11422">https://arxiv.org/abs/2405.11422</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant particularly to your interest in how large language models may be used as decision-making agents. It examines different biases in the model's decision-making process, which is fundamental information if you're interested in using large language models for tasks such as controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11446" target="_blank">MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning</a></h3>
            <a href="https://arxiv.org/html/2405.11446v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11446v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, Aidong Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.11446v1 Announce Type: cross 
Abstract: Adapting large language models (LLMs) to unseen tasks with in-context training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only perform well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in meta-training literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11446">https://arxiv.org/abs/2405.11446</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the approach of meta-training Large Language Models (LLMs), which is relevant to your interest in 'Agents based on Large Language Models'. It particularly focuses on the adaptability of these models to unseen tasks, which could be relevant in their use to control software and web browsers as outlined in your interests. While it doesn't directly address your subtopics, the overall topic match and relevance to large language model makes it worth considering.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11715" target="_blank">Semantic Trajectory Data Mining with LLM-Informed POI Classification</a></h3>
            <a href="https://arxiv.org/html/2405.11715v1/extracted/5606049/figures/flow_diagram_new.png" target="_blank"><img src="https://arxiv.org/html/2405.11715v1/extracted/5606049/figures/flow_diagram_new.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yifan Liu, Chenchen Kuai, Haoxuan Ma, Xishun Liao, Brian Yueshuai He, Jiaqi Ma</p>
            <p><strong>Summary:</strong> arXiv:2405.11715v1 Announce Type: cross 
Abstract: Human travel trajectory mining is crucial for transportation systems, enhancing route optimization, traffic management, and the study of human travel patterns. Previous rule-based approaches without the integration of semantic information show a limitation in both efficiency and accuracy. Semantic information, such as activity types inferred from Points of Interest (POI) data, can significantly enhance the quality of trajectory mining. However, integrating these insights is challenging, as many POIs have incomplete feature information, and current learning-based POI algorithms require the integrity of datasets to do the classification. In this paper, we introduce a novel pipeline for human travel trajectory mining. Our approach first leverages the strong inferential and comprehension capabilities of large language models (LLMs) to annotate POI with activity types and then uses a Bayesian-based algorithm to infer activity for each stay point in a trajectory. In our evaluation using the OpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a 96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1 score in activity inference.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11715">https://arxiv.org/abs/2405.11715</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest because it leverages large language models to annotate Points of Interest data, contributing to advances in the computer automation using LLMs. However, it doesn't directly discuss controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11751" target="_blank">Asymptotic theory of in-context learning by linear attention</a></h3>
            <a href="https://arxiv.org/html/2405.11751v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11751v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, Cengiz Pehlevan</p>
            <p><strong>Summary:</strong> arXiv:2405.11751v1 Announce Type: cross 
Abstract: Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11751">https://arxiv.org/abs/2405.11751</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses in-context learning capabilities of Transformers, a critical component in designing large language model based agents. Although it doesn't directly deal with either software control or web browsers, the concepts can potentially be applied in those domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12130" target="_blank">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></h3>
            <a href="https://arxiv.org/html/2405.12130v1/extracted/5606540/lora.png" target="_blank"><img src="https://arxiv.org/html/2405.12130v1/extracted/5606540/lora.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang</p>
            <p><strong>Summary:</strong> arXiv:2405.12130v1 Announce Type: cross 
Abstract: Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12130">https://arxiv.org/abs/2405.12130</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses a new method, MoRA, for large language models (LLMs). It could be beneficial for control and automation tasks, using LLMs, that require effective learning and memorization of new knowledge.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12205" target="_blank">Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</a></h3>
            <a href="https://arxiv.org/html/2405.12205v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12205v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora</p>
            <p><strong>Summary:</strong> arXiv:2405.12205v1 Announce Type: cross 
Abstract: Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.
  To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12205">https://arxiv.org/abs/2405.12205</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems relevant to your interest in large-language model agents. It explores how large language models (LLMs) can be used for task-specific reasoning and problem-solving, which aligns with your interest in automation using LLMs, although it does not specifically mention control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12213" target="_blank">Octo: An Open-Source Generalist Robot Policy</a></h3>
            <a href="https://arxiv.org/html/2405.12213v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12213v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong>  Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine</p>
            <p><strong>Summary:</strong> arXiv:2405.12213v1 Announce Type: cross 
Abstract: Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12213">https://arxiv.org/abs/2405.12213</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper involves large language models controlling robots, which seems closest to your interest in large language models controlling software or web browsers. Though it's not exactly the same, the underlying principles applied in robotic manipulation could likely be applied in the contexts you are interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.05467" target="_blank">Active Label Correction for Building LLM-based Modular AI Systems</a></h3>
            <a href="https://arxiv.org/html/2401.05467v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.05467v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Karan Taneja, Ashok Goel</p>
            <p><strong>Summary:</strong> arXiv:2401.05467v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been used to build modular AI systems such as HuggingGPT, Microsoft Bing Chat, and more. To improve such systems after deployment using the data collected from human interactions, each module can be replaced by a fine-tuned model but the annotations received from LLMs are low quality. We propose that active label correction can be used to improve the data quality by only examining a fraction of the dataset. In this paper, we analyze the noise in datasets annotated by ChatGPT and study denoising it with human feedback. Our results show that active label correction can lead to oracle performance with feedback on fewer examples than the number of noisy examples in the dataset across three different NLP tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.05467">https://arxiv.org/abs/2401.05467</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your third interest as it discusses the improvement of systems (like AI) that are built using Large Language Models (LLM), which includes the use of human feedback. However, it does not specifically cover controlling software or web browsers with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.19187" target="_blank">Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2305.19187v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.19187v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Lin, Shubhendu Trivedi, Jimeng Sun</p>
            <p><strong>Summary:</strong> arXiv:2305.19187v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.19187">https://arxiv.org/abs/2305.19187</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not directly discuss controlling software or web browsers with large language models, it gives valuable insights into uncertainty management when utilizing LLMs which could be critical in any applications of language models in autonomous agents. The confidence measurement methods proposed can be beneficial in understanding and interpreting the output of LLMs when being used as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.00012" target="_blank">FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair</a></h3>
            <a href="https://arxiv.org/html/2307.00012v3/extracted/5606017/Images/FlakyFix__.png" target="_blank"><img src="https://arxiv.org/html/2307.00012v3/extracted/5606017/Images/FlakyFix__.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sakina Fatima, Hadi Hemmati, Lionel Briand</p>
            <p><strong>Summary:</strong> arXiv:2307.00012v3 Announce Type: replace-cross 
Abstract: Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky test cases where the root cause of flakiness is in the test case itself and not in the production code. Our key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, in addition to informing testers, we augment a Large Language Model (LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 70% and 90%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.00012">https://arxiv.org/abs/2307.00012</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models, more specifically on utilising such models to generate fixes for flaky tests. Although it does not directly deal with controlling software or web browsers, it is related to the topic of computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11809" target="_blank">Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding</a></h3>
            <a href="https://arxiv.org/html/2402.11809v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11809v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao</p>
            <p><strong>Summary:</strong> arXiv:2402.11809v3 Announce Type: replace-cross 
Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11809">https://arxiv.org/abs/2402.11809</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses techniques for accelerating the inference speed of large language models, which is quite relevant to your interest in computer automation and controlling softwares or web browsers using large language models. However, it's not directly proposing a method for these applications, which is why it's not a perfect 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14472" target="_blank">Detoxifying Large Language Models via Knowledge Editing</a></h3>
            
            <p><strong>Authors:</strong> Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2403.14472v4 Announce Type: replace-cross 
Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14472">https://arxiv.org/abs/2403.14472</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> You should read this paper because it focuses on detoxifying Large Language Models which could have an impact on their use in controlling software or automation. However, it does not directly address controlling software or web browsers with these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.03204" target="_blank">RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis</a></h3>
            <a href="https://arxiv.org/html/2404.03204v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.03204v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, Dongchao Yang, Yuancheng Wang, Shinnosuke Takamichi, Hiroshi Saruwatari, Shujie Liu, Jinyu Li, Sheng Zhao</p>
            <p><strong>Summary:</strong> arXiv:2404.03204v3 Announce Type: replace-cross 
Abstract: We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\%$ (without reranking) and $1.7\%$ (with reranking) to $2.5\%$ and $1.0\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\%$ to $4\%$.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.03204">https://arxiv.org/abs/2404.03204</a></p>
            <p><strong>Category:</strong> eess.AS</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper provides insights into the use of large language models (LLMs) for Text-to-Speech (TTS) tasks, it does not directly touch upon using LLMs for controlling software or web browsers. However, its innovative method of enhancing LLM robustness through 'Chain-of-Thought' prompting could potentially be applicable to LLM agent development and therefore could be of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08164" target="_blank">Language Model Prompt Selection via Simulation Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.08164v2/extracted/5606136/figure1_test3.png" target="_blank"><img src="https://arxiv.org/html/2404.08164v2/extracted/5606136/figure1_test3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoting Zhang, Jinghai He, Rhonda Righter, Zeyu Zheng</p>
            <p><strong>Summary:</strong> arXiv:2404.08164v2 Announce Type: replace-cross 
Abstract: With the advancement in generative language models, the selection of prompts has gained significant attention in recent years. A prompt is an instruction or description provided by the user, serving as a guide for the generative language model in content generation. Despite existing methods for prompt selection that are based on human labor, we consider facilitating this selection through simulation optimization, aiming to maximize a pre-defined score for the selected prompt. Specifically, we propose a two-stage framework. In the first stage, we determine a feasible set of prompts in sufficient numbers, where each prompt is represented by a moderate-dimensional vector. In the subsequent stage for evaluation and selection, we construct a surrogate model of the score regarding the moderate-dimensional vectors that represent the prompts. We propose sequentially selecting the prompt for evaluation based on this constructed surrogate model. We prove the consistency of the sequential evaluation procedure in our framework. We also conduct numerical experiments to demonstrate the efficacy of our proposed framework, providing practical instructions for implementation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08164">https://arxiv.org/abs/2404.08164</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it deals with the use of language models for prompting a system, which is a method for controlling software through a large language model. Although it is not specifically about time-series or causal discovery, it provides essential methods for optimizing the selection process which can be applied in creating llm-agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.06424" target="_blank">Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation</a></h3>
            <a href="https://arxiv.org/html/2405.06424v2/extracted/5602248/Figures/orca_gpt4.png" target="_blank"><img src="https://arxiv.org/html/2405.06424v2/extracted/5602248/Figures/orca_gpt4.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> JoonHo Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min</p>
            <p><strong>Summary:</strong> arXiv:2405.06424v2 Announce Type: replace-cross 
Abstract: Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.06424">https://arxiv.org/abs/2405.06424</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is fairly relevant to your third area of interest - 'Agents based on large-language models'. It proposes a new method for training language models to follow instructions, which may be seen as a form of software control. Moreover, the approach improves data curation for training large language models, which could be of interest to you.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11470" target="_blank">VCformer: Variable Correlation Transformer with Inherent Lagged Correlation for Multivariate Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.11470v1/extracted/5597090/illustrations/lag0.png" target="_blank"><img src="https://arxiv.org/html/2405.11470v1/extracted/5597090/illustrations/lag0.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yingnan Yang, Qingling Zhu, Jianyong Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.11470v1 Announce Type: new 
Abstract: Multivariate time series (MTS) forecasting has been extensively applied across diverse domains, such as weather prediction and energy consumption. However, current studies still rely on the vanilla point-wise self-attention mechanism to capture cross-variable dependencies, which is inadequate in extracting the intricate cross-correlation implied between variables. To fill this gap, we propose Variable Correlation Transformer (VCformer), which utilizes Variable Correlation Attention (VCA) module to mine the correlations among variables. Specifically, based on the stochastic process theory, VCA calculates and integrates the cross-correlation scores corresponding to different lags between queries and keys, thereby enhancing its ability to uncover multivariate relationships. Additionally, inspired by Koopman dynamics theory, we also develop Koopman Temporal Detector (KTD) to better address the non-stationarity in time series. The two key components enable VCformer to extract both multivariate correlations and temporal dependencies. Our extensive experiments on eight real-world datasets demonstrate the effectiveness of VCformer, achieving top-tier performance compared to other state-of-the-art baseline models. Code is available at this repository: https://github.com/CSyyn/VCformer.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11470">https://arxiv.org/abs/2405.11470</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests. It presents a new method, the Variable Correlation Transformer, for multivariate time series forecasting. This falls under the subtopic 'New transformer-like models for time series'. Additionally, it addresses the topic of 'New deep learning methods for time series', hence the high relevancy score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11848" target="_blank">Alternators For Sequence Modeling</a></h3>
            <a href="https://arxiv.org/html/2405.11848v1/extracted/5606510/figures/Fig1.png" target="_blank"><img src="https://arxiv.org/html/2405.11848v1/extracted/5606510/figures/Fig1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mohammad Reza Rezaei, Adji Bousso Dieng</p>
            <p><strong>Summary:</strong> arXiv:2405.11848v1 Announce Type: cross 
Abstract: This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11848">https://arxiv.org/abs/2405.11848</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'time series and deep learning' since it presents a novel model, called alternators, for sequence-to-sequence predictors. It deals with the subtopic of 'new deep learning methods for time series' forecasting. They showcase the capabilities of alternators in a variety of applications, including time series forecasting for sea-surface temperatures.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03199" target="_blank">Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.03199v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03199v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nannan Bian, Minhong Zhu, Li Chen, Weiran Cai</p>
            <p><strong>Summary:</strong> arXiv:2405.03199v2 Announce Type: replace 
Abstract: Deep learning methods have been exerting their strengths in long-term time series forecasting. However, they often struggle to strike a balance between expressive power and computational efficiency. Resorting to multi-layer perceptrons (MLPs) provides a compromising solution, yet they suffer from two critical problems caused by the intrinsic point-wise mapping mode, in terms of deficient contextual dependencies and inadequate information bottleneck. Here, we propose the Coarsened Perceptron Network (CP-Net), featured by a coarsening strategy that alleviates the above problems associated with the prototype MLPs by forming information granules in place of solitary temporal points. The CP-Net utilizes primarily a two-stage framework for extracting semantic and contextual patterns, which preserves correlations over larger timespans and filters out volatile noises. This is further enhanced by a multi-scale setting, where patterns of diverse granularities are fused towards a comprehensive prediction. Based purely on convolutions of structural simplicity, CP-Net is able to maintain a linear computational complexity and low runtime, while demonstrates an improvement of 4.1% compared with the SOTA method on seven forecasting benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03199">https://arxiv.org/abs/2405.03199</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it introduces a new deep learning method called Coarsened Perceptron Network (CP-Net) specifically for time series forecasting. The method attempts to improve balance between expressive power and computational efficiency, issues that are integral to your interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11124" target="_blank">AdaWaveNet: Adaptive Wavelet Network for Time Series Analysis</a></h3>
            <a href="https://arxiv.org/html/2405.11124v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11124v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Han Yu, Peikun Guo, Akane Sano</p>
            <p><strong>Summary:</strong> arXiv:2405.11124v1 Announce Type: new 
Abstract: Time series data analysis is a critical component in various domains such as finance, healthcare, and meteorology. Despite the progress in deep learning for time series analysis, there remains a challenge in addressing the non-stationary nature of time series data. Traditional models, which are built on the assumption of constant statistical properties over time, often struggle to capture the temporal dynamics in realistic time series, resulting in bias and error in time series analysis. This paper introduces the Adaptive Wavelet Network (AdaWaveNet), a novel approach that employs Adaptive Wavelet Transformation for multi-scale analysis of non-stationary time series data. AdaWaveNet designed a lifting scheme-based wavelet decomposition and construction mechanism for adaptive and learnable wavelet transforms, which offers enhanced flexibility and robustness in analysis. We conduct extensive experiments on 10 datasets across 3 different tasks, including forecasting, imputation, and a newly established super-resolution task. The evaluations demonstrate the effectiveness of AdaWaveNet over existing methods in all three tasks, which illustrates its potential in various real-world applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11124">https://arxiv.org/abs/2405.11124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to your interests since it introduces a new deep learning model, AdaWaveNet, for time series data analysis. While it might not be directly aimed at forecasting, the experiments carried out include predicting future data, and therefore it might provide valuable insights for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11238" target="_blank">SimAD: A Simple Dissimilarity-based Approach for Time Series Anomaly Detection</a></h3>
            <a href="https://arxiv.org/html/2405.11238v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11238v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhijie Zhong, Zhiwen Yu, Xing Xi, Yue Xu, Jiahui Chen, Kaixiang Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.11238v1 Announce Type: new 
Abstract: Despite the prevalence of reconstruction-based deep learning methods, time series anomaly detection remains challenging. Existing approaches often struggle with limited temporal contexts, inadequate representation of normal patterns, and flawed evaluation metrics, hindering their effectiveness in identifying aberrant behavior. To address these issues, we introduce $\textbf{{SimAD}}$, a $\textbf{{Sim}}$ple dissimilarity-based approach for time series $\textbf{{A}}$nomaly $\textbf{{D}}$etection. SimAD incorporates an advanced feature extractor adept at processing extended temporal windows, utilizes the EmbedPatch encoder to integrate normal behavioral patterns comprehensively, and introduces an innovative ContrastFusion module designed to accentuate distributional divergences between normal and abnormal data, thereby enhancing the robustness of anomaly discrimination. Additionally, we propose two robust evaluation metrics, UAff and NAff, addressing the limitations of existing metrics and demonstrating their reliability through theoretical and experimental analyses. Experiments across $\textbf{seven}$ diverse time series datasets demonstrate SimAD's superior performance compared to state-of-the-art methods, achieving relative improvements of $\textbf{19.85%}$ on F1, $\textbf{4.44%}$ on Aff-F1, $\textbf{77.79%}$ on NAff-F1, and $\textbf{9.69%}$ on AUC on six multivariate datasets. Code and pre-trained models are available at https://github.com/EmorZz1G/SimAD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11238">https://arxiv.org/abs/2405.11238</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The research paper introduces new deep learning method, SimAD, for time series anomaly detection which makes it relevant to your interest in new methods for forecasting using deep learning in time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11333" target="_blank">GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing</a></h3>
            <a href="https://arxiv.org/html/2405.11333v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11333v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Yongjun Xu</p>
            <p><strong>Summary:</strong> arXiv:2405.11333v1 Announce Type: new 
Abstract: Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90% of variables are missing, it can still accurately predict the future values of all variables.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11333">https://arxiv.org/abs/2405.11333</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper is relevant as it focuses on 'multivariate time series forecasting'. The method proposed (GinAR) is a new deep learning technique that involves interpolation attention and adaptive graph convolution. However, the paper is not specific to multimodal or transformer-like models for time series, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12038" target="_blank">Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.12038v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12038v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dandan Zhang, Yun Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.12038v1 Announce Type: new 
Abstract: Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics. However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant. To address this challenge, we introduce multi-resolution convolution and deformable convolution operations. By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables. Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series. Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series. The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series. We evaluated the performance of ATVCNet across eight real-world datasets. The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12038">https://arxiv.org/abs/2405.12038</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'New deep learning methods for time series'. It introduces a new model, ATVCNet, for time series forecasting that leverages multi-resolution and deformable convolutions to better understand temporal correlations. However, it might not directly address multimodal or transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12094" target="_blank">Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</a></h3>
            <a href="https://arxiv.org/html/2405.12094v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12094v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Shengchao Hu, Mengzhu Wang, Shouling Ji, Jincai Huang, Li Shen</p>
            <p><strong>Summary:</strong> arXiv:2405.12094v1 Announce Type: new 
Abstract: Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\% with 30\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12094">https://arxiv.org/abs/2405.12094</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Although the paper is primarily about reinforcement learning, it introduces a new transformer-like model called Decision Mamba for trajectory optimization in offline RL. So, it might be interesting for you because it proposes a new transformer-like model which is a subtopic of your interest in time series. However, it does not directly address time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11237" target="_blank">Lag Selection for Univariate Time Series Forecasting using Deep Learning: An Empirical Study</a></h3>
            <a href="https://arxiv.org/html/2405.11237v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11237v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jos\'e Leites, Vitor Cerqueira, Carlos Soares</p>
            <p><strong>Summary:</strong> arXiv:2405.11237v1 Announce Type: cross 
Abstract: Most forecasting methods use recent past observations (lags) to model the future values of univariate time series. Selecting an adequate number of lags is important for training accurate forecasting models. Several approaches and heuristics have been devised to solve this task. However, there is no consensus about what the best approach is. Besides, lag selection procedures have been developed based on local models and classical forecasting techniques such as ARIMA. We bridge this gap in the literature by carrying out an extensive empirical analysis of different lag selection methods. We focus on deep learning methods trained in a global approach, i.e., on datasets comprising multiple univariate time series. The experiments were carried out using three benchmark databases that contain a total of 2411 univariate time series. The results indicate that the lag size is a relevant parameter for accurate forecasts. In particular, excessively small or excessively large lag sizes have a considerable negative impact on forecasting performance. Cross-validation approaches show the best performance for lag selection, but this performance is comparable with simple heuristics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11237">https://arxiv.org/abs/2405.11237</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Although this paper does not present a new deep learning method or model, it investigates a crucial factor for improving the performance of existing models, i.e., the selection of lags for univariate time series forecasting using deep learning. Therefore, it indirectly contributes to new methods in this domain.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11795" target="_blank">Application of time-series quantum generative model to financial data</a></h3>
            <a href="https://arxiv.org/html/2405.11795v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11795v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shun Okumura, Masayuki Ohzeki, Masaya Abe</p>
            <p><strong>Summary:</strong> arXiv:2405.11795v1 Announce Type: cross 
Abstract: Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems. In this study, a time-series generative model was applied as a quantum generative model to actual financial data. Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression. Furthermore, numerical experiments were performed to complete missing values. Based on the results, we evaluated the practical applications of the time-series quantum generation model. It was observed that fewer parameter values were required compared with the classical method. In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data. These results suggest that several parameters can be applied to various types of time-series data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11795">https://arxiv.org/abs/2405.11795</a></p>
            <p><strong>Category:</strong> quant-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper seems relevant because it introduces a new way (quantum generative model) for time series analysis which you are interested in. However, it seems to be geared towards application in financial data more than presenting a completely new method, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.06937" target="_blank">Can a Transformer Represent a Kalman Filter?</a></h3>
            
            <p><strong>Authors:</strong> Gautam Goel, Peter Bartlett</p>
            <p><strong>Summary:</strong> arXiv:2312.06937v3 Announce Type: replace 
Abstract: Transformers are a class of autoregressive deep learning architectures which have recently achieved state-of-the-art performance in various vision, language, and robotics tasks. We revisit the problem of Kalman Filtering in linear dynamical systems and show that Transformers can approximate the Kalman Filter in a strong sense. Specifically, for any observable LTI system we construct an explicit causally-masked Transformer which implements the Kalman Filter, up to a small additive error which is bounded uniformly in time; we call our construction the Transformer Filter. Our construction is based on a two-step reduction. We first show that a softmax self-attention block can exactly represent a Nadaraya-Watson kernel smoothing estimator with a Gaussian kernel. We then show that this estimator closely approximates the Kalman Filter. We also investigate how the Transformer Filter can be used for measurement-feedback control and prove that the resulting nonlinear controllers closely approximate the performance of standard optimal control policies such as the LQG controller.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.06937">https://arxiv.org/abs/2312.06937</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper demonstrates a novel approach by using Transformers for time series forecasting, which aligns with your interest in 'New transformer-like models for time series'. It also correlates with 'New deep learning methods for time series'. However, it is more focused on applying existing methods (Kalman Filter, Nadaraya-Watson estimator, Transformers) in a novel way vs. proposing a completely new method.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.09793" target="_blank">PatchAD: A Lightweight Patch-based MLP-Mixer for Time Series Anomaly Detection</a></h3>
            <a href="https://arxiv.org/html/2401.09793v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.09793v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</p>
            <p><strong>Summary:</strong> arXiv:2401.09793v4 Announce Type: replace 
Abstract: Anomaly detection in time series analysis is a pivotal task, yet it poses the challenge of discerning normal and abnormal patterns in label-deficient scenarios. While prior studies have largely employed reconstruction-based approaches, which limits the models' representational capacities. Moreover, existing deep learning-based methods are not sufficiently lightweight. Addressing these issues, we present PatchAD, our novel, highly efficient multiscale patch-based MLP-Mixer architecture that utilizes contrastive learning for representation extraction and anomaly detection. With its four distinct MLP Mixers and innovative dual project constraint module, PatchAD mitigates potential model degradation and offers a lightweight solution, requiring only $3.2$MB. Its efficacy is demonstrated by state-of-the-art results across $9$ datasets sourced from different application scenarios, outperforming over $30$ comparative algorithms. PatchAD significantly improves the classical F1 score by $50.5\%$, the Aff-F1 score by $7.8\%$, and the AUC by $10.0\%$. The code is publicly available. \url{https://github.com/EmorZz1G/PatchAD}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.09793">https://arxiv.org/abs/2401.09793</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces PatchAD, a new lightweight deep learning model for time series anomaly detection. It doesn't directly align with your subtopics of forecasting, foundation models, datasets, multimodal models, or transformer-like models, but it could be of interest as it presents a deep learning approach for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11838" target="_blank">UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction</a></h3>
            <a href="https://arxiv.org/html/2402.11838v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11838v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li</p>
            <p><strong>Summary:</strong> arXiv:2402.11838v2 Announce Type: replace 
Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts that align and leverage intrinsic and shared knowledge across scenarios. These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability. Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in few-shot and zero-shot scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11838">https://arxiv.org/abs/2402.11838</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in new deep learning methods for time series forecasting. It introduces a model named UniST for urban spatio-temporal prediction, which can be relevant to foundation models for time series, thereby aligning with your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.12418" target="_blank">STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model</a></h3>
            <a href="https://arxiv.org/html/2403.12418v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.12418v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster</p>
            <p><strong>Summary:</strong> arXiv:2403.12418v4 Announce Type: replace 
Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Spatial-Temporal Selective State Space Module (ST-S3M) to precisely focus on the selected STG latent features. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of selective state space models, we propose Kalman Filtering Graph Neural Networks (KFGN) for dynamically integrate and upgrade the STG embeddings from different temporal granularities through a learnable Kalman Filtering statistical theory-based approach. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time. The implementation code is available at: \url{https://github.com/LincanLi98/STG-Mamba}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.12418">https://arxiv.org/abs/2403.12418</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper appears highly relevant to your interests as it presents a new model for spatial-temporal graph learning and forecasting. However, it may be slightly less relevant to the topic of deep learning, which lowers the score to a 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.06211" target="_blank">Structured state-space models are deep Wiener models</a></h3>
            <a href="https://arxiv.org/html/2312.06211v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.06211v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fabio Bonassi, Carl Andersson, Per Mattsson, Thomas B. Sch\"on</p>
            <p><strong>Summary:</strong> arXiv:2312.06211v2 Announce Type: replace-cross 
Abstract: The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.06211">https://arxiv.org/abs/2312.06211</a></p>
            <p><strong>Category:</strong> eess.SY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces Structured State-space Models (SSMs) which is a new learning model relevant to time series. It also discusses future research directions which could be useful for understanding new methods for time series.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11548" target="_blank">Adaptive Online Experimental Design for Causal Discovery</a></h3>
            <a href="https://arxiv.org/html/2405.11548v1/extracted/5603546/synthresL1.png" target="_blank"><img src="https://arxiv.org/html/2405.11548v1/extracted/5603546/synthresL1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</p>
            <p><strong>Summary:</strong> arXiv:2405.11548v1 Announce Type: new 
Abstract: Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11548">https://arxiv.org/abs/2405.11548</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'causality and machine learning'. It proposes a new method for causal discovery using an adaptive track-and-stop approach, which is pertinent to your specific interest in causal discovery. Moreover, it uses large datasets for learning causal graphs which may indirectly link it to your interest in 'using large language models in causal discovery'. However, the paper doesn't mention explicitly the use of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2206.07837" target="_blank">Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization</a></h3>
            <a href="https://arxiv.org/html/2206.07837v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2206.07837v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jivat Neet Kaur, Emre Kiciman, Amit Sharma</p>
            <p><strong>Summary:</strong> arXiv:2206.07837v4 Announce Type: replace 
Abstract: Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2206.07837">https://arxiv.org/abs/2206.07837</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper introduces Causally Adaptive Constraint Minimization (CACM), a new method that makes use of knowledge about the data-generating process for domain generalization. Though not directly related to large language models, it does discuss causality, therefore it's relevant to your interest in causal representation learning and causal discovery.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 21, 2024 at 21:45:55</div></body></html>