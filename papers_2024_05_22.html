
            <html>
            <head>
                <title>Report Generated on May 22, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 22, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12264" target="_blank">Directed Metric Structures arising in Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> St\'ephane Gaubert, Yiannis Vlassopoulos</p>
            <p><strong>Summary:</strong> arXiv:2405.12264v1 Announce Type: new 
Abstract: Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text. In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions. Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts $\mathcal{L}$, by -log probabilities. We then construct a metric polyhedron $P(\mathcal{L})$ and an isometric embedding (called Yoneda embedding) of $\mathcal{L}$ into $P(\mathcal{L})$ such that texts map to generators of certain special extremal rays. We explain that $P(\mathcal{L})$ is a $(\min,+)$ (tropical) linear span of these extremal ray generators. The generators also satisfy a system of $(\min+)$ linear equations. We then show that $P(\mathcal{L})$ is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text. We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different). Moreover we prove that $P(\mathcal{L})$ is the lattice closure of (a version of) the so called, Isbell completion of $\mathcal{L}$ which turns out to be the $(\max,+)$ span of the text extremal ray generators. All constructions have interpretations in category theory but we don't use category theory explicitly. The categorical interpretations are briefly explained in an appendix. In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12264">https://arxiv.org/abs/2405.12264</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper offers a deep analysis of Large Language Models through the lens of mathematical structures, likely contributing relevant background for you. However, it does not explicitly discuss controlling software or web browsers with LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12739" target="_blank">SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling</a></h3>
            
            <p><strong>Authors:</strong> Xingzhou Lou, Junge Zhang, Jian Xie, Lifeng Liu, Dong Yan, Kaiqi Huang</p>
            <p><strong>Summary:</strong> arXiv:2405.12739v1 Announce Type: new 
Abstract: Human preference alignment is critical in building powerful and reliable large language models (LLMs). However, current methods either ignore the multi-dimensionality of human preferences (e.g. helpfulness and harmlessness) or struggle with the complexity of managing multiple reward models. To address these issues, we propose Sequential Preference Optimization (SPO), a method that sequentially fine-tunes LLMs to align with multiple dimensions of human preferences. SPO avoids explicit reward modeling, directly optimizing the models to align with nuanced human preferences. We theoretically derive closed-form optimal SPO policy and loss function. Gradient analysis is conducted to show how SPO manages to fine-tune the LLMs while maintaining alignment on previously optimized dimensions. Empirical results on LLMs of different size and multiple evaluation datasets demonstrate that SPO successfully aligns LLMs across multiple dimensions of human preferences and significantly outperforms the baselines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12739">https://arxiv.org/abs/2405.12739</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses Sequential Preference Optimization (SPO), a method to fine-tune Large Language Models (LLMs). Though not explicitly for controlling software or web browsers, it's highly relevant for the integration of LLMs into agent-based systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12522" target="_blank">Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models</a></h3>
            
            <p><strong>Authors:</strong> Charles O'Neill, Thang Bui</p>
            <p><strong>Summary:</strong> arXiv:2405.12522v1 Announce Type: cross 
Abstract: This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12522">https://arxiv.org/abs/2405.12522</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the identification of computations done by large language models, which could be directly tied to the interaction and control of these models in actions like computer automation or software control. Despite the lack of explicit mention of the mentioned subtopics, the methods described could be a foundational technique in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12856" target="_blank">LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</a></h3>
            
            <p><strong>Authors:</strong> James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud</p>
            <p><strong>Summary:</strong> arXiv:2405.12856v1 Announce Type: cross 
Abstract: Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12856">https://arxiv.org/abs/2405.12856</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though this paper does not directly discuss controlling software or web browsers using LLMs, it provides insights into using LLMs with numerical data for predicting at arbitrary locations. This approach could be an essential foundation to understand how we can use LLM for computer automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12946" target="_blank">Tutorly: Turning Programming Videos Into Apprenticeship Learning Environments with LLMs</a></h3>
            
            <p><strong>Authors:</strong> Wengxi Li, Roy Pea, Nick Haber, Hari Subramonyam</p>
            <p><strong>Summary:</strong> arXiv:2405.12946v1 Announce Type: cross 
Abstract: Online programming videos, including tutorials and streamcasts, are widely popular and contain a wealth of expert knowledge. However, effectively utilizing these resources to achieve targeted learning goals can be challenging. Unlike direct tutoring, video content lacks tailored guidance based on individual learning paces, personalized feedback, and interactive engagement necessary for support and monitoring. Our work transforms programming videos into one-on-one tutoring experiences using the cognitive apprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allows learners to (1) set personalized learning goals, (2) engage in learning-by-doing through a conversational LLM-based mentor agent, (3) receive guidance and feedback based on a student model that steers the mentor moves. In a within-subject study with 16 participants learning exploratory data analysis from a streamcast, Tutorly significantly improved their performance from 61.9% to 76.6% based on a post-test questionnaire. Tutorly demonstrates the potential for enhancing programming video learning experiences with LLM and learner modeling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12946">https://arxiv.org/abs/2405.12946</a></p>
            <p><strong>Category:</strong> cs.HC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in the 'llm-agents' category. It discusses using a large language model in a tutoring context, which might serve as an interesting example for your interest in computer automation using large language models. However, its focus is on programming tutorials rather than direct control of software or browsers, so it might not fully align with your specified subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.16458" target="_blank">BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein</p>
            <p><strong>Summary:</strong> arXiv:2308.16458v5 Announce Type: replace 
Abstract: Pre-trained large language models (LLMs) have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code. BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate various models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT- 4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our training dataset can enhance the performance on our testing benchmark (by >15% in terms of Pass@K under certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> 2,600 tokens) with full context, including functional dependencies. (2) They contain domain-specific knowledge of bioinformatics, beyond just general coding capability. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on our benchmark (50% vs. up to 25%). Availability and implementation: Code is available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark. github.io/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.16458">https://arxiv.org/abs/2308.16458</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it does not directly discuss controlling software or web browsers with large language models, the paper touches on the topic of large language models generating domain-specific code, which is relevant to your interest in the broader application of these models in computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.18018" target="_blank">On Prompt-Driven Safeguarding for Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng</p>
            <p><strong>Summary:</strong> arXiv:2401.18018v3 Announce Type: replace 
Abstract: Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a "higher-refusal" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.18018">https://arxiv.org/abs/2401.18018</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is mainly focusing on the use of large language models and the safe application of these models, specifically in response to different types of queries. Though it doesn't directly address control of software, browsers, or automation, it provides valuable insights about safety prompt optimization that can improve interaction with large language models in those use cases.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08086" target="_blank">Text-centric Alignment for Multi-Modality Learning</a></h3>
            
            <p><strong>Authors:</strong> Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin</p>
            <p><strong>Summary:</strong> arXiv:2402.08086v2 Announce Type: replace 
Abstract: This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08086">https://arxiv.org/abs/2402.08086</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it employs Large Language Models with in-context learning for enhancing multimodal systems' generalizability. The approach named 'TAMML' shares direct overlap with your interests in large language models used in diverse applications. However, it does not explicitly address control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08707" target="_blank">Large Language Model Can Continue Evolving From Mistakes</a></h3>
            
            <p><strong>Authors:</strong> Haokun Zhao, Haixia Han, Jie Shi, Chengyu Du, Jiaqing Liang, Yanghua Xiao</p>
            <p><strong>Summary:</strong> arXiv:2404.08707v3 Announce Type: replace 
Abstract: As world knowledge evolves and new task paradigms emerge, Large Language Models (LLMs) often fall short of meeting new demands due to knowledge deficiencies and outdated information. Continual Learning (CL) is crucial for keeping LLMs up-to-date and addressing these deficiencies. However, traditional CL approaches struggle to balance task-width generality with task-depth specificity and often lack efficient data collection strategies, leading to increased training costs without addressing the model's most critical needs.
  Inspired by the `summarizing mistakes' learning skill, we propose the Continue Evolving from Mistakes (CEM) method. This iterative approach continually evaluates LLMs to identify knowledge deficiencies based on their mistakes, collecting relevant data from multiple sources to supplement training in a targeted manner. To enhance the model's utilization of supplemental knowledge and prevent forgetting, we developed three dataset construction strategies that integrate various types of continual pretraining (CPT) data and continual instruction tuning (CIT) data. Extensive experiments demonstrate the efficacy of the CEM method, achieving up to a 17% improvement in LLM accuracy in the best scenarios. Additionally, further experiments confirm the potential of combining CEM with other catastrophic forgetting mitigation strategies, enabling multi-round iterative optimization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08707">https://arxiv.org/abs/2404.08707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the evolution and improvement of large language models, which is relevant to your interest in agents based on large language models. It doesn't specifically mention controlling software or web browsers, but the methodologies proposed can potentially be applied in these areas. Thus, it could provide a useful starting point.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.09656" target="_blank">Learn Your Reference Model for Real Good Alignment</a></h3>
            
            <p><strong>Authors:</strong> Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov</p>
            <p><strong>Summary:</strong> arXiv:2404.09656v2 Announce Type: replace 
Abstract: The complexity of the alignment problem stems from the fact that existing methods are considered unstable. Reinforcement Learning from Human Feedback (RLHF) addresses this issue by minimizing the KL divergence between the trained policy and the initial supervised fine-tuned policy (SFT) to avoid generating out-of-domain samples for the reward model (RM). Recently, many methods have emerged that shift from online to offline optimization, reformulating the RLHF objective and removing the reward model (DPO, IPO, KTO). Despite eliminating the reward model and the challenges it posed, these algorithms are still constrained in terms of closeness of the trained policy to the SFT one. In our paper, we argue that this implicit limitation in the offline optimization methods leads to suboptimal results. To address this issue, we propose a class of new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update the reference policy during training. With this straightforward update approach, we demonstrate the effectiveness of the new paradigm of language model alignment against the classical one on the Anthropic-HH and Reddit TL;DR datasets. Most notably, when automatically comparing TR methods and baselines side by side using pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference in win rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO. Finally, by assessing model response ratings grounded on criteria such as coherence, correctness, helpfulness, and harmlessness, we demonstrate that our proposed methods significantly outperform existing techniques.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.09656">https://arxiv.org/abs/2404.09656</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your 'LLM-agents' category. It discusses new methods akin to large language models to control software, specifically focusing on language model alignment. Although it does not directly address web browser control or automation, it could have broader implications for these sub-topics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15993" target="_blank">Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach</a></h3>
            
            <p><strong>Authors:</strong> Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen</p>
            <p><strong>Summary:</strong> arXiv:2404.15993v2 Announce Type: replace 
Abstract: In this paper, we study the problem of uncertainty estimation and calibration for LLMs. We first formulate the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden neurons of the LLMs may contain uncertainty information. Our designed approach demonstrates the benefits of utilizing hidden activations to enhance uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. We distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. Furthermore, our method is easy to implement and adaptable to different levels of model accessibility including black box, grey box, and white box.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15993">https://arxiv.org/abs/2404.15993</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it does not specifically address controlling software or web browser with LLMs, this paper does focus on improving the performance of large language models, a topic you mentioned interest in. Specifically, it explores uncertainty estimation, which could be relevant for functional applications of LLMs in your mentioned subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.05465" target="_blank">Vidur: A Large-Scale Simulation Framework For LLM Inference</a></h3>
            
            <p><strong>Authors:</strong> Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov</p>
            <p><strong>Summary:</strong> arXiv:2405.05465v2 Announce Type: replace 
Abstract: Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.05465">https://arxiv.org/abs/2405.05465</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper covers the optimization of Large Language Models (LLMs) for different workloads which can be beneficial for software control and automation using LLMs. However, it does not directly discuss the control of software or web browsers by LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11344" target="_blank">Improved Content Understanding With Effective Use of Multi-task Contrastive Learning</a></h3>
            
            <p><strong>Authors:</strong> Akanksha Bindal, Sudarshan Ramanujam, Dave Golland, TJ Hazen, Tina Jiang, Fengyu Zhang, Peng Yan</p>
            <p><strong>Summary:</strong> arXiv:2405.11344v2 Announce Type: replace 
Abstract: In enhancing LinkedIn core content recommendation models, a significant challenge lies in improving their semantic understanding capabilities. This paper addresses the problem by leveraging multi-task learning, a method that has shown promise in various domains. We fine-tune a pre-trained, transformer-based LLM using multi-task contrastive learning with data from a diverse set of semantic labeling tasks. We observe positive transfer, leading to superior performance across all tasks when compared to training independently on each. Our model outperforms the baseline on zero shot learning and offers improved multilingual support, highlighting its potential for broader application. The specialized content embeddings produced by our model outperform generalized embeddings offered by OpenAI on Linkedin dataset and tasks. This work provides a robust foundation for vertical teams across LinkedIn to customize and fine-tune the LLM to their specific applications. Our work offers insights and best practices for the field to build on.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11344">https://arxiv.org/abs/2405.11344</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Even though this paper does not directly cover a large language model controlling software or a web browser, it speaks about using multi-task contrastive learning to fine-tune a transformer-based large language model (LLM). This technique could potentially be employed in further automation and control applications using LLMs. Therefore, it is judged to have high relevance to the broader topic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.07887" target="_blank">Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models</a></h3>
            
            <p><strong>Authors:</strong> Junhao Zheng, Shengjie Qiu, Qianli Ma</p>
            <p><strong>Summary:</strong> arXiv:2312.07887v3 Announce Type: replace-cross 
Abstract: Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods and requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.07887">https://arxiv.org/abs/2312.07887</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research paper appears to focus on the use of Pre-trained Language Models (PLMs), which are large language models, for Incremental Learning (IL) tasks. Although it doesn't explicitly mention controlling software or web browsers, it provides insights on a new method for using large language models in a learning context. It may still be very relevant to your interest in agents based on large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.01854" target="_blank">Multilingual Instruction Tuning With Just a Pinch of Multilinguality</a></h3>
            
            <p><strong>Authors:</strong> Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal</p>
            <p><strong>Summary:</strong> arXiv:2401.01854v4 Announce Type: replace-cross 
Abstract: As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages from the pre-training corpus. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples integrated in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in multiple languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that diversifying the instruction tuning set with even just 2-4 languages significantly improves cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.01854">https://arxiv.org/abs/2401.01854</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is very much relevant to your interests in large language models particularly with regards to multilanguage capabilities. It does not specifically mention the control of software or web browsers, however the discussion on instruction tuning can be highly important for such applications, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08277" target="_blank">Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</a></h3>
            
            <p><strong>Authors:</strong> Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</p>
            <p><strong>Summary:</strong> arXiv:2402.08277v4 Announce Type: replace-cross 
Abstract: Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08277">https://arxiv.org/abs/2402.08277</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper describes a way to fine-tune Large Language Models for better source quality and answer attributability, which falls in line with your interest in automation using LLMs. However, the paper does not specifically address controlling software or web browsers, hence the score is not a full 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.14901" target="_blank">Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice</a></h3>
            
            <p><strong>Authors:</strong> Ranim Khojah, Mazen Mohamad, Philipp Leitner, Francisco Gomes de Oliveira Neto</p>
            <p><strong>Summary:</strong> arXiv:2404.14901v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that, rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.14901">https://arxiv.org/abs/2404.14901</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is on the usage of large language models like ChatGPT in software engineering, which aligns with your interest on how large language models can be used to control software. While it does not focus on proposing new methods, it offers valuable insights into the practical application and usefulness of these models in an industry setting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2205.09235" target="_blank">GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints</a></h3>
            
            <p><strong>Authors:</strong> Mohammadsajad Abavisani, David Danks, Sergey Plis</p>
            <p><strong>Summary:</strong> arXiv:2205.09235v4 Announce Type: replace-cross 
Abstract: Graphical structures estimated by causal learning algorithms from time series data can provide misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might 1) require that the difference between causal and measurement timescales is known; 2) only handle very small number of random variables when the timescale difference is unknown; 3) only apply to pairs of variables; or 4) be unable to find a solution given statistical noise in the data. This research addresses these challenges. Our approach combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve multiple orders of magnitude in speed-up. The resulting system maintains theoretical guarantees while scaling to significantly larger sets of random variables (>100) without knowledge of timescale differences. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal solution among many possible ones.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2205.09235">https://arxiv.org/abs/2205.09235</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it covers causal discovery, creating a new method to estimate causal structures from time series data. The paper claims it works even when causal and measurement timescales do not match, which can be an improvement over other causal learning algorithms.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12340" target="_blank">Cascade-based Randomization for Inferring Causal Effects under Diffusion Interference</a></h3>
            
            <p><strong>Authors:</strong> Zahra Fatemi, Jean Pouget-Abadie, Elena Zheleva</p>
            <p><strong>Summary:</strong> arXiv:2405.12340v1 Announce Type: new 
Abstract: The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation. Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control. However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors. When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias. With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error. Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12340">https://arxiv.org/abs/2405.12340</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper delves into the field of causal inference, particularly focusing on estimating causal effects under the influence of neighboring nodes. Although it does not specifically include large language models for causal discovery, it could provide valuable insights for your interest in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12387" target="_blank">Conformal Counterfactual Inference under Hidden Confounding</a></h3>
            
            <p><strong>Authors:</strong> Zonghao Chen, Ruocheng Guo, Jean-Fran\c{c}ois Ton, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.12387v1 Announce Type: new 
Abstract: Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. To overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12387">https://arxiv.org/abs/2405.12387</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in causality and machine learning as it discusses a novel approach for predicting potential outcomes, a fundamental challenge in causal inference. It tackles methods of counterfactual inference, which relates to 'Causal Representation Learning' and 'Causal Discovery' subtopics you specified. However, it doesn't specifically mention large language models involved in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12615" target="_blank">Learning Causal Dynamics Models in Object-Oriented Environments</a></h3>
            
            <p><strong>Authors:</strong> Zhongwei Yu, Jingqing Ruan, Dengpeng Xing</p>
            <p><strong>Summary:</strong> arXiv:2405.12615v1 Announce Type: new 
Abstract: Causal dynamics models (CDMs) have demonstrated significant potential in addressing various challenges in reinforcement learning. To learn CDMs, recent studies have performed causal discovery to capture the causal dependencies among environmental variables. However, the learning of CDMs is still confined to small-scale environments due to computational complexity and sample efficiency constraints. This paper aims to extend CDMs to large-scale object-oriented environments, which consist of a multitude of objects classified into different categories. We introduce the Object-Oriented CDM (OOCDM) that shares causalities and parameters among objects belonging to the same class. Furthermore, we propose a learning method for OOCDM that enables it to adapt to a varying number of objects. Experiments on large-scale tasks indicate that OOCDM outperforms existing CDMs in terms of causal discovery, prediction accuracy, generalization, and computational efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12615">https://arxiv.org/abs/2405.12615</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to your interests. It discusses causal discovery and introduces Object-Oriented Causal dynamics models (OOCDMs), demonstrating a new method for causal representation learning. While it does not specifically mention large language models, the causal learning techniques proposed might be beneficial to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08845" target="_blank">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</a></h3>
            
            <p><strong>Authors:</strong> Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</p>
            <p><strong>Summary:</strong> arXiv:2402.08845v2 Announce Type: replace 
Abstract: We investigate the problem of explainability in machine learning. To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations. However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation. In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the Probability of Necessity and Sufficiency (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance. Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional). In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Our source code is available at \url{https://github.com/DMIRLAB-Group/FANS}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08845">https://arxiv.org/abs/2402.08845</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning as it discusses enhancing Feature Attribution Methods (FAMs) for causality. However, the relevance isn't exclusive to causal discovery or the use of large language models in causal discovery, hence the score of 4.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12462" target="_blank">Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Zhicheng Zhang, Yong Wang, Shaoqi Tan, Bowei Xia, Yujie Luo</p>
            <p><strong>Summary:</strong> arXiv:2405.12462v1 Announce Type: new 
Abstract: Transformer-based models for long sequence time series forecasting (LSTF) problems have gained significant attention due to their exceptional forecasting precision. As the cornerstone of these models, the self-attention mechanism poses a challenge to efficient training and inference due to its quadratic time complexity. In this article, we propose a novel architectural design for Transformer-based models in LSTF, leveraging a substitution framework that incorporates Surrogate Attention Blocks and Surrogate FFN Blocks. The framework aims to boost any well-designed model's efficiency without sacrificing its accuracy. We further establish the equivalence of the Surrogate Attention Block to the self-attention mechanism in terms of both expressiveness and trainability. Through extensive experiments encompassing nine Transformer-based models across five time series tasks, we observe an average performance improvement of 9.45% while achieving a significant reduction in model size by 46%</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12462">https://arxiv.org/abs/2405.12462</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in 'new transformer-like models for time series'. It presents a novel design for Transformer-based models used in long sequence time series forecasting. They have leveraged a substitution framework that incorporates Surrogate Attention Blocks and Surrogate FFN Blocks, increasing efficiency without sacrificing accuracy, which goes along with your interest in new efficient models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.03589" target="_blank">TimeGPT-1</a></h3>
            
            <p><strong>Authors:</strong> Azul Garza, Cristian Challu, Max Mergenthaler-Canseco</p>
            <p><strong>Summary:</strong> arXiv:2310.03589v2 Announce Type: replace 
Abstract: In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.03589">https://arxiv.org/abs/2310.03589</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces TimeGPT, the first foundation model for time series, demonstrating application of deep learning methods to time series, thus aligning with your interests in new deep learning methods and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.06293" target="_blank">Probabilistic Forecasting of Irregular Time Series via Conditional Flows</a></h3>
            
            <p><strong>Authors:</strong> Vijaya Krishna Yalavarthi, Randolf Scholz, Stefan Born, Lars Schmidt-Thieme</p>
            <p><strong>Summary:</strong> arXiv:2402.06293v2 Announce Type: replace 
Abstract: Probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. State-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. In this work, we propose a novel model, ProFITi, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. The model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. As model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. We conduct extensive experiments on four datasets and demonstrate that the proposed model provides $4$ times higher likelihood over the previously best model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.06293">https://arxiv.org/abs/2402.06293</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interests in 'time-series', as it proposes a new deep learning model, ProFITi, for forecasting irregularly sampled multivariate time series, which makes it relevant to your subtopics 'new deep learning methods for time series' and 'new foundation models for time series'. Furthermore, the model also deals with missing values, which is a challenge in real-world time series data, providing a novel approach for consideration.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12234" target="_blank">Joint Prediction Regions for time-series models</a></h3>
            
            <p><strong>Authors:</strong> Eshant English, Nicola Paoletti</p>
            <p><strong>Summary:</strong> arXiv:2405.12234v1 Announce Type: cross 
Abstract: Machine Learning algorithms are notorious for providing point predictions but not prediction intervals. There are many applications where one requires confidence in predictions and prediction intervals. Stringing together, these intervals give rise to joint prediction regions with the desired significance level. It is an easy task to compute Joint Prediction regions (JPR) when the data is IID. However, the task becomes overly difficult when JPR is needed for time series because of the dependence between the observations. This project aims to implement Wolf and Wunderli's method for constructing JPRs and compare it with other methods (e.g. NP heuristic, Joint Marginals). The method under study is based on bootstrapping and is applied to different datasets (Min Temp, Sunspots), using different predictors (e.g. ARIMA and LSTM). One challenge of applying the method under study is to derive prediction standard errors for models, it cannot be obtained analytically. A novel method to estimate prediction standard error for different predictors is also devised. Finally, the method is applied to a synthetic dataset to find empirical averages and empirical widths and the results from the Wolf and Wunderli paper are consolidated. The experimental results show a narrowing of width with strong predictors like neural nets, widening of width with increasing forecast horizon H and decreasing significance level alpha, controlling the width with parameter k in K-FWE, and loss of information using Joint Marginals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12234">https://arxiv.org/abs/2405.12234</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses new time-series forecasting methods which related to 'time-series' tag of your interest. However, it does not exactly mention deep learning approaches or foundation models as stated in your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12456" target="_blank">Mutual Information Analysis in Multimodal Learning Systems</a></h3>
            
            <p><strong>Authors:</strong> Hadi Hadizadeh, S. Faegheh Yeganli, Bahador Rashidi, Ivan V. Baji\'c</p>
            <p><strong>Summary:</strong> arXiv:2405.12456v1 Announce Type: cross 
Abstract: In recent years, there has been a significant increase in applications of multimodal signal processing and analysis, largely driven by the increased availability of multimodal datasets and the rapid progress in multimodal learning systems. Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on. Such systems integrate multiple signal modalities: text, speech, images, video, LiDAR, etc., to perform various tasks. A key issue for understanding such systems is the relationship between various modalities and how it impacts task performance. In this paper, we employ the concept of mutual information (MI) to gain insight into this issue. Taking advantage of the recent progress in entropy modeling and estimation, we develop a system called InfoMeter to estimate MI between modalities in a multimodal learning system. We then apply InfoMeter to analyze a multimodal 3D object detection system over a large-scale dataset for autonomous driving. Our experiments on this system suggest that a lower MI between modalities is beneficial for detection accuracy. This new insight may facilitate improvements in the development of future multimodal learning systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12456">https://arxiv.org/abs/2405.12456</a></p>
            <p><strong>Category:</strong> eess.IV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses multimodal learning systems that are relevant to your interest in new multimodal deep learning models for time series. However, it is not specifically focused on time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14989" target="_blank">Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data</a></h3>
            
            <p><strong>Authors:</strong> YongKyung Oh, Dongyoung Lim, Sungil Kim</p>
            <p><strong>Summary:</strong> arXiv:2402.14989v3 Announce Type: replace 
Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14989">https://arxiv.org/abs/2402.14989</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper introduces new stable classes of Neural Stochastic Differential Equations for handling irregular time series data, which relates to your interest in 'New deep learning methods for time series' and 'New foundation models for time series'. However, it doesn't cover new transformer-like models or multimodal deep learning models for time series.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 22, 2024 at 21:36:56</div></body></html>