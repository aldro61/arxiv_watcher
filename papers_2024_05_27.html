
            <html>
            <head>
                <title>Report Generated on May 27, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 27, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14918" target="_blank">AnalogCoder: Analog Circuit Design via Training-Free Code Generation</a></h3>
            
            <p><strong>Authors:</strong> Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z. Pan, Ping Luo</p>
            <p><strong>Summary:</strong> arXiv:2405.14918v1 Announce Type: new 
Abstract: Analog circuit design is a significant task in modern chip technology, focusing on the selection of component types, connectivity, and parameters to ensure proper circuit functionality. Despite advances made by Large Language Models (LLMs) in digital circuit design, the complexity and scarcity of data in analog circuitry pose significant challenges. To mitigate these issues, we introduce AnalogCoder, the first training-free LLM agent for designing analog circuits through Python code generation. Firstly, AnalogCoder incorporates a feedback-enhanced flow with tailored domain-specific prompts, enabling the automated and self-correcting design of analog circuits with a high success rate. Secondly, it proposes a circuit tool library to archive successful designs as reusable modular sub-circuits, simplifying composite circuit creation. Thirdly, extensive experiments on a benchmark designed to cover a wide range of analog circuit tasks show that AnalogCoder outperforms other LLM-based methods. It has successfully designed 20 circuits, 5 more than standard GPT-4o. We believe AnalogCoder can significantly improve the labor-intensive chip design process, enabling non-experts to design analog circuits efficiently. Codes and the benchmark are provided at https://github.com/anonyanalog/AnalogCoder.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14918">https://arxiv.org/abs/2405.14918</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the utilization of Large Language Models in analog circuit design, which indicates automation using these models. However, it focuses on analog circuit design, not strictly on controlling software or web browsers with LLMs, hence the score is 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14953" target="_blank">Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions</a></h3>
            <a href="https://arxiv.org/html/2405.14953v1/extracted/5616068/Figures/Mallows_link_approximation/sigmoid.png" target="_blank"><img src="https://arxiv.org/html/2405.14953v1/extracted/5616068/Figures/Mallows_link_approximation/sigmoid.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoxian Chen, Hanyang Zhao, Henry Lam, David Yao, Wenpin Tang</p>
            <p><strong>Summary:</strong> arXiv:2405.14953v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has recently emerged as a popular approach to improve reinforcement learning with human feedback (RLHF), leading to better techniques to fine-tune large language models (LLM). A weakness of DPO, however, lies in its lack of capability to characterize the diversity of human preferences. Inspired by Mallows' theory of preference ranking, we develop in this paper a new approach, the Mallows-DPO. A distinct feature of this approach is a dispersion index, which reflects the dispersion of human preference to prompts. We show that existing DPO models can be reduced to special cases of this dispersion index, thus unified with Mallows-DPO. More importantly, we demonstrate (empirically) how to use this dispersion index to enhance the performance of DPO in a broad array of benchmark tasks, from synthetic bandit selection to controllable generations and dialogues, while maintaining great generalization capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14953">https://arxiv.org/abs/2405.14953</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the focus of the paper is on improving the performance of LLM through a new approach (Mallows-DPO), it represents a novel method in the context of large language models. The described technique could potentially be used for controlling software or automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15115" target="_blank">Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification</a></h3>
            <a href="https://arxiv.org/html/2405.15115v1/extracted/5616625/fig/exp_1_L2_compare.png" target="_blank"><img src="https://arxiv.org/html/2405.15115v1/extracted/5616625/fig/exp_1_L2_compare.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shang Liu, Zhongze Cai, Guanting Chen, Xiaocheng Li</p>
            <p><strong>Summary:</strong> arXiv:2405.15115v1 Announce Type: new 
Abstract: Predicting simple function classes has been widely used as a testbed for developing theory and understanding of the trained Transformer's in-context learning (ICL) ability. In this paper, we revisit the training of Transformers on linear regression tasks, and different from all the existing literature, we consider a bi-objective prediction task of predicting both the conditional expectation $\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This additional uncertainty quantification objective provides a handle to (i) better design out-of-distribution experiments to distinguish ICL from in-weight learning (IWL) and (ii) make a better separation between the algorithms with and without using the prior information of the training distribution. Theoretically, we show that the trained Transformer reaches near Bayes-optimum, suggesting the usage of the information of the training distribution. Our method can be extended to other cases. Specifically, with the Transformer's context window $S$, we prove a generalization bound of $\tilde{\mathcal{O}}(\sqrt{\min\{S, T\}/(n T)})$ on $n$ tasks with sequences of length $T$, providing sharper analysis compared to previous results of $\tilde{\mathcal{O}}(\sqrt{1/n})$. Empirically, we illustrate that while the trained Transformer behaves as the Bayes-optimal solution as a natural consequence of supervised training in distribution, it does not necessarily perform a Bayesian inference when facing task shifts, in contrast to the \textit{equivalence} between these two proposed in many existing literature. We also demonstrate the trained Transformer's ICL ability over covariates shift and prompt-length shift and interpret them as a generalization over a meta distribution.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15115">https://arxiv.org/abs/2405.15115</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is relevant to your interest in large language models, specifically it discusses in-depth about the Transformer's in-context learning ability and generalization over different shifts. However, it doesn't directly discuss controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15116" target="_blank">Quantifying the Gain in Weak-to-Strong Generalization</a></h3>
            <a href="https://arxiv.org/html/2405.15116v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15116v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Moses Charikar, Chirag Pabbaraju, Kirankumar Shiragur</p>
            <p><strong>Summary:</strong> arXiv:2405.15116v1 Announce Type: new 
Abstract: Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. (2023) empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts -- a phenomenon they term weak-to-strong generalization.
  In this work, we present a theoretical framework for understanding weak-to-strong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the misfit error incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15116">https://arxiv.org/abs/2405.15116</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses significant aspects of large language models which can control and direct capabilities and treatments. It explores the topic of weak-to-strong generalization which could be applicable at an agent level.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15143" target="_blank">Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2405.15143v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15143v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cong Lu, Shengran Hu, Jeff Clune</p>
            <p><strong>Summary:</strong> arXiv:2405.15143v1 Announce Type: new 
Abstract: Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems, built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration, which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these heuristics with the intelligence and internalized human notions of interestingness captured by giant foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g. discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting and previously impossible opportunity to recognize and capitalize on serendipitous discoveries that cannot be predicted ahead of time. We evaluate IGE on a range of language-based tasks that require search and exploration. In Game of 24, a multistep mathematical reasoning problem, IGE reaches 100% success rate 70.8% faster than the best classic graph search baseline. Next, in BabyAI-Text, a challenging partially observable gridworld, IGE exceeds the previous SOTA with orders of magnitude fewer online samples. Finally, in TextWorld, we show the unique ability of IGE to succeed in settings requiring long-horizon exploration where prior SOTA FM agents like Reflexion completely fail. Overall, IGE combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15143">https://arxiv.org/abs/2405.15143</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not exactly fit your query about controlling specific applications, it does cover the topic of using large language models in creating more generally capable agents with impressive exploration capabilities. This paper might give you insights into how large language models can be used in control environments.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15194" target="_blank">Efficient Reinforcement Learning via Large Language Model-based Search</a></h3>
            <a href="https://arxiv.org/html/2405.15194v1/extracted/5616936/images/pipeline.png" target="_blank"><img src="https://arxiv.org/html/2405.15194v1/extracted/5616936/images/pipeline.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siddhant Bhambri, Amrita Bhattacharjee, Huan Liu, Subbarao Kambhampati</p>
            <p><strong>Summary:</strong> arXiv:2405.15194v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward domains, and the problem is pronounced if there are stochastic transitions. To improve the sample efficiency, reward shaping is a well-studied approach to introduce intrinsic rewards that can help the RL agent converge to an optimal policy faster. However, designing a useful reward shaping function specific to each problem is challenging, even for domain experts. They would either have to rely on task-specific domain knowledge or provide an expert demonstration independently for each task. Given, that Large Language Models (LLMs) have rapidly gained prominence across a magnitude of natural language tasks, we aim to answer the following question: Can we leverage LLMs to construct a reward shaping function that can boost the sample efficiency of an RL agent? In this work, we aim to leverage off-the-shelf LLMs to generate a guide policy by solving a simpler deterministic abstraction of the original problem that can then be used to construct the reward shaping function for the downstream RL agent. Given the ineffectiveness of directly prompting LLMs, we propose MEDIC: a framework that augments LLMs with a Model-based feEDback critIC, which verifies LLM-generated outputs, to generate a possibly sub-optimal but valid plan for the abstract problem. Our experiments across domains from the BabyAI environment suite show 1) the effectiveness of augmenting LLMs with MEDIC, 2) a significant improvement in the sample complexity of PPO and A2C-based RL agents when guided by our LLM-generated plan, and finally, 3) pave the direction for further explorations of how these models can be used to augment existing RL pipelines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15194">https://arxiv.org/abs/2405.15194</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper should interest you because it explores a new approach to utilizing Large Language Models (LLMs) in reinforcement learning (RL), towards computer automation objectives. Specific methods are proposed to generate a guided policy and improve sample efficiency in RL, aligning well with your interests in LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15282" target="_blank">Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation</a></h3>
            <a href="https://arxiv.org/html/2405.15282v1/extracted/5617029/figures/dig_existing_new.png" target="_blank"><img src="https://arxiv.org/html/2405.15282v1/extracted/5617029/figures/dig_existing_new.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Abhinav Jain, Swarat Chaudhuri, Thomas Reps, Chris Jermaine</p>
            <p><strong>Summary:</strong> arXiv:2405.15282v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) has become the standard for customising Foundation Models (FMs) to user-specific downstream tasks. However, typical PEFT methods require storing multiple task-specific adapters, creating scalability issues as these adapters must be housed and run at the FM server. Traditional prompt tuning offers a potential solution by customising them through task-specific input prefixes, but it under-performs compared to other PEFT methods like LoRA. To address this gap, we propose Low-Rank Prompt Adaptation (LOPA), a prompt-tuning-based approach that performs on par with state-of-the-art PEFT methods and full fine-tuning while being more parameter-efficient and not requiring a server-based adapter. LOPA generates soft prompts by balancing between sharing task-specific information across instances and customization for each instance. It uses a low-rank decomposition of the soft-prompt component encoded for each instance to achieve parameter efficiency. We provide a comprehensive evaluation on multiple natural language understanding and code generation and understanding tasks across a wide range of foundation models with varying sizes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15282">https://arxiv.org/abs/2405.15282</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> I recommend this paper as it refers to 'code generation and understanding tasks', which is relevant to your interest in using large language models for computer automation. Although it doesn't directly refer to controlling software or web browsers, the methods described could be applicable in these contexts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15512" target="_blank">ChatGPT Code Detection: Techniques for Uncovering the Source of Code</a></h3>
            <a href="https://arxiv.org/html/2405.15512v1/extracted/5617778/Figures/Code_sample_2.png" target="_blank"><img src="https://arxiv.org/html/2405.15512v1/extracted/5617778/Figures/Code_sample_2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Marc Oedingen, Raphael C. Engelhardt, Robin Denz, Maximilian Hammer, Wolfgang Konen</p>
            <p><strong>Summary:</strong> arXiv:2405.15512v1 Announce Type: new 
Abstract: In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most 85-88% accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15512">https://arxiv.org/abs/2405.15512</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper details how a large language model, specifically ChatGPT, can generate code, which aligns with your interest in computer automation with large language models. It might not propose a new method of using LLMs to control software, but the detection and classification techniques could be insightful.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15551" target="_blank">Thinking Forward: Memory-Efficient Federated Finetuning of Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.15551v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15551v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan</p>
            <p><strong>Summary:</strong> arXiv:2405.15551v1 Announce Type: new 
Abstract: Finetuning large language models (LLMs) in federated learning (FL) settings has become important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. This work introduces Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimates of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We theoretically show that the global gradients in Spry are unbiased estimates of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3$\times$ and achieves 5.2-13.5\% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory usage of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices. Source code is available at https://github.com/Astuary/Spry.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15551">https://arxiv.org/abs/2405.15551</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your research interest in large language models (LLMs). Its discussions on federated learning and fine-tuning LLMs, while not directly dealing with control software or web browsers, could provide valuable concepts about agent memory efficiency and gradient computation. But, since it does not introduce a new method for using LLMs to control software or web browsers directly, the relevance is moderate.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15586" target="_blank">DAGER: Exact Gradient Inversion for Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas M\"uller, Martin Vechev</p>
            <p><strong>Summary:</strong> arXiv:2405.15586v1 Announce Type: new 
Abstract: Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15586">https://arxiv.org/abs/2405.15586</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is related to large language models (LLMs) which is a direct match for your interest in 'Agents based on large-language models'. However, it primarily discusses security and privacy issues related to LLMs rather than directly addressing their use for controlling software or for automation. Still, it will likely enhance your understanding of the challenges and considerations related to your area of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15589" target="_blank">Efficient Adversarial Training in LLMs with Continuous Attacks</a></h3>
            <a href="https://arxiv.org/html/2405.15589v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15589v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sophie Xhonneux, Alessandro Sordoni, Stephan G\"unnemann, Gauthier Gidel, Leo Schwinn</p>
            <p><strong>Summary:</strong> arXiv:2405.15589v1 Announce Type: new 
Abstract: Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15589">https://arxiv.org/abs/2405.15589</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines a method to improve robustness and alignment of Large Language Models, which is relevant to your interest in LLM-based computer automation and control. The paper's relevance is particularly in the context of system robustness.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15624" target="_blank">Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.15624v1/extracted/5618696/figs/fig_1_0521.png" target="_blank"><img src="https://arxiv.org/html/2405.15624v1/extracted/5618696/figs/fig_1_0521.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hao Sun, Mihaela van der Schaar</p>
            <p><strong>Summary:</strong> arXiv:2405.15624v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. In this work, we introduce Alignment from Demonstrations (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD. Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior. Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15624">https://arxiv.org/abs/2405.15624</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it explores the usage of Large-Language Models (LLMs) within a sequential decision-making framework, which can be related to controlling software. However, it does not address all subtopics of your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15756" target="_blank">Sparse Expansion and Neuronal Disentanglement</a></h3>
            <a href="https://arxiv.org/html/2405.15756v1/extracted/5619006/sections/figures/clustering_saves_llama.png" target="_blank"><img src="https://arxiv.org/html/2405.15756v1/extracted/5619006/sections/figures/clustering_saves_llama.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shashata Sawmya, Linghao Kong, Ilia Markov, Dan Alistarh, Nir Shavit</p>
            <p><strong>Summary:</strong> arXiv:2405.15756v1 Announce Type: new 
Abstract: We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values. We call this approach $\textit{Sparse Expansion}$. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups.
  But why? To answer this, we provide strong evidence that the mixture of sparse experts is effectively $\textit{disentangling}$ the input-output relationship of every individual neuron across clusters of inputs. Specifically, sparse experts approximate the dense neuron output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it. Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model. Every layer of an LLM has a fraction of highly entangled Wasserstein neurons, and model performance suffers more when these are sparsified as opposed to others.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15756">https://arxiv.org/abs/2405.15756</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large-language models as it discusses the improvement of the inference efficiency of LLMs through a method called Sparse Expansion.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14899" target="_blank">DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning</a></h3>
            
            <p><strong>Authors:</strong> Zijian Zhou, Xiaoqiang Lin, Xinyi Xu, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low</p>
            <p><strong>Summary:</strong> arXiv:2405.14899v1 Announce Type: cross 
Abstract: In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few "task demonstrations" without updating their parameters, significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning, thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results, we then show how DETAIL can help improve model performance in real-world scenarios through demonstration reordering and curation. Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14899">https://arxiv.org/abs/2405.14899</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might interest you because it delves deep into the workings of transformer-based language models and their application in specific tasks without updating their parameters, a characteristic of using large language models for controlling agents. However, it does not explicitly cover controlling software or web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14992" target="_blank">Linking In-context Learning in Transformers to Human Episodic Memory</a></h3>
            <a href="https://arxiv.org/html/2405.14992v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14992v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Li Ji-An, Corey Y. Zhou, Marcus K. Benna, Marcelo G. Mattar</p>
            <p><strong>Summary:</strong> arXiv:2405.14992v1 Announce Type: cross 
Abstract: Understanding the connections between artificial and biological intelligent systems can reveal fundamental principles underlying general intelligence. While many artificial intelligence (AI) models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between attention heads and human episodic memory. We focus on the induction heads, which contribute to the in-context learning capabilities of Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate model layers and that their behavior qualitatively mirrors the memory biases seen in humans. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14992">https://arxiv.org/abs/2405.14992</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper investigates the behaviours of large language models (LLMs) and their computational mechanisms related to human memory processes - such insights could be applicable to the control and automation of software using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15007" target="_blank">RE-Adapt: Reverse Engineered Adaptation of Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> William Fleshman, Benjamin Van Durme</p>
            <p><strong>Summary:</strong> arXiv:2405.15007v1 Announce Type: cross 
Abstract: We introduce RE-Adapt, an approach to fine-tuning large language models on new domains without degrading any pre-existing instruction-tuning. We reverse engineer an adapter which isolates what an instruction-tuned model has learned beyond its corresponding pretrained base model. Importantly, this requires no additional data or training. We can then fine-tune the base model on a new domain and readapt it to instruction following with the reverse engineered adapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other methods of fine-tuning, across multiple popular LLMs and datasets, even when the models are used in conjunction with retrieval-augmented generation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15007">https://arxiv.org/abs/2405.15007</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses fine-tuning of large language models, which could be potentially used in controlling software or automating computers effectively. However, it does not directly tackle web browsers control or computer automation, which lowers its score slightly.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15012" target="_blank">Extracting Prompts by Inverting LLM Outputs</a></h3>
            
            <p><strong>Authors:</strong> Collin Zhang, John X. Morris, Vitaly Shmatikov</p>
            <p><strong>Summary:</strong> arXiv:2405.15012v1 Announce Type: cross 
Abstract: We consider the problem of language model inversion: given outputs of a language model, we seek to extract the prompt that generated these outputs. We develop a new black-box method, output2prompt, that learns to extract prompts without access to the model's logits and without adversarial or jailbreaking queries. In contrast to previous work, output2prompt only needs outputs of normal user queries. To improve memory efficiency, output2prompt employs a new sparse encoding techique. We measure the efficacy of output2prompt on a variety of user and system prompts and demonstrate zero-shot transferability across different LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15012">https://arxiv.org/abs/2405.15012</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it discusses a method to control large language models by inverting their outputs. It presents a new technique, named output2prompt, for extracting prompts without accessing the language model's logits. This paper does not focus on controlling software or web browsers, hence it doesn't score 5, but it still provides valuable insight into large language model manipulation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15019" target="_blank">Agentic Skill Discovery</a></h3>
            <a href="https://arxiv.org/html/2405.15019v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15019v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xufeng Zhao, Cornelius Weber, Stefan Wermter</p>
            <p><strong>Summary:</strong> arXiv:2405.15019v1 Announce Type: cross 
Abstract: Language-conditioned robotic skills make it possible to apply the high-level reasoning of Large Language Models (LLMs) to low-level robotic control. A remaining challenge is to acquire a diverse set of fundamental skills. Existing approaches either manually decompose a complex task into atomic robotic actions in a top-down fashion, or bootstrap as many combinations as possible in a bottom-up fashion to cover a wider range of task possibilities. These decompositions or combinations, however, require an initial skill library. For example, a "grasping" capability can never emerge from a skill library containing only diverse "pushing" skills. Existing skill discovery techniques with reinforcement learning acquire skills by an exhaustive exploration but often yield non-meaningful behaviors. In this study, we introduce a novel framework for skill discovery that is entirely driven by LLMs. The framework begins with an LLM generating task proposals based on the provided scene description and the robot's configurations, aiming to incrementally acquire new skills upon task completion. For each proposed task, a series of reinforcement learning processes are initiated, utilizing reward and success determination functions sampled by the LLM to develop the corresponding policy. The reliability and trustworthiness of learned behaviors are further ensured by an independent vision-language model. We show that starting with zero skill, the ASD skill library emerges and expands to more and more meaningful and reliable skills, enabling the robot to efficiently further propose and complete advanced tasks. The project page can be found at: https://agentic-skill-discovery.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15019">https://arxiv.org/abs/2405.15019</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a new framework for skill discovery that is driven by Large Language Models (LLMs), which falls under your interest in using large language models for computer automation. Although it focuses on robotic control, the methods it proposes can be informative for your broader interest in LLMs and agent control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15130" target="_blank">OptLLM: Optimal Assignment of Queries to Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.15130v1/extracted/5606185/Figs/Motivation_log_parsing.png" target="_blank"><img src="https://arxiv.org/html/2405.15130v1/extracted/5606185/Figs/Motivation_log_parsing.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yueyue Liu, Hongyu Zhang, Yuantian Miao, Van-Hoang Le, Zhiqiang Li</p>
            <p><strong>Summary:</strong> arXiv:2405.15130v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have garnered considerable attention owing to their remarkable capabilities, leading to an increasing number of companies offering LLMs as services. Different LLMs achieve different performance at different costs. A challenge for users lies in choosing the LLMs that best fit their needs, balancing cost and performance. In this paper, we propose a framework for addressing the cost-effective query allocation problem for LLMs. Given a set of input queries and candidate LLMs, our framework, named OptLLM, provides users with a range of optimal solutions to choose from, aligning with their budget constraints and performance preferences, including options for maximizing accuracy and minimizing cost. OptLLM predicts the performance of candidate LLMs on each query using a multi-label classification model with uncertainty estimation and then iteratively generates a set of non-dominated solutions by destructing and reconstructing the current solution. To evaluate the effectiveness of OptLLM, we conduct extensive experiments on various types of tasks, including text classification, question answering, sentiment analysis, reasoning, and log parsing. Our experimental results demonstrate that OptLLM substantially reduces costs by 2.40% to 49.18% while achieving the same accuracy as the best LLM. Compared to other multi-objective optimization algorithms, OptLLM improves accuracy by 2.94% to 69.05% at the same cost or saves costs by 8.79% and 95.87% while maintaining the highest attainable accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15130">https://arxiv.org/abs/2405.15130</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a way to optimally assign tasks to different Large Language Models, which can be utilised in controlling software or web browsers for better efficiency and cost-effectiveness. However, it doesn't particularly focus on automation using Large Language Models, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15164" target="_blank">From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks</a></h3>
            <a href="https://arxiv.org/html/2405.15164v1/extracted/5616660/transformer.png" target="_blank"><img src="https://arxiv.org/html/2405.15164v1/extracted/5616660/transformer.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jacob Russin, Sam Whitman McGrath, Danielle J. Williams, Lotem Elber-Dorozko</p>
            <p><strong>Summary:</strong> arXiv:2405.15164v1 Announce Type: cross 
Abstract: Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15164">https://arxiv.org/abs/2405.15164</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper seems highly relevant to your interest in large language models (LLMs) since it covers their role in tasks including writing computer programs, which aligns with your subtopics like 'Using large language models to control software'. Although it does not talk about controlling web browsers or automation directly, insights from this paper could be applicable to those areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15223" target="_blank">iVideoGPT: Interactive VideoGPTs are Scalable World Models</a></h3>
            <a href="https://arxiv.org/html/2405.15223v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15223v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long</p>
            <p><strong>Summary:</strong> arXiv:2405.15223v1 Announce Type: cross 
Abstract: World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15223">https://arxiv.org/abs/2405.15223</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest as it proposes Interactive VideoGPT, a scalable autoregressive transformer framework which integrates multimodal signals. It's not directly about using a large language model to control software or web browsers, but it does relate to the broader topic of agent-based, model-based reinforcement learning applications, which is a form of automation using large models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15230" target="_blank">$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization</a></h3>
            <a href="https://arxiv.org/html/2405.15230v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15230v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Long Tan Le, Han Shu, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran</p>
            <p><strong>Summary:</strong> arXiv:2405.15230v1 Announce Type: cross 
Abstract: While astonishingly capable, large Language Models (LLM) can sometimes produce outputs that deviate from human expectations. Such deviations necessitate an alignment phase to prevent disseminating untruthful, toxic, or biased information. Traditional alignment methods based on reinforcement learning often struggle with the identified instability, whereas preference optimization methods are limited by their overfitting to pre-collected hard-label datasets. In this paper, we propose a novel LLM alignment framework named $i$REPO, which utilizes implicit Reward pairwise difference regression for Empirical Preference Optimization. Particularly, $i$REPO employs self-generated datasets labelled by empirical human (or AI annotator) preference to iteratively refine the aligned policy through a novel regression-based loss function. Furthermore, we introduce an innovative algorithm backed by theoretical guarantees for achieving optimal results under ideal assumptions and providing a practical performance-gap result without such assumptions. Experimental results with Phi-2 and Mistral-7B demonstrate that $i$REPO effectively achieves self-alignment using soft-label, self-generated responses and the logit of empirical AI annotators. Furthermore, our approach surpasses preference optimization baselines in evaluations using the Language Model Evaluation Harness and Multi-turn benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15230">https://arxiv.org/abs/2405.15230</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a novel framework for aligning large language models, which could be relevant to your interest in controlling software or web browsers with large language models. However, it does not directly address these specific interest points.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15302" target="_blank">Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation</a></h3>
            <a href="https://arxiv.org/html/2405.15302v1/extracted/5617303/pic/LLM_fail_to_reasoning.png" target="_blank"><img src="https://arxiv.org/html/2405.15302v1/extracted/5617303/pic/LLM_fail_to_reasoning.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhiwei Wang, Yunji Wang, Zhongwang Zhang, Zhangchen Zhou, Hui Jin, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Yaoyu Zhang, Zhi-Qin John Xu</p>
            <p><strong>Summary:</strong> arXiv:2405.15302v1 Announce Type: cross 
Abstract: Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capabilities. In this study, we examine the matching mechanism employed by Transformer for multi-step reasoning on a constructed dataset. We investigate factors that influence the model's matching mechanism and discover that small initialization and post-LayerNorm can facilitate the formation of the matching mechanism, thereby enhancing the model's reasoning ability. Moreover, we propose a method to improve the model's reasoning capability by adding orthogonal noise. Finally, we investigate the parallel reasoning mechanism of Transformers and propose a conjecture on the upper bound of the model's reasoning ability based on this phenomenon. These insights contribute to a deeper understanding of the reasoning processes in large language models and guide designing more effective reasoning architectures and training strategies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15302">https://arxiv.org/abs/2405.15302</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models and their reasoning capabilities. Although it does not focus on control of software or web browsers specifically, understanding reasoning mechanisms can potentially be applied to improve such control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15346" target="_blank">BiSup: Bidirectional Quantization Error Suppression for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.15346v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15346v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Minghui Zou, Ronghui Guo, Sai Zhang, Xiaowang Zhang, Zhiyong Feng</p>
            <p><strong>Summary:</strong> arXiv:2405.15346v1 Announce Type: cross 
Abstract: As the size and context length of Large Language Models (LLMs) grow, weight-activation quantization has emerged as a crucial technique for efficient deployment of LLMs. Compared to weight-only quantization, weight-activation quantization presents greater challenges due to the presence of outliers in activations. Existing methods have made significant progress by exploring mixed-precision quantization and outlier suppression. However, these methods primarily focus on optimizing the results of single matrix multiplication, neglecting the bidirectional propagation of quantization errors in LLMs. Specifically, errors accumulate vertically within the same token through layers, and diffuse horizontally across different tokens due to self-attention mechanisms. To address this issue, we introduce BiSup, a Bidirectional quantization error Suppression method. By constructing appropriate optimizable parameter spaces, BiSup utilizes a small amount of data for quantization-aware parameter-efficient fine-tuning to suppress the error vertical accumulation. Besides, BiSup employs prompt mixed-precision quantization strategy, which preserves high precision for the key-value cache of system prompts, to mitigate the error horizontal diffusion. Extensive experiments on Llama and Qwen families demonstrate that BiSup can improve performance over two state-of-the-art methods (the average WikiText2 perplexity decreases from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128 configuration), further facilitating the practical applications of low-bit weight-activation quantization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15346">https://arxiv.org/abs/2405.15346</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper doesn't directly address the use of large language models for software or browser control, it speaks to optimizing the performance of these models through bidirectional quantization error suppression. This could be beneficial in expanding the efficiency and capabilities of automated programs controlled by LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15485" target="_blank">Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.15485v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15485v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siyuan Guo, Aniket Didolkar, Nan Rosemary Ke, Anirudh Goyal, Ferenc Husz\'ar, Bernhard Sch\"olkopf</p>
            <p><strong>Summary:</strong> arXiv:2405.15485v1 Announce Type: cross 
Abstract: We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems. In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics. Motivated by the Neural Tangent Kernel (NTK), we propose \textit{NTKEval} to assess changes in LLM's probability distribution via training on different kinds of math data. Our systematic analysis finds evidence of domain understanding during in-context learning. By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15485">https://arxiv.org/abs/2405.15485</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While not entirely focused on controlling software or web browsers, the paper discusses the use of large language models for scientific discovery and problem-solving. Therefore, insights from this paper could potentially be helpful for creating LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15729" target="_blank">Optimizing Large Language Models for OpenAPI Code Completion</a></h3>
            <a href="https://arxiv.org/html/2405.15729v1/extracted/5618899/benchmark-pipeline.png" target="_blank"><img src="https://arxiv.org/html/2405.15729v1/extracted/5618899/benchmark-pipeline.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bohdan Petryshyn, Mantas Luko\v{s}evi\v{c}ius</p>
            <p><strong>Summary:</strong> arXiv:2405.15729v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development. Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions. This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama. A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed. The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model. Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15729">https://arxiv.org/abs/2405.15729</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models for code completion, which pertains to the control of software. It also proposes task-specific optimizations for these models, aligning with your interest in new method proposals.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15739" target="_blank">Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias</a></h3>
            <a href="https://arxiv.org/html/2405.15739v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15739v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis</p>
            <p><strong>Summary:</strong> arXiv:2405.15739v1 Announce Type: cross 
Abstract: Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue. Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination. Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15739">https://arxiv.org/abs/2405.15739</a></p>
            <p><strong>Category:</strong> cs.DL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it investigates the implication of large language models in citation patterns, which could be an aspect of using these models for computer automation. Although it doesn't directly address using these models for control purposes, it discusses their influence and potential biases which are important when considering their application for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15765" target="_blank">Scaling Laws for Discriminative Classification in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.15765v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15765v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dean Wyatte, Fatemeh Tahmasbi, Ming Li, Thomas Markovich</p>
            <p><strong>Summary:</strong> arXiv:2405.15765v1 Announce Type: cross 
Abstract: Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models. The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be useful in customer support applications. While powerful, LLMs have been observed to be prone to hallucination which unfortunately makes their near term use in customer support applications challenging. To address this issue we present a system that allows us to use an LLM to augment our customer support advocates by re-framing the language modeling task as a discriminative classification task. In this framing, we seek to present the top-K best template responses for a customer support advocate to use when responding to a customer. We present the result of both offline and online experiments where we observed offline gains and statistically significant online lifts for our experimental system. Along the way, we present observed scaling curves for validation loss and top-K accuracy, resulted from model parameter ablation studies. We close by discussing the space of trade-offs with respect to model size, latency, and accuracy as well as and suggesting future applications to explore.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15765">https://arxiv.org/abs/2405.15765</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models for customer support applications which can be considered as a form of software control. However, it doesn't propose a new method but it shows an application of existing methods, hence the score isn't the max.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.09993" target="_blank">Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</a></h3>
            <a href="https://arxiv.org/html/2305.09993v2/extracted/5616464/figures/logical_deduction_chatgpt_zeroshot.png" target="_blank"><img src="https://arxiv.org/html/2305.09993v2/extracted/5616464/figures/logical_deduction_chatgpt_zeroshot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Weijia Xu, Andrzej Banburski-Fahey, Nebojsa Jojic</p>
            <p><strong>Summary:</strong> arXiv:2305.09993v2 Announce Type: replace 
Abstract: We introduce Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems. We conduct extensive experiments on 20 challenging reasoning tasks. Results show that Reprompting outperforms human-written CoT prompts substantially by +9.4 points on average. It also achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.09993">https://arxiv.org/abs/2305.09993</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it focuses on automation of Chain-of-Thought processes using a sampling algorithm, which is a form of control using a model likely deriving from a large language model. However, it doesn't specifically mention control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02992" target="_blank">Decoding-time Realignment of Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.02992v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02992v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, Mathieu Blondel</p>
            <p><strong>Summary:</strong> arXiv:2402.02992v2 Announce Type: replace 
Abstract: Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02992">https://arxiv.org/abs/2402.02992</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a new process for better aligning large language models with human preferences, potentially leading to improved control over software. However, the paper does not explicitly discuss software or web browser control, hence the 4 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10207" target="_blank">Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment</a></h3>
            <a href="https://arxiv.org/html/2402.10207v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10207v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</p>
            <p><strong>Summary:</strong> arXiv:2402.10207v4 Announce Type: replace 
Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10207">https://arxiv.org/abs/2402.10207</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses the alignment of Large Language Models (LLMs) with human preferences, a procedure that may have implications for the control of software or web browsers via LLMs. It does not specifically mention automation or browsers, hence the score of 4, but the explored method could potentially be applied to these contexts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17453" target="_blank">DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning</a></h3>
            <a href="https://arxiv.org/html/2402.17453v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.17453v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang</p>
            <p><strong>Summary:</strong> arXiv:2402.17453v4 Announce Type: replace 
Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\% success rate in the development stage, while attaining 36\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17453">https://arxiv.org/abs/2402.17453</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it deals with automating data science tasks using large language models (LLMs), which is a subtopic of your interest in agent-based LLMs. Although it doesn't specifically mention control of software or web browsers, the methodologies it presents could be applicable to these use-cases.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00747" target="_blank">Soft Preference Optimization: Aligning Language Models to Expert Distributions</a></h3>
            <a href="https://arxiv.org/html/2405.00747v2/extracted/5616275/Figures/win_rate_main_body.png" target="_blank"><img src="https://arxiv.org/html/2405.00747v2/extracted/5616275/Figures/win_rate_main_body.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Arsalan Sharifnassab, Sina Ghiassian, Saber Salehkaleybar, Surya Kanoria, Dale Schuurmans</p>
            <p><strong>Summary:</strong> arXiv:2405.00747v2 Announce Type: replace 
Abstract: We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's "softness" adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00747">https://arxiv.org/abs/2405.00747</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method for aligning Large Language Models (LLMs) with human preferences, which can be relevant for creating better performing agents based on LLMs. However, it does not specifically address control of software or web browsers, which is why a full score of 5 hasn't been given.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03341" target="_blank">Enhancing Q-Learning with Large Language Model Heuristics</a></h3>
            <a href="https://arxiv.org/html/2405.03341v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.03341v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiefeng Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.03341v3 Announce Type: replace 
Abstract: Q-learning excels in learning from feedback within sequential decision-making tasks but often requires extensive sampling to achieve significant improvements. While reward shaping can enhance learning efficiency, non-potential-based methods introduce biases that affect performance, and potential-based reward shaping, though unbiased, lacks the ability to provide heuristics for state-action pairs, limiting its effectiveness in complex environments. Large language models (LLMs) can achieve zero-shot learning for simpler tasks, but they suffer from low inference speeds and occasional hallucinations. To address these challenges, we propose \textbf{LLM-guided Q-learning}, a framework that leverages LLMs as heuristics to aid in learning the Q-function for reinforcement learning. Our theoretical analysis demonstrates that this approach adapts to hallucinations, improves sample efficiency, and avoids biasing final performance. Experimental results show that our algorithm is general, robust, and capable of preventing ineffective exploration.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03341">https://arxiv.org/abs/2405.03341</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper deals with harnessing Large Language Models (LLMs) to enhance reinforcement learning through Q-learning, particularly in sequential decision-making tasks – an aspect that’s very relevant for controlling software or web browsers. However, it does not explicitly mention control of software or web browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13746" target="_blank">CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.13746v2/extracted/5616928/pic/archi/plain_fl.png" target="_blank"><img src="https://arxiv.org/html/2405.13746v2/extracted/5616928/pic/archi/plain_fl.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Huiwen Wu, Xiaohan Li, Deyi Zhang, Xiaogang Xu, Jiafei Wu, Puning Zhao, Zhe Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.13746v2 Announce Type: replace 
Abstract: The success of current Large-Language Models (LLMs) hinges on extensive training data that is collected and stored centrally, called Centralized Learning (CL). However, such a collection manner poses a privacy threat, and one potential solution is Federated Learning (FL), which transfers gradients, not raw data, among clients. Unlike traditional networks, FL for LLMs incurs significant communication costs due to their tremendous parameters. This study introduces an innovative approach to compress gradients to improve communication efficiency during LLM FL, formulating the new FL pipeline named CG-FedLLM. This approach integrates an encoder on the client side to acquire the compressed gradient features and a decoder on the server side to reconstruct the gradients. We also developed a novel training strategy that comprises Temporal-ensemble Gradient-Aware Pre-training (TGAP) to identify characteristic gradients of the target model and Federated AutoEncoder-Involved Fine-tuning (FAF) to compress gradients adaptively. Extensive experiments confirm that our approach reduces communication costs and improves performance (e.g., average 3 points increment compared with traditional CL- and FL-based fine-tuning with LlaMA on a well-recognized benchmark, C-Eval). This improvement is because our encoder-decoder, trained via TGAP and FAF, can filter gradients while selectively preserving critical features. Furthermore, we present a series of experimental analyses focusing on the signal-to-noise ratio, compression rate, and robustness within this privacy-centric framework, providing insight into developing more efficient and secure LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13746">https://arxiv.org/abs/2405.13746</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it involves the use of Large Language Models. It pertains to the optimization of such models for Fedaration Learning and potentially their control on a larger scale through improved training strategies. Although not directly about controlling software or web browsers, it introduces a novel training strategy to Large Language Models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08147" target="_blank">VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search</a></h3>
            <a href="https://arxiv.org/html/2402.08147v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08147v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> David Brandfonbrener, Simon Henniger, Sibi Raja, Tarun Prasad, Chloe Loughridge, Federico Cassano, Sabrina Ruixin Hu, Jianang Yang, William E. Byrd, Robert Zinkov, Nada Amin</p>
            <p><strong>Summary:</strong> arXiv:2402.08147v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) can generate useful code, but often the code they generate cannot be trusted to be sound. In this paper, we present VerMCTS, an approach to begin to resolve this issue by generating verified programs in Dafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide a modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier to gain intermediate feedback inside the search algorithm by checking partial programs at each step to estimate an upper bound on the value function. To measure the performance of VerMCTS, we develop a new suite of multi-step verified programming problems in Dafny and Coq. In terms of pass@T, a new metric which computes the pass rate given a budget of T tokens sampled from the LLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000 across the suite over repeated sampling from the base language model. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08147">https://arxiv.org/abs/2402.08147</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models' as it discusses the application of Large Language Models in synthesizing multi-step programs. However, it doesn't directly address control of software or browsers but shows potential for LLMs in computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.19521" target="_blank">Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models</a></h3>
            <a href="https://arxiv.org/html/2403.19521v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.19521v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ang Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan</p>
            <p><strong>Summary:</strong> arXiv:2403.19521v4 Announce Type: replace-cross 
Abstract: In this paper, we delve into several mechanisms employed by Transformer-based language models (LLMs) for factual recall tasks. We outline a pipeline consisting of three major steps: (1) Given a prompt ``The capital of France is,'' task-specific attention heads extract the topic token, such as ``France,'' from the context and pass it to subsequent MLPs. (2) As attention heads' outputs are aggregated with equal weight and added to the residual stream, the subsequent MLP acts as an ``activation,'' which either erases or amplifies the information originating from individual heads. As a result, the topic token ``France'' stands out in the residual stream. (3) A deep MLP takes ``France'' and generates a component that redirects the residual stream towards the direction of the correct answer, i.e., ``Paris.'' This procedure is akin to applying an implicit function such as ``get\_capital($X$),'' and the argument $X$ is the topic token information passed by attention heads. To achieve the above quantitative and qualitative analysis for MLPs, we proposed a novel analytic method aimed at decomposing the outputs of the MLP into components understandable by humans. Additionally, we observed a universal anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall confidence. The above interpretations are evaluated across diverse tasks spanning various domains of factual knowledge, using various language models from the GPT-2 families, 1.3B OPT, up to 7B Llama-2, and in both zero- and few-shot setups.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.19521">https://arxiv.org/abs/2403.19521</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research paper 'Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models' is interesting for you, given your interest in large-language based models. Although it doesn't specifically cover the usage of these models in controlling software or web browsers, it does provide insight into the mechanisms how large language models recall information, which would be useful in understanding and controlling the behavior of such models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15124" target="_blank">Scaling Law for Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Jingzhe Shi, Qinwei Ma, Huan Ma, Lei Li</p>
            <p><strong>Summary:</strong> arXiv:2405.15124v1 Announce Type: new 
Abstract: Scaling law that rewards large datasets, complex models and enhanced data granularity has been observed in various fields of deep learning. Yet, studies on time series forecasting have cast doubt on scaling behaviors of deep learning methods for time series forecasting: while more training data improves performance, more capable models do not always outperform less capable models, and longer input horizons may hurt performance for some models. We propose a theory for scaling law for time series forecasting that can explain these seemingly abnormal behaviors. We take into account the impact of dataset size and model complexity, as well as time series data granularity, particularly focusing on the look-back horizon, an aspect that has been unexplored in previous theories. Furthermore, we empirically evaluate various models using a diverse set of time series forecasting datasets, which (1) verifies the validity of scaling law on dataset size and model complexity within the realm of time series forecasting, and (2) validates our theoretical framework, particularly regarding the influence of look back horizon. We hope our findings may inspire new models targeting time series forecasting datasets of limited size, as well as large foundational datasets and models for time series forecasting in future works.\footnote{Codes for our experiments will be made public at: \url{https://github.com/JingzheShi/ScalingLawForTimeSeriesForecasting}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15124">https://arxiv.org/abs/2405.15124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it discusses the impact of large datasets and complex models in time-series forecasting. It proposes a new theory for a scaling law for time series forecasting which is foundational for the field. The research also hints towards the creation of new models and advances in foundational datasets for time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15256" target="_blank">FTMixer: Frequency and Time Domain Representations Fusion for Time Series Modeling</a></h3>
            <a href="https://arxiv.org/html/2405.15256v1/extracted/5613292/figures/fig1.png" target="_blank"><img src="https://arxiv.org/html/2405.15256v1/extracted/5613292/figures/fig1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhengnan Li, Yunxiao Qin, Xilong Cheng, Yuting Tan</p>
            <p><strong>Summary:</strong> arXiv:2405.15256v1 Announce Type: new 
Abstract: Time series data can be represented in both the time and frequency domains, with the time domain emphasizing local dependencies and the frequency domain highlighting global dependencies. To harness the strengths of both domains in capturing local and global dependencies, we propose the Frequency and Time Domain Mixer (FTMixer). To exploit the global characteristics of the frequency domain, we introduce the Frequency Channel Convolution (FCC) module, designed to capture global inter-series dependencies. Inspired by the windowing concept in frequency domain transformations, we present the Windowing Frequency Convolution (WFC) module to capture local dependencies. The WFC module first applies frequency transformation within each window, followed by convolution across windows. Furthermore, to better capture these local dependencies, we employ channel-independent scheme to mix the time domain and frequency domain patches. Notably, FTMixer employs the Discrete Cosine Transformation (DCT) with real numbers instead of the complex-number-based Discrete Fourier Transformation (DFT), enabling direct utilization of modern deep learning operators in the frequency domain. Extensive experimental results across seven real-world long-term time series datasets demonstrate the superiority of FTMixer, in terms of both forecasting performance and computational efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15256">https://arxiv.org/abs/2405.15256</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests. It proposes the FTMixer, a new deep learning method for time series modeling that utilizes both time and frequency domain representations. The paper also introduces the FCC and WFC modules for capturing global and local dependencies, respectively, which could be considered as new foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14982" target="_blank">In-context Time Series Predictor</a></h3>
            <a href="https://arxiv.org/html/2405.14982v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14982v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiecheng Lu, Yan Sun, Shihao Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.14982v1 Announce Type: new 
Abstract: Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate "time series forecasting tasks" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms, and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14982">https://arxiv.org/abs/2405.14982</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in time series and deep learning. It presents an in-context learning approach for time series forecasting using Transformer-based large language models, which directly covers your interests in new deep learning methods for time series and transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15273" target="_blank">Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders</a></h3>
            <a href="https://arxiv.org/html/2405.15273v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15273v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qichao Shentu, Beibu Li, Kai Zhao, Yang shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</p>
            <p><strong>Summary:</strong> arXiv:2405.15273v1 Announce Type: new 
Abstract: Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios with scarce training data. Aiming at this problem, we propose constructing a general time series anomaly detection model, which is pre-trained on extensive multi-domain datasets and can subsequently apply to a multitude of downstream scenarios. The significant divergence of time series data across different domains presents two primary challenges in building such a general model: (1) meeting the diverse requirements of appropriate information bottlenecks tailored to different datasets in one unified model, and (2) enabling distinguishment between multiple normal and abnormal patterns, both are crucial for effective anomaly detection in various target scenarios. To tackle these two challenges, we propose a General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible selection of bottlenecks based on different data and explicitly enhances clear differentiation between normal and abnormal series. We conduct extensive experiments on nine target datasets from different domains. After pre-training on multi-domain data, DADA, serving as a zero-shot anomaly detector for these datasets, still achieves competitive or even superior results compared to those models tailored to each specific dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15273">https://arxiv.org/abs/2405.15273</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant under the 'time-series' tag as it proposes a new technique for anomaly detection in time series data. Although it does not specifically focus on forecasting, the exploration of a pre-training, general model for time series anomalies may be of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15310" target="_blank">Spectraformer: A Unified Random Feature Framework for Transformer</a></h3>
            <a href="https://arxiv.org/html/2405.15310v1/extracted/5617148/plot_experiment_avg.png" target="_blank"><img src="https://arxiv.org/html/2405.15310v1/extracted/5617148/plot_experiment_avg.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Duke Nguyen, Aditya Joshi, Flora Salim</p>
            <p><strong>Summary:</strong> arXiv:2405.15310v1 Announce Type: new 
Abstract: Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods use a subset of combinations of component functions and weight matrices within the random features paradigm. We identify the need for a systematic comparison of different combinations of weight matrix and component functions for attention learning in Transformer. In this work, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in linearized attention of the Transformer. We experiment with broad classes of component functions and weight matrices for three textual tasks in the LRA benchmark. Our experimentation with multiple combinations of component functions and weight matrices leads us to a novel combination with 23.4% faster training time and 25.2% lower memory consumption over the previous SOTA random feature Transformer, while maintaining the performance, as compared to the Original Transformer. Our code is available at: https://anonymous.4open.science/r/spectraformer-8A97 .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15310">https://arxiv.org/abs/2405.15310</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper involves the Spectraformer, a unified framework involving transformers, which is related to your interest in 'new transformer-like models for time series'. However, the paper is not specifically focused on time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15317" target="_blank">NuwaTS: Mending Every Incomplete Time Series</a></h3>
            <a href="https://arxiv.org/html/2405.15317v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15317v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, Yuankai Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.15317v1 Announce Type: new 
Abstract: Time series imputation plays a crucial role in various real-world systems and has been extensively explored. Models for time series imputation often require specialization, necessitating distinct designs for different domains and missing patterns. In this study, we introduce NuwaTS, a framework to repurpose Pre-trained Language Model (PLM) for general time series imputation. Once trained, this model can be applied to imputation tasks on incomplete time series from any domain with any missing patterns. We begin by devising specific embeddings for each sub-series patch of the incomplete time series. These embeddings encapsulate information about the patch itself, the missing data patterns within the patch, and the patch's statistical characteristics. To enhance the model's adaptability to different missing patterns, we propose a contrastive learning approach to make representations of the same patch more similar across different missing patterns. By combining this contrastive loss with the missing data imputation task, we train PLMs to obtain a one-for-all imputation model. Furthermore, we utilize a plug-and-play layer-wise fine-tuning approach to train domain-specific models. Experimental results demonstrate that leveraging a dataset of over seventeen million time series from diverse domains, we obtain a one-for-all imputation model which outperforms existing domain-specific models across various datasets and missing patterns. Additionally, we find that NuwaTS can be generalized to other time series tasks such as forecasting. Our codes are available at https://github.com/Chengyui/NuwaTS.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15317">https://arxiv.org/abs/2405.15317</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'time-series' topic you are interested in. It presents a new method called NuwaTS for time series imputation leveraging Pre-trained Language Models. Also, it briefly discusses its application in forecasting which is one of your sub-interests within this area.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15598" target="_blank">MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel Data Fusion Network Model Integrating CNN, LSTM, and GRU</a></h3>
            <a href="https://arxiv.org/html/2405.15598v1/extracted/5618519/framework.png" target="_blank"><img src="https://arxiv.org/html/2405.15598v1/extracted/5618519/framework.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Md Abrar Jahin, Asef Shahriar, Md Al Amin</p>
            <p><strong>Summary:</strong> arXiv:2405.15598v1 Announce Type: new 
Abstract: Accurate demand forecasting is crucial for optimizing supply chain management. Traditional methods often fail to capture complex patterns from seasonal variability and special events. Despite advancements in deep learning, interpretable forecasting models remain a challenge. To address this, we introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive performance by extracting spatial and temporal features from time series data. Our rigorous benchmarking demonstrates that MCDFN outperforms seven other deep-learning models, achieving superior metrics: MSE (23.5738%), RMSE (4.8553%), MAE (3.9991%), and MAPE (20.1575%). Additionally, MCDFN's predictions were statistically indistinguishable from actual values, confirmed by a paired t-test with a 5% p-value and a 10-fold cross-validated statistical paired t-test. We apply explainable AI techniques like ShapTime and Permutation Feature Importance to enhance interpretability. This research advances demand forecasting methodologies and offers practical guidelines for integrating MCDFN into supply chain systems, highlighting future research directions for scalability and user-friendly deployment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15598">https://arxiv.org/abs/2405.15598</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it presents a new approach (MCDFN) for time-series forecasting using a hybrid model integrating CNN, LSTM, and GRU. Although it doesn't mention multimodal features or foundation models specifically, the method provides a novel deep learning approach to time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15731" target="_blank">Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks</a></h3>
            
            <p><strong>Authors:</strong> Jerome Sieber, Carmen Amo Alonso, Alexandre Didier, Melanie N. Zeilinger, Antonio Orvieto</p>
            <p><strong>Summary:</strong> arXiv:2405.15731v1 Announce Type: new 
Abstract: Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15731">https://arxiv.org/abs/2405.15731</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Despite not being directly targeted towards time series, the methods and insights presented within the paper have tangible applications to time series analysis, specifically with their discussion on new foundation models which include the Dynamical Systems Framework (DSF).</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14913" target="_blank">High Rank Path Development: an approach of learning the filtration of stochastic processes</a></h3>
            
            <p><strong>Authors:</strong> Jiajie Tao, Hao Ni, Chong Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.14913v1 Announce Type: cross 
Abstract: Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14913">https://arxiv.org/abs/2405.14913</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper could be of interest to you as it presents a new approach for training from data, which could potentially be applied to time series forecasting. Even though it doesn't directly address the subject of deep learning, it discusses the concept of synthetic time series generation that is linked to your interest of new methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15412" target="_blank">ORCA: A Global Ocean Emulator for Multi-year to Decadal Predictions</a></h3>
            <a href="https://arxiv.org/html/2405.15412v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15412v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zijie Guo, Pumeng Lyu, Fenghua Ling, Jing-Jia Luo, Niklas Boers, Wanli Ouyang, Lei Bai</p>
            <p><strong>Summary:</strong> arXiv:2405.15412v1 Announce Type: cross 
Abstract: Ocean dynamics plays a crucial role in driving global weather and climate patterns. Accurate and efficient modeling of ocean dynamics is essential for improved understanding of complex ocean circulation and processes, for predicting climate variations and their associated teleconnections, and for addressing the challenges of climate change. While great efforts have been made to improve numerical Ocean General Circulation Models (OGCMs), accurate forecasting of global oceanic variations for multi-year remains to be a long-standing challenge. Here, we introduce ORCA (Oceanic Reliable foreCAst), the first data-driven model predicting global ocean circulation from multi-year to decadal time scales. ORCA accurately simulates the three-dimensional circulations and dynamics of the global ocean with high physical consistency. Hindcasts of key oceanic variables demonstrate ORCA's remarkable prediction skills in predicting ocean variations compared with state-of-the-art numerical OGCMs and abilities in capturing occurrences of extreme events at the subsurface ocean and ENSO vertical patterns. These results demonstrate the potential of data-driven ocean models for providing cheap, efficient, and accurate global ocean modeling and prediction. Moreover, ORCA stably and faithfully emulates ocean dynamics at decadal timescales, demonstrating its potential even for climate projections. The model will be available at https://github.com/OpenEarthLab/ORCA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15412">https://arxiv.org/abs/2405.15412</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper focuses on a new method, ORCA, for predicting global ocean circulation which can be seen as a form of time series forecasting. However, it relies more on data-driven models than on deep learning methods as specified in your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15579" target="_blank">Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout</a></h3>
            <a href="https://arxiv.org/html/2405.15579v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15579v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Krist\'of N\'emeth, D\'aniel Hadh\'azi</p>
            <p><strong>Summary:</strong> arXiv:2405.15579v1 Announce Type: cross 
Abstract: Recent results in the literature indicate that artificial neural networks (ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy of GDP nowcasts. Compared to the DFM, the performance advantage of these highly flexible, nonlinear estimators is particularly evident in periods of recessions and structural breaks. From the perspective of policy-makers, however, nowcasts are the most useful when they are conveyed with uncertainty attached to them. While the DFM and other classical time series approaches analytically derive the predictive (conditional) distribution for GDP growth, ANNs can only produce point nowcasts based on their default training procedure (backpropagation). To fill this gap, first in the literature, we adapt two different deep learning algorithms that enable ANNs to generate density nowcasts for U.S. GDP growth: Bayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts, defined as the mean of the empirical predictive distribution, is evaluated relative to a naive constant growth model for GDP and a benchmark DFM specification. Using a 1D CNN as the underlying ANN architecture, both algorithms outperform those benchmarks during the evaluation period (2012:Q1 -- 2022:Q4). Furthermore, both algorithms are able to dynamically adjust the location (mean), scale (variance), and shape (skew) of the empirical predictive distribution. The results indicate that both Bayes by Backprop and Monte Carlo dropout can effectively augment the scope and functionality of ANNs, rendering them a fully compatible and competitive alternative for classical time series approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15579">https://arxiv.org/abs/2405.15579</a></p>
            <p><strong>Category:</strong> econ.EM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the use of deep learning methods (namely, Bayes by Backprop and Monte Carlo dropout) for forecasting (a form of time series analysis). However, it doesn't directly discuss new foundation models and new multimodal or transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.11929" target="_blank">Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2401.11929v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.11929v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinliang Deng, Feiyang Ye, Du Yin, Xuan Song, Ivor W. Tsang, Hui Xiong</p>
            <p><strong>Summary:</strong> arXiv:2401.11929v3 Announce Type: replace 
Abstract: Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches. While longer sequences inherently offer richer information for enhanced predictive precision, prevailing studies often respond by escalating model complexity. These intricate models can inflate into millions of parameters, resulting in prohibitive parameter scales. Our study demonstrates, through both analytical and empirical evidence, that decomposition is key to containing excessive model inflation while achieving uniformly superior and robust results across various datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of time series data, our proposed model outperforms existing benchmarks, using over 99 \% fewer parameters than the majority of competing methods. Through this work, we aim to unleash the power of a restricted set of parameters by capitalizing on domain characteristics--a timely reminder that in the realm of LTSF, bigger is not invariably better.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.11929">https://arxiv.org/abs/2401.11929</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in time-series and deep learning, specifically in long-term forecasting. While it does not specifically mention new deep learning or transformer-like models, it introduces a new approach in which decompositions resolve issues of model complexity. This can be seen as a new method for handling time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00522" target="_blank">Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</a></h3>
            
            <p><strong>Authors:</strong> Mingze Wang, Weinan E</p>
            <p><strong>Summary:</strong> arXiv:2402.00522v4 Announce Type: replace 
Abstract: We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads. These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00522">https://arxiv.org/abs/2402.00522</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper could be beneficial to you as it looks into the approximation properties of Transformer models for sequence modeling, which is relevant to your interest in new transformer-like models for time series. While not focused on forecasting, understanding the Expressive Power and Mechanisms of Transformer models could inspire innovative methods for time series-related tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17966" target="_blank">STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting</a></h3>
            <a href="https://arxiv.org/html/2402.17966v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.17966v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hira Saleem, Flora Salim, Cormac Purcell</p>
            <p><strong>Summary:</strong> arXiv:2402.17966v2 Announce Type: replace 
Abstract: Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate STC-ViT against a operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT performs competitively with current data-driven methods in global forecasting while only being trained at lower resolution data and with less compute power.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17966">https://arxiv.org/abs/2402.17966</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes the STC-ViT, a new transformer-like model using Neural ODE layers and attention mechanisms for time series forecasting, particularly in weather prediction. It's a strong relevance but gets the score of 4 because it does not cover new datasets for foundation models or multimodal methods for time series, which were part of your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.18508" target="_blank">Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</a></h3>
            <a href="https://arxiv.org/html/2402.18508v2/extracted/5617149/FigureTable/Orchid-diagram2.jpg" target="_blank"><img src="https://arxiv.org/html/2402.18508v2/extracted/5617149/FigureTable/Orchid-diagram2.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mahdi Karami, Ali Ghodsi</p>
            <p><strong>Summary:</strong> arXiv:2402.18508v2 Announce Type: replace 
Abstract: In the rapidly evolving field of deep learning, the demand for models that are both expressive and computationally efficient has never been more critical. This paper introduces Orchid, a novel architecture designed to address the quadratic complexity of traditional attention mechanisms without compromising the ability to capture long-range dependencies and in-context learning. At the core of this architecture lies a new data-dependent global convolution layer, which contextually adapts its kernel conditioned on input sequence using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in our data-dependent convolution operation. The dynamic nature of the proposed convolution kernel grants Orchid high expressivity while maintaining quasilinear scalability for long sequences. We evaluate the proposed model across multiple domains, including language modeling and image classification, to highlight its performance and generality. Our experiments demonstrate that this architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.18508">https://arxiv.org/abs/2402.18508</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The research paper introduces 'Orchid', a novel architecture that seems adaptable for sequence modeling. Its design approach might be useful for extension into new deep learning methods for time series. However, the paper does not directly address time series forecasting, foundation models or multimodal and transformer-like models for time series, which are part of your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13522" target="_blank">Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues</a></h3>
            <a href="https://arxiv.org/html/2405.13522v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.13522v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhijian Xu, Yuxuan Bian, Jianyuan Zhong, Xiangyu Wen, Qiang Xu</p>
            <p><strong>Summary:</strong> arXiv:2405.13522v2 Announce Type: replace 
Abstract: This work introduces a novel Text-Guided Time Series Forecasting (TGTSF) task. By integrating textual cues, such as channel descriptions and dynamic news, TGTSF addresses the critical limitations of traditional methods that rely purely on historical data. To support this task, we propose TGForecaster, a robust baseline model that fuses textual cues and time series data using cross-attention mechanisms. We then present four meticulously curated benchmark datasets to validate the proposed framework, ranging from simple periodic data to complex, event-driven fluctuations. Our comprehensive evaluations demonstrate that TGForecaster consistently achieves state-of-the-art performance, highlighting the transformative potential of incorporating textual information into time series forecasting. This work not only pioneers a novel forecasting task but also establishes a new benchmark for future research, driving advancements in multimodal data integration for time series models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13522">https://arxiv.org/abs/2405.13522</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in new multimodal deep learning models for time series. It introduces TGForecaster, a model that uses cross-attention mechanisms to integrate textual cues and time series data, and validates it using multiple benchmark datasets. This suggests it may provide some novel insights into time series forecasting with deep learning.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2203.15756" target="_blank">Causal de Finetti: On the Identification of Invariant Causal Structure in Exchangeable Data</a></h3>
            
            <p><strong>Authors:</strong> Siyuan Guo, Viktor T\'oth, Bernhard Sch\"olkopf, Ferenc Husz\'ar</p>
            <p><strong>Summary:</strong> arXiv:2203.15756v3 Announce Type: replace-cross 
Abstract: Constraint-based causal discovery methods leverage conditional independence tests to infer causal relationships in a wide variety of applications. Just as the majority of machine learning methods, existing work focuses on studying $\textit{independent and identically distributed}$ data. However, it is known that even with infinite i.i.d.$\ $ data, constraint-based methods can only identify causal structures up to broad Markov equivalence classes, posing a fundamental limitation for causal discovery. In this work, we observe that exchangeable data contains richer conditional independence structure than i.i.d.$\ $ data, and show how the richer structure can be leveraged for causal discovery. We first present causal de Finetti theorems, which state that exchangeable distributions with certain non-trivial conditional independences can always be represented as $\textit{independent causal mechanism (ICM)}$ generative processes. We then present our main identifiability theorem, which shows that given data from an ICM generative process, its unique causal structure can be identified through performing conditional independence tests. We finally develop a causal discovery algorithm and demonstrate its applicability to inferring causal relationships from multi-environment data. Our code and models are publicly available at: https://github.com/syguo96/Causal-de-Finetti</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2203.15756">https://arxiv.org/abs/2203.15756</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interest in 'Causality and machine learning'. It specifically talks about causal discovery using exchangeable data and new ways to identify unique causal structures.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15120" target="_blank">A Counterfactual Analysis of the Dishonest Casino</a></h3>
            <a href="https://arxiv.org/html/2405.15120v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15120v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Martin Haugh, Raghav Singal</p>
            <p><strong>Summary:</strong> arXiv:2405.15120v1 Announce Type: new 
Abstract: The dishonest casino is a well-known hidden Markov model (HMM) used in educational settings to introduce HMMs and graphical models. Here, a sequence of die rolls is observed, with the casino switching between a fair and a loaded die. Typically, the goal is to use the observed rolls to infer the pattern of fair and loaded dice, leading to filtering, smoothing, and Viterbi algorithms. This paper, however, explores how much of the winnings is attributable to the casino's cheating, a counterfactual question beyond the scope of HMM primitives. To address this, we introduce a structural causal model (SCM) consistent with the HMM and show that the expected winnings attributable to cheating (EWAC) can be bounded using linear programs (LPs). Through numerical experiments, we compute these bounds and develop intuition using benchmark SCMs based on independence, comonotonic, and counter-monotonic copulas. We show that tighter bounds are obtained with a time-homogeneity condition on the SCM, while looser bounds allow for an almost explicit LP solution. Domain-specific knowledge like pathwise monotonicity or counterfactual stability can be incorporated via linear constraints. Our work contributes to bounding counterfactuals in causal inference and is the first to develop LP bounds in a dynamic HMM setting, benefiting educational contexts where counterfactual inference is taught.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15120">https://arxiv.org/abs/2405.15120</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper introduces a structural causal model (SCM) in a hidden Markov model (HMM) setting to answer a counterfactual question. This is relevant to your interest in causal discovery and causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15325" target="_blank">On the Identification of Temporally Causal Representation with Instantaneous Dependence</a></h3>
            <a href="https://arxiv.org/html/2405.15325v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15325v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song, Mingming Gong, Zhifeng Hao, Zhengmao Zhu, Guangyi Chen, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.15325v1 Announce Type: new 
Abstract: Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15325">https://arxiv.org/abs/2405.15325</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it proposes a new framework (IDOL) for identifying latent causal processes from time series data, which is specific to your subtopic of 'Causal discovery'. Although it does not describe utilizing large language models, it aligns well with your general interest in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15443" target="_blank">Fairness-Accuracy Trade-Offs: A Causal Perspective</a></h3>
            <a href="https://arxiv.org/html/2405.15443v1/extracted/5618078/figures/intro-plot.png" target="_blank"><img src="https://arxiv.org/html/2405.15443v1/extracted/5618078/figures/intro-plot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Drago Plecko, Elias Bareinboim</p>
            <p><strong>Summary:</strong> arXiv:2405.15443v1 Announce Type: new 
Abstract: Systems based on machine learning may exhibit discriminatory behavior based on sensitive characteristics such as gender, sex, religion, or race. In light of this, various notions of fairness and methods to quantify discrimination were proposed, leading to the development of numerous approaches for constructing fair predictors. At the same time, imposing fairness constraints may decrease the utility of the decision-maker, highlighting a tension between fairness and utility. This tension is also recognized in legal frameworks, for instance in the disparate impact doctrine of Title VII of the Civil Rights Act of 1964 -- in which specific attention is given to considerations of business necessity -- possibly allowing the usage of proxy variables associated with the sensitive attribute in case a high-enough utility cannot be achieved without them. In this work, we analyze the tension between fairness and accuracy from a causal lens for the first time. We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor's loss increases when a causal fairness constraint is enforced. We then show that the total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs. At the same time, enforcing a causal constraint often reduces the disparity between demographic groups. Thus, we introduce a quantity that summarizes the fairness-utility trade-off, called the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess loss from constraining a causal pathway. This quantity is suitable for comparing the fairness-utility trade-off across causal pathways. Finally, as our approach requires causally-constrained fair predictors, we introduce a new neural approach for causally-constrained fair learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15443">https://arxiv.org/abs/2405.15443</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in causality and machine learning. The study introduces new concepts like the path-specific excess loss and causal fairness/utility ratio. Although not specifically about causal discovery or representation learning, it uses causal reasoning to examine fairness and accuracy trade-offs in machine learning, which could broaden your understanding of the field.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15446" target="_blank">Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making</a></h3>
            <a href="https://arxiv.org/html/2405.15446v1/extracted/5618032/figures/mimic-mtg.png" target="_blank"><img src="https://arxiv.org/html/2405.15446v1/extracted/5618032/figures/mimic-mtg.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Drago Plecko, Elias Bareinboim</p>
            <p><strong>Summary:</strong> arXiv:2405.15446v1 Announce Type: new 
Abstract: Investigating fairness and equity of automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice). In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor $\widehat Y$ between groups, written $P(\hat y \mid x_1) - P(\hat y \mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15446">https://arxiv.org/abs/2405.15446</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in causality and machine learning. It discusses a causal framework for fairness and prediction in machine learning systems, which touches upon the concept of 'Causal discovery'. Even though it's not strictly about new causal methods, it provides important insights on how to achieve fairness in machine learning, a growing field of interest for many researchers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15505" target="_blank">Revisiting Counterfactual Regression through the Lens of Gromov-Wasserstein Information Bottleneck</a></h3>
            <a href="https://arxiv.org/html/2405.15505v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15505v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hao Yang, Zexu Sun, Hongteng Xu, Xu Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.15505v1 Announce Type: new 
Abstract: As a promising individualized treatment effect (ITE) estimation method, counterfactual regression (CFR) maps individuals' covariates to a latent space and predicts their counterfactual outcomes. However, the selection bias between control and treatment groups often imbalances the two groups' latent distributions and negatively impacts this method's performance. In this study, we revisit counterfactual regression through the lens of information bottleneck and propose a novel learning paradigm called Gromov-Wasserstein information bottleneck (GWIB). In this paradigm, we learn CFR by maximizing the mutual information between covariates' latent representations and outcomes while penalizing the kernelized mutual information between the latent representations and the covariates. We demonstrate that the upper bound of the penalty term can be implemented as a new regularizer consisting of $i)$ the fused Gromov-Wasserstein distance between the latent representations of different groups and $ii)$ the gap between the transport cost generated by the model and the cross-group Gromov-Wasserstein distance between the latent representations and the covariates. GWIB effectively learns the CFR model through alternating optimization, suppressing selection bias while avoiding trivial latent distributions. Experiments on ITE estimation tasks show that GWIB consistently outperforms state-of-the-art CFR methods. To promote the research community, we release our project at https://github.com/peteryang1031/Causal-GWIB.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15505">https://arxiv.org/abs/2405.15505</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses a new approach in terms of individualized treatment effect (ITE) estimation method, counterfactual regression (CFR), which is very relevant to the subtopic 'Causal Representation Learning' and 'Causal Discovery' under your interest in 'Causality and Machine Learning'. Moreover, the paper incorporates information about the utilization of language models in CFR, touching on your interest in new methodologies and foundation models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15673" target="_blank">Consistency of Neural Causal Partial Identification</a></h3>
            
            <p><strong>Authors:</strong> Jiyuan Tan, Jose Blanchet, Vasilis Syrgkanis</p>
            <p><strong>Summary:</strong> arXiv:2405.15673v1 Announce Type: new 
Abstract: Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization the NCM may not be asymptotically consistent. Our results are enabled by new results on the approximability of structural causal models via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15673">https://arxiv.org/abs/2405.15673</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses the topic of Neural Causal Models, which pertains to causal discovery. It dives deep into the mechanisms of model identification and prediction using neural generative models - a fresh perspective that uses the idea of consistency in the field of causality. However, it does not discuss large language models, thus not fully matching all your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15167" target="_blank">ProDAG: Projection-induced variational inference for directed acyclic graphs</a></h3>
            <a href="https://arxiv.org/html/2405.15167v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15167v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ryan Thompson, Edwin V. Bonilla, Robert Kohn</p>
            <p><strong>Summary:</strong> arXiv:2405.15167v1 Announce Type: cross 
Abstract: Directed acyclic graph (DAG) learning is a rapidly expanding field of research. Though the field has witnessed remarkable advances over the past few years, it remains statistically and computationally challenging to learn a single (point estimate) DAG from data, let alone provide uncertainty quantification. Our article addresses the difficult task of quantifying graph uncertainty by developing a variational Bayes inference framework based on novel distributions that have support directly on the space of DAGs. The distributions, which we use to form our prior and variational posterior, are induced by a projection operation, whereby an arbitrary continuous distribution is projected onto the space of sparse weighted acyclic adjacency matrices (matrix representations of DAGs) with probability mass on exact zeros. Though the projection constitutes a combinatorial optimization problem, it is solvable at scale via recently developed techniques that reformulate acyclicity as a continuous constraint. We empirically demonstrate that our method, ProDAG, can deliver accurate inference, and often outperforms existing state-of-the-art alternatives.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15167">https://arxiv.org/abs/2405.15167</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to deal with the development of learning directed acyclic graphs (DAGs), which could be useful in causal discovery and representation learning. It proposes a new method, ProDAG, creating a variational Bayes inference framework beneficial in causal learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15172" target="_blank">Learning the Distribution Map in Reverse Causal Performative Prediction</a></h3>
            <a href="https://arxiv.org/html/2405.15172v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15172v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daniele Bracale, Subha Maity, Moulinath Banerjee, Yuekai Sun</p>
            <p><strong>Summary:</strong> arXiv:2405.15172v1 Announce Type: cross 
Abstract: In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learn the distribution shift. Our method is predicated on a reverse causal model, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to be effective in minimizing the performative prediction risk.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15172">https://arxiv.org/abs/2405.15172</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses the understanding of distribution shifts caused by predictive models, which is a part of causal discovery. It proposes a novel approach to learn these distribution shifts which falls under 'new methods' of causality in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15358" target="_blank">Coordinated Multi-Neighborhood Learning on a Directed Acyclic Graph</a></h3>
            <a href="https://arxiv.org/html/2405.15358v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15358v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Stephen Smith, Qing Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.15358v1 Announce Type: cross 
Abstract: Learning the structure of causal directed acyclic graphs (DAGs) is useful in many areas of machine learning and artificial intelligence, with wide applications. However, in the high-dimensional setting, it is challenging to obtain good empirical and theoretical results without strong and often restrictive assumptions. Additionally, it is questionable whether all of the variables purported to be included in the network are observable. It is of interest then to restrict consideration to a subset of the variables for relevant and reliable inferences. In fact, researchers in various disciplines can usually select a set of target nodes in the network for causal discovery. This paper develops a new constraint-based method for estimating the local structure around multiple user-specified target nodes, enabling coordination in structure learning between neighborhoods. Our method facilitates causal discovery without learning the entire DAG structure. We establish consistency results for our algorithm with respect to the local neighborhood structure of the target nodes in the true graph. Experimental results on synthetic and real-world data show that our algorithm is more accurate in learning the neighborhood structures with much less computational cost than standard methods that estimate the entire DAG. An R package implementing our methods may be accessed at https://github.com/stephenvsmith/CML.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15358">https://arxiv.org/abs/2405.15358</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper centers on learning the structure of causal directed acyclic graphs (DAGs), which aligns with your interest in causal discovery within the 'causality and machine learning' topic. Moreover, the paper proposes a new constraint-based method, which is consistent with your preference for papers that propose new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.03614" target="_blank">Bayesian Vector AutoRegression with Factorised Granger-Causal Graphs</a></h3>
            <a href="https://arxiv.org/html/2402.03614v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.03614v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> He Zhao, Vassili Kitsios, Terence J. O'Kane, Edwin V. Bonilla</p>
            <p><strong>Summary:</strong> arXiv:2402.03614v2 Announce Type: replace 
Abstract: We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data.Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks. Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs. Instead, we propose a new Bayesian VAR model with a hierarchical factorised prior distribution over binary Granger causal graphs, separately from the VAR coefficients. We develop an efficient algorithm to infer the posterior over binary Granger causal graphs. Comprehensive experiments on synthetic, semi-synthetic, and climate data show that our method is more uncertainty aware, has less hyperparameters, and achieves better performance than competing approaches, especially in low-data regimes where there are less observations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.03614">https://arxiv.org/abs/2402.03614</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper may be of interest to you as it discusses the automatic discovery of Granger causal relations from time-series data using a new Bayesian VAR model. While this new method pertains more to causality, it also loosely doubles up on your interest in time-series. However, it doesn't focus on deep learning techniques in detail.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.07259" target="_blank">Towards Bounding Causal Effects under Markov Equivalence</a></h3>
            
            <p><strong>Authors:</strong> Alexis Bellot</p>
            <p><strong>Summary:</strong> arXiv:2311.07259v2 Announce Type: replace-cross 
Abstract: Predicting the effect of unseen interventions is a fundamental research question across the data sciences. It is well established that in general such questions cannot be answered definitively from observational data. This realization has fuelled a growing literature introducing various identifying assumptions, for example in the form of a causal diagram among relevant variables. In practice, this paradigm is still too rigid for many practical applications as it is generally not possible to confidently delineate the true causal diagram. In this paper, we consider the derivation of bounds on causal effects given only observational data. We propose to take as input a less informative structure known as a Partial Ancestral Graph, which represents a Markov equivalence class of causal diagrams and is learnable from data. In this more ``data-driven'' setting, we provide a systematic algorithm to derive bounds on causal effects that exploit the invariant properties of the equivalence class, and that can be computed analytically. We demonstrate our method with synthetic and real data examples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.07259">https://arxiv.org/abs/2311.07259</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses leveraging partial ancestral graphs for bounding causal effects, suggesting a new method for causal discovery. It aligns with your interest in novel approaches to causal representation learning and discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.05639" target="_blank">Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients</a></h3>
            <a href="https://arxiv.org/html/2402.05639v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.05639v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuri Fonseca, Caio Peixoto, Yuri Saporito</p>
            <p><strong>Summary:</strong> arXiv:2402.05639v2 Announce Type: replace-cross 
Abstract: Instrumental variables (IVs) provide a powerful strategy for identifying causal effects in the presence of unobservable confounders. Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality. In a novel direction, we show how to formulate a functional stochastic gradient descent algorithm to tackle NPIV regression by directly minimizing the populational risk. We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method's superior stability and competitive performance relative to current state-of-the-art alternatives. This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise. Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05639">https://arxiv.org/abs/2402.05639</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses instrumental variables which is a method used for causal discovery, addressing particularly nonparametric identification of causal effects. It uses methods like gradient descent, which ties in with machine learning techniques, and the implementation includes the use of neural networks. Therefore, it could be relevant to your interest in causal discovery in the field of machine learning.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 27, 2024 at 21:44:43</div></body></html>