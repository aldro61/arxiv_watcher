
            <html>
            <head>
                <title>Report Generated on May 28, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 28, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15903" target="_blank">UnitNorm: Rethinking Normalization for Transformers in Time Series</a></h3>
            
            <p><strong>Authors:</strong> Nan Huang, Christian K\"ummerle, Xiang Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.15903v1 Announce Type: new 
Abstract: Normalization techniques are crucial for enhancing Transformer models' performance and stability in time series analysis tasks, yet traditional methods like batch and layer normalization often lead to issues such as token shift, attention shift, and sparse attention. We propose UnitNorm, a novel approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing these challenges. Grounded in existing normalization frameworks, UnitNorm's effectiveness is demonstrated across diverse time series analysis tasks, including forecasting, classification, and anomaly detection, via a rigorous evaluation on 6 state-of-the-art models and 10 datasets. Notably, UnitNorm shows superior performance, especially in scenarios requiring robust attention mechanisms and contextual comprehension, evidenced by significant improvements by up to a 1.46 decrease in MSE for forecasting, and a 4.89% increase in accuracy for classification. This work not only calls for a reevaluation of normalization strategies in time series Transformers but also sets a new direction for enhancing model performance and stability. The source code is available at https://anonymous.4open.science/r/UnitNorm-5B84.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15903">https://arxiv.org/abs/2405.15903</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it introduces a new enhancement technique for Transformer models applied to forecasting in time series analysis. It corresponds to your interest in 'New transformer-like models for time series'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16440" target="_blank">MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.16440v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16440v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiuding Cai, Yaoyao Zhu, Xueyao Wang, Yu Yao</p>
            <p><strong>Summary:</strong> arXiv:2405.16440v1 Announce Type: new 
Abstract: In recent years, Transformers have become the de-facto architecture for long-term sequence forecasting (LTSF), but faces challenges such as quadratic complexity and permutation invariant bias. A recent model, Mamba, based on selective state space models (SSMs), has emerged as a competitive alternative to Transformer, offering comparable performance with higher throughput and linear complexity related to sequence length. In this study, we analyze the limitations of current Mamba in LTSF and propose four targeted improvements, leading to MambaTS. We first introduce variable scan along time to arrange the historical information of all the variables together. We suggest that causal convolution in Mamba is not necessary for LTSF and propose the Temporal Mamba Block (TMB). We further incorporate a dropout mechanism for selective parameters of TMB to mitigate model overfitting. Moreover, we tackle the issue of variable scan order sensitivity by introducing variable permutation training. We further propose variable-aware scan along time to dynamically discover variable relationships during training and decode the optimal variable scan order by solving the shortest path visiting all nodes problem during inference. Extensive experiments conducted on eight public datasets demonstrate that MambaTS achieves new state-of-the-art performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16440">https://arxiv.org/abs/2405.16440</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it discusses new deep learning methods for time series forecasting. It proposes a new model called MambaTS which is based on selective state space models (SSMs). This model aims to improve long-term sequence forecasting, a topic that you specified as an interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11463" target="_blank">Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective</a></h3>
            <a href="https://arxiv.org/html/2402.11463v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11463v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang</p>
            <p><strong>Summary:</strong> arXiv:2402.11463v2 Announce Type: replace 
Abstract: In long-term time series forecasting (LTSF) tasks, an increasing number of models have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their dynamical structures. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding and the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11463">https://arxiv.org/abs/2402.11463</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it proposes a new deep learning model, Attraos, for long-term time series forecasting. The approach is grounded in chaos theory and uses a proposed multi-scale dynamic memory unit for encoding historical dynamics structure.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14252" target="_blank">Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.14252v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14252v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qingxiang Liu, Xu Liu, Chenghao Liu, Qingsong Wen, Yuxuan Liang</p>
            <p><strong>Summary:</strong> arXiv:2405.14252v2 Announce Type: replace 
Abstract: Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity. While recent efforts are focused on building such FMs by unlocking the potential of language models (LMs) for time series analysis, dedicated parameters for various downstream forecasting tasks need training, which hinders the common knowledge sharing across domains. Moreover, data owners may hesitate to share the access to local data due to privacy concerns and copyright protection, which makes it impossible to simply construct a FM on cross-domain training instances. To address these issues, we propose Time-FFM, a Federated Foundation Model for Time series forecasting by leveraging pretrained LMs. Specifically, we begin by transforming time series into the modality of text tokens. To bootstrap LMs for time series reasoning, we propose a prompt adaption module to determine domain-customized prompts dynamically instead of artificially. Given the data heterogeneity across domains, we design a personalized federated training strategy by learning global encoders and local prediction heads. Our comprehensive experiments indicate that Time-FFM outperforms state-of-the-arts and promises effective few-shot and zero-shot forecaster.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14252">https://arxiv.org/abs/2405.14252</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests. It introduces a new federated foundation model for time series forecasting, which also leverages language models, and a strategy for personalized federated training. It addresses several of your subtopics: new deep learning methods and foundation models for time series, and the use of data sets to train such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15124" target="_blank">Scaling Law for Time Series Forecasting</a></h3>
            
            <p><strong>Authors:</strong> Jingzhe Shi, Qinwei Ma, Huan Ma, Lei Li</p>
            <p><strong>Summary:</strong> arXiv:2405.15124v2 Announce Type: replace 
Abstract: Scaling law that rewards large datasets, complex models and enhanced data granularity has been observed in various fields of deep learning. Yet, studies on time series forecasting have cast doubt on scaling behaviors of deep learning methods for time series forecasting: while more training data improves performance, more capable models do not always outperform less capable models, and longer input horizons may hurt performance for some models. We propose a theory for scaling law for time series forecasting that can explain these seemingly abnormal behaviors. We take into account the impact of dataset size and model complexity, as well as time series data granularity, particularly focusing on the look-back horizon, an aspect that has been unexplored in previous theories. Furthermore, we empirically evaluate various models using a diverse set of time series forecasting datasets, which (1) verifies the validity of scaling law on dataset size and model complexity within the realm of time series forecasting, and (2) validates our theoretical framework, particularly regarding the influence of look back horizon. We hope our findings may inspire new models targeting time series forecasting datasets of limited size, as well as large foundational datasets and models for time series forecasting in future works.\footnote{Codes for our experiments will be made public at: \url{https://github.com/JingzheShi/ScalingLawForTimeSeriesForecasting}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15124">https://arxiv.org/abs/2405.15124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interest in time series and deep learning. It proposes a new theory for the scaling law for time series forecasting, focusing on dataset size, model complexity, and data granularity (look-back horizon), which aligns with your interest in new deep learning methods and foundation models for time series. Additionally, this paper discusses the use of a diverse set of time series forecasting datasets, which aligns with your interest in datasets to train foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15871" target="_blank">CausalConceptTS: Causal Attributions for Time Series Classification using High Fidelity Diffusion Models</a></h3>
            <a href="https://arxiv.org/html/2405.15871v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15871v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Juan Miguel Lopez Alcaraz, Nils Strodthoff</p>
            <p><strong>Summary:</strong> arXiv:2405.15871v1 Announce Type: new 
Abstract: Despite the excelling performance of machine learning models, understanding the decisions of machine learning models remains a long-standing goal. While commonly used attribution methods in explainable AI attempt to address this issue, they typically rely on associational rather than causal relationships. In this study, within the context of time series classification, we introduce a novel framework to assess the causal effect of concepts, i.e., predefined segments within a time series, on specific classification outcomes. To achieve this, we leverage state-of-the-art diffusion-based generative models to estimate counterfactual outcomes. Our approach compares these causal attributions with closely related associational attributions, both theoretically and empirically. We demonstrate the insights gained by our approach for a diverse set of qualitatively different time series classification tasks. Although causal and associational attributions might often share some similarities, in all cases they differ in important details, underscoring the risks associated with drawing causal conclusions from associational data alone. We believe that the proposed approach is widely applicable also in other domains, particularly where predefined segmentations are available, to shed some light on the limits of associational attributions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15871">https://arxiv.org/abs/2405.15871</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in time series and causality. It introduces a new causal attribution framework for time series classification tasks, which can be critical for time series forecasting. Though it doesn't directly propose a deep learning method, it presents a novel approach to understanding machine learning decisions in time series, which can be informative for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16075" target="_blank">Continuous Temporal Domain Generalization</a></h3>
            <a href="https://arxiv.org/html/2405.16075v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16075v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zekun Cai, Guangji Bai, Renhe Jiang, Xuan Song, Liang Zhao</p>
            <p><strong>Summary:</strong> arXiv:2405.16075v1 Announce Type: new 
Abstract: Temporal Domain Generalization (TDG) addresses the challenge of training predictive models under temporally varying data distributions. Traditional TDG approaches typically focus on domain data collected at fixed, discrete time intervals, which limits their capability to capture the inherent dynamics within continuous-evolving and irregularly-observed temporal domains. To overcome this, this work formalizes the concept of Continuous Temporal Domain Generalization (CTDG), where domain data are derived from continuous times and are collected at arbitrary times. CTDG tackles critical challenges including: 1) Characterizing the continuous dynamics of both data and models, 2) Learning complex high-dimensional nonlinear dynamics, and 3) Optimizing and controlling the generalization across continuous temporal domains. To address them, we propose a Koopman operator-driven continuous temporal domain generalization (Koodos) framework. We formulate the problem within a continuous dynamic system and leverage the Koopman theory to learn the underlying dynamics; the framework is further enhanced with a comprehensive optimization strategy equipped with analysis and control driven by prior knowledge of the dynamics patterns. Extensive experiments demonstrate the effectiveness and efficiency of our approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16075">https://arxiv.org/abs/2405.16075</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it introduces a new method of time series forecasting using Continuous Temporal Domain Generalization taking into account the continuous dynamics of both the data and models. However, it doesn't explicitly mention deep learning or transformer-like methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16083" target="_blank">From Orthogonality to Dependency: Learning Disentangled Representation for Multi-Modal Time-Series Sensing Signals</a></h3>
            <a href="https://arxiv.org/html/2405.16083v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16083v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruichu Cai, Zhifang Jiang, Zijian Li, Weilin Chen, Xuexin Chen, Zhifeng Hao, Yifan Shen, Guangyi Chen, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.16083v1 Announce Type: new 
Abstract: Existing methods for multi-modal time series representation learning aim to disentangle the modality-shared and modality-specific latent variables. Although achieving notable performances on downstream tasks, they usually assume an orthogonal latent space. However, the modality-specific and modality-shared latent variables might be dependent on real-world scenarios. Therefore, we propose a general generation process, where the modality-shared and modality-specific latent variables are dependent, and further develop a \textbf{M}ulti-mod\textbf{A}l \textbf{TE}mporal Disentanglement (\textbf{MATE}) model. Specifically, our \textbf{MATE} model is built on a temporally variational inference architecture with the modality-shared and modality-specific prior networks for the disentanglement of latent variables. Furthermore, we establish identifiability results to show that the extracted representation is disentangled. More specifically, we first achieve the subspace identifiability for modality-shared and modality-specific latent variables by leveraging the pairing of multi-modal data. Then we establish the component-wise identifiability of modality-specific latent variables by employing sufficient changes of historical latent variables. Extensive experimental studies on multi-modal sensors, human activity recognition, and healthcare datasets show a general improvement in different downstream tasks, highlighting the effectiveness of our method in real-world scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16083">https://arxiv.org/abs/2405.16083</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in new multimodal deep learning models for time series. It introduces the MATE model which aims to disentangle modality-shared and modality-specific latent variables in time-series data from multiple sensors. Although it doesn't seem to directly discuss forecasting, the method could potentially enhance forecasting accuracy by better understanding the time-series structure.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16312" target="_blank">Time-SSM: Simplifying and Unifying State Space Models for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.16312v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16312v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaxi Hu, Disen Lan, Ziyu Zhou, Qingsong Wen, Yuxuan Liang</p>
            <p><strong>Summary:</strong> arXiv:2405.16312v1 Announce Type: new 
Abstract: State Space Models (SSMs) have emerged as a potent tool in sequence modeling tasks in recent years. These models approximate continuous systems using a set of basis functions and discretize them to handle input data, making them well-suited for modeling time series data collected at specific frequencies from continuous systems. Despite its potential, the application of SSMs in time series forecasting remains underexplored, with most existing models treating SSMs as a black box for capturing temporal or channel dependencies. To address this gap, this paper proposes a novel theoretical framework termed Dynamic Spectral Operator, offering more intuitive and general guidance on applying SSMs to time series data. Building upon our theory, we introduce Time-SSM, a novel SSM-based foundation model with only one-seventh of the parameters compared to Mamba. Various experiments validate both our theoretical framework and the superior performance of Time-SSM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16312">https://arxiv.org/abs/2405.16312</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it introduces a new State Space Model (SSM) for time series forecasting. However, it doesn't specifically mention multimodal models or transformer-like models which are also your areas of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16456" target="_blank">Dominant Shuffle: A Simple Yet Powerful Data Augmentation for Time-series Prediction</a></h3>
            
            <p><strong>Authors:</strong> Kai Zhao, Zuojie He, Alex Hung, Dan Zeng</p>
            <p><strong>Summary:</strong> arXiv:2405.16456v1 Announce Type: new 
Abstract: Recent studies have suggested frequency-domain Data augmentation (DA) is effec tive for time series prediction. Existing frequency-domain augmentations disturb the original data with various full-spectrum noises, leading to excess domain gap between augmented and original data. Although impressive performance has been achieved in certain cases, frequency-domain DA has yet to be generalized to time series prediction datasets. In this paper, we found that frequency-domain augmentations can be significantly improved by two modifications that limit the perturbations. First, we found that limiting the perturbation to only dominant frequencies significantly outperforms full-spectrum perturbations. Dominant fre quencies represent the main periodicity and trends of the signal and are more important than other frequencies. Second, we found that simply shuffling the dominant frequency components is superior over sophisticated designed random perturbations. Shuffle rearranges the original components (magnitudes and phases) and limits the external noise. With these two modifications, we proposed dominant shuffle, a simple yet effective data augmentation for time series prediction. Our method is very simple yet powerful and can be implemented with just a few lines of code. Extensive experiments with eight datasets and six popular time series models demonstrate that our method consistently improves the baseline performance under various settings and significantly outperforms other DA methods. Code can be accessed at https://kaizhao.net/time-series.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16456">https://arxiv.org/abs/2405.16456</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new data augmentation technique called 'Dominant Shuffle' for time series prediction. It's especially relevant for your interest in new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16504" target="_blank">A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models</a></h3>
            <a href="https://arxiv.org/html/2405.16504v1/extracted/5621307/figs/github.png" target="_blank"><img src="https://arxiv.org/html/2405.16504v1/extracted/5621307/figs/github.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Itamar Zimerman, Ameen Ali, Lior Wolf</p>
            <p><strong>Summary:</strong> arXiv:2405.16504v1 Announce Type: new 
Abstract: Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16504">https://arxiv.org/abs/2405.16504</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Despite not being focused specifically on time-series forecasting, the paper proposes a new unified model for sequence modeling, which may provide valuable insights for time-series data handling. It also mentions the construction of a new type of foundation models, aligning with your interest in new foundation models for time series. However, it does not explicitly mention time series or any of the specific subtopics you're interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16557" target="_blank">Scalable Numerical Embeddings for Multivariate Time Series: Enhancing Healthcare Data Representation Learning</a></h3>
            <a href="https://arxiv.org/html/2405.16557v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16557v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chun-Kai Huang, Yi-Hsien Hsieh, Ta-Jung Chien, Li-Cheng Chien, Shao-Hua Sun, Tung-Hung Su, Jia-Horng Kao, Che Lin</p>
            <p><strong>Summary:</strong> arXiv:2405.16557v1 Announce Type: new 
Abstract: Multivariate time series (MTS) data, when sampled irregularly and asynchronously, often present extensive missing values. Conventional methodologies for MTS analysis tend to rely on temporal embeddings based on timestamps that necessitate subsequent imputations, yet these imputed values frequently deviate substantially from their actual counterparts, thereby compromising prediction accuracy. Furthermore, these methods typically fail to provide robust initial embeddings for values infrequently observed or even absent within the training set, posing significant challenges to model generalizability. In response to these challenges, we propose SCAlable Numerical Embedding (SCANE), a novel framework that treats each feature value as an independent token, effectively bypassing the need for imputation. SCANE regularizes the traits of distinct feature embeddings and enhances representational learning through a scalable embedding mechanism. Coupling SCANE with the Transformer Encoder architecture, we develop the Scalable nUMerical eMbeddIng Transformer (SUMMIT), which is engineered to deliver precise predictive outputs for MTS characterized by prevalent missing entries. Our experimental validation, conducted across three disparate electronic health record (EHR) datasets marked by elevated missing value frequencies, confirms the superior performance of SUMMIT over contemporary state-of-the-art approaches addressing similar challenges. These results substantiate the efficacy of SCANE and SUMMIT, underscoring their potential applicability across a broad spectrum of MTS data analytical tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16557">https://arxiv.org/abs/2405.16557</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces SCAlable Numerical Embedding (SCANE), a new method for improving the representation learning of multivariate time series data with significant missing values, in conjunction with the Transformer Encoder architecture. The distinct focus on time series, deep learning methods, and the introduction of a new model makes this paper relevant to your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16563" target="_blank">Reality Only Happens Once: Single-Path Generalization Bounds for Transformers</a></h3>
            
            <p><strong>Authors:</strong> Yannick Limmer, Anastasis Kratsios, Xuwei Yang, Raeid Saqur, Blanka Horvath</p>
            <p><strong>Summary:</strong> arXiv:2405.16563v1 Announce Type: new 
Abstract: One of the inherent challenges in deploying transformers on time series is that \emph{reality only happens once}; namely, one typically only has access to a single trajectory of the data-generating process comprised of non-i.i.d. observations. We derive non-asymptotic statistical guarantees in this setting through bounds on the \textit{generalization} of a transformer network at a future-time $t$, given that it has been trained using $N\le t$ observations from a single perturbed trajectory of a Markov process. Under the assumption that the Markov process satisfies a log-Sobolev inequality, we obtain a generalization bound which effectively converges at the rate of ${O}(1/\sqrt{N})$. Our bound depends explicitly on the activation function ($\operatorname{Swish}$, $\operatorname{GeLU}$, or $\tanh$ are considered), the number of self-attention heads, depth, width, and norm-bounds defining the transformer architecture. Our bound consists of three components: (I) The first quantifies the gap between the stationary distribution of the data-generating Markov process and its distribution at time $t$, this term converges exponentially to $0$. (II) The next term encodes the complexity of the transformer model and, given enough time, eventually converges to $0$ at the rate ${O}(\log(N)^r/\sqrt{N})$ for any $r>0$. (III) The third term guarantees that the bound holds with probability at least $1$-$\delta$, and converges at a rate of ${O}(\sqrt{\log(1/\delta)}/\sqrt{N})$.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16563">https://arxiv.org/abs/2405.16563</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper explores the use of transformers on time series data, providing new insights and statistical guarantees. However, it does not directly address multimodal learning or foundation datasets, which is why it doesn't get a perfect score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16828" target="_blank">Kernel-based optimally weighted conformal prediction intervals</a></h3>
            <a href="https://arxiv.org/html/2405.16828v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16828v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jonghyeok Lee, Chen Xu, Yao Xie</p>
            <p><strong>Summary:</strong> arXiv:2405.16828v1 Announce Type: new 
Abstract: Conformal prediction has been a popular distribution-free framework for uncertainty quantification. In this paper, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real time-series against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16828">https://arxiv.org/abs/2405.16828</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it introduces a new method (Kernel-based Optimally Weighted Conformal Prediction Intervals) for time series forecasting. However, it does not specifically mention deep learning, foundation models, or multimodal models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16877" target="_blank">Are Self-Attentions Effective for Time Series Forecasting?</a></h3>
            <a href="https://arxiv.org/html/2405.16877v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16877v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dongbin Kim, Jinseong Park, Jaewook Lee, Hoki Kim</p>
            <p><strong>Summary:</strong> arXiv:2405.16877v1 Announce Type: new 
Abstract: Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformer models have dramatically shifted the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift focus from the overall architecture of the Transformer to the effectiveness of self-attentions for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16877">https://arxiv.org/abs/2405.16877</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses a new architecture for time series forecasting, presents a new Transformer-like model (CATS), and demonstrates its effectiveness compared to existing models. However, it focuses on a specific aspect of deep learning (i.e., self-attentions) rather than a broad new method or multimodal models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17051" target="_blank">BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics</a></h3>
            <a href="https://arxiv.org/html/2405.17051v1/extracted/5623275/Idea_main.png" target="_blank"><img src="https://arxiv.org/html/2405.17051v1/extracted/5623275/Idea_main.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hao Wu, Xingjian Shi, Ziyue Huang, Penghao Zhao, Wei Xiong, Jinbao Xue, Yangyu Tao, Xiaomeng Huang, Weiyan Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.17051v1 Announce Type: new 
Abstract: Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose \emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17051">https://arxiv.org/abs/2405.17051</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it proposes a new method called BeamVQ for dealing with space-time forecasting which falls under your time-series interest. While it doesn't explicitly address all your subtopics, the method potentially improves physical alignment of data-driven forecasting models, which may be useful in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17156" target="_blank">The Scaling Law in Stellar Light Curves</a></h3>
            <a href="https://arxiv.org/html/2405.17156v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17156v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jia-Shu Pan, Yuan-Sen Ting, Yang Huang, Jie Yu, Ji-Feng Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.17156v1 Announce Type: cross 
Abstract: Analyzing time series of fluxes from stars, known as stellar light curves, can reveal valuable information about stellar properties. However, most current methods rely on extracting summary statistics, and studies using deep learning have been limited to supervised approaches. In this research, we investigate the scaling law properties that emerge when learning from astronomical time series data using self-supervised techniques. By employing the GPT-2 architecture, we show the learned representation improves as the number of parameters increases from $10^4$ to $10^9$, with no signs of performance plateauing. We demonstrate that a self-supervised Transformer model achieves 3-10 times the sample efficiency compared to the state-of-the-art supervised learning model when inferring the surface gravity of stars as a downstream task. Our research lays the groundwork for analyzing stellar light curves by examining them through large-scale auto-regressive generative models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17156">https://arxiv.org/abs/2405.17156</a></p>
            <p><strong>Category:</strong> astro-ph.IM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes a new approach for analyzing time series data using a self-supervised, transformer-like model which is a GPT-2 architecture and this matches your first interest in deep learning methods for time series and transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2303.16668" target="_blank">Protecting Federated Learning from Extreme Model Poisoning Attacks via Multidimensional Time Series Anomaly Detection</a></h3>
            <a href="https://arxiv.org/html/2303.16668v2/extracted/5623101/img/tdmi-density.png" target="_blank"><img src="https://arxiv.org/html/2303.16668v2/extracted/5623101/img/tdmi-density.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Edoardo Gabrielli, Dimitri Belli, Vittorio Miori, Gabriele Tolomei</p>
            <p><strong>Summary:</strong> arXiv:2303.16668v2 Announce Type: replace 
Abstract: Current defense mechanisms against model poisoning attacks in federated learning (FL) systems have proven effective up to a certain threshold of malicious clients. In this work, we introduce FLANDERS, a novel pre-aggregation filter for FL resilient to large-scale model poisoning attacks, i.e., when malicious clients far exceed legitimate participants. FLANDERS treats the sequence of local models sent by clients in each FL round as a matrix-valued time series. Then, it identifies malicious client updates as outliers in this time series by comparing actual observations with estimates generated by a matrix autoregressive forecasting model maintained by the server. Experiments conducted in several non-iid FL setups show that FLANDERS significantly improves robustness across a wide spectrum of attacks when paired with standard and robust existing aggregation methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2303.16668">https://arxiv.org/abs/2303.16668</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper relates to the 'time series and deep learning' interest, specifically the subtopic of new deep learning methods for time series. It presents a novel approach to handling large-scale model poisoning attacks in federated learning systems using a matrix autoregressive forecasting model. However, it might be more focused on the defense mechanism rather than proposing a new foundation model for time series forecasting or a new multimodal model, hence the score is 4 not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2304.05749" target="_blank">Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation</a></h3>
            
            <p><strong>Authors:</strong> Yuxing Tian, Mingjie Zhu, Jiachi Luo, Song Li</p>
            <p><strong>Summary:</strong> arXiv:2304.05749v2 Announce Type: replace 
Abstract: This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph datasets, the results demonstrate that UmmU can effectively improve the long-term forecasting performance for CTDGNs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2304.05749">https://arxiv.org/abs/2304.05749</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper fits your interest in deep learning methods for time-series forecasting. It presents a new data augmentation method called UmmU for improving long-term forecasting performance in continuous-time dynamic graph networks. However, it does not specifically discuss foundation models, multimodal models, or transformer-like models for time series, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17501" target="_blank">Intensive Care as One Big Sequence Modeling Problem</a></h3>
            
            <p><strong>Authors:</strong> Vadim Liventsev, Tobias Fritz</p>
            <p><strong>Summary:</strong> arXiv:2402.17501v2 Announce Type: replace 
Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17501">https://arxiv.org/abs/2402.17501</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to your interests as it applies sequence modeling, akin to time-series forecasting, to healthcare data. It entails a transformer-like approach to predict future events in a healthcare setting. Additionally, it presents MIMIC-SEQ, a benchmark for sequence modeling, which could be a novel dataset for training foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16049" target="_blank">Improving Demand Forecasting in Open Systems with Cartogram-Enhanced Deep Learning</a></h3>
            <a href="https://arxiv.org/html/2403.16049v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.16049v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sangjoon Park, Yongsung Kwon, Hyungjoon Soh, Mi Jin Lee, Seung-Woo Son</p>
            <p><strong>Summary:</strong> arXiv:2403.16049v2 Announce Type: replace 
Abstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal convolutional graph attention network. Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales. We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16049">https://arxiv.org/abs/2403.16049</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes a new deep learning framework for demand forecasting which is a type of time-series forecasting. It doesn't seem to discuss multimodality or transformers but it is novel in how it leverages cartogram approaches.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.03140" target="_blank">TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning</a></h3>
            <a href="https://arxiv.org/html/2405.03140v2/extracted/5623684/figs/intro2.jpg" target="_blank"><img src="https://arxiv.org/html/2405.03140v2/extracted/5623684/figs/intro2.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiwen Chen, Peijie Qiu, Wenhui Zhu, Huayu Li, Hao Wang, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</p>
            <p><strong>Summary:</strong> arXiv:2405.03140v2 Announce Type: replace 
Abstract: Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC. The code will be available at https://github.com/xiwenc1/TimeMIL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.03140">https://arxiv.org/abs/2405.03140</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper could be of interest to you as it introduces a novel deep learning method, TimeMIL, for time series. Although it does not directly target forecasting, the method aims at dealing with sparsity and locality of patterns in time series data, which are important aspects of time series forecasting. Moreover, it uses a type of transformer-like model.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13796" target="_blank">Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling</a></h3>
            
            <p><strong>Authors:</strong> Wanghan Xu, Fenghua Ling, Wenlong Zhang, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai</p>
            <p><strong>Summary:</strong> arXiv:2405.13796v2 Announce Type: replace 
Abstract: Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which Generalizes weather forecasts to Finer-grained Temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, achieves state-of-the-art performance across multiple lead times and exhibits the capability to generalize 30-minute forecasts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13796">https://arxiv.org/abs/2405.13796</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new AI-based model for fine-grained weather forecasting, which falls under your interest in new deep learning methods for time series forecasting. However, the paper doesn't mention specifics about transformer-like or multimodal models, which are also part of your interests in time series topic. Thus, I would score it a 4 out of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15317" target="_blank">NuwaTS: a Foundation Model Mending Every Incomplete Time Series</a></h3>
            <a href="https://arxiv.org/html/2405.15317v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15317v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, Yuankai Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.15317v2 Announce Type: replace 
Abstract: Time series imputation plays a crucial role in various real-world systems and has been extensively explored. Models for time series imputation often require specialization, necessitating distinct designs for different domains and missing patterns. In this study, we introduce NuwaTS, a framework to repurpose Pre-trained Language Model (PLM) for general time series imputation. Once trained, this model can be applied to imputation tasks on incomplete time series from any domain with any missing patterns. We begin by devising specific embeddings for each sub-series patch of the incomplete time series. These embeddings encapsulate information about the patch itself, the missing data patterns within the patch, and the patch's statistical characteristics. To enhance the model's adaptability to different missing patterns, we propose a contrastive learning approach to make representations of the same patch more similar across different missing patterns. By combining this contrastive loss with the missing data imputation task, we train PLMs to obtain a one-for-all imputation model. Furthermore, we utilize a plug-and-play layer-wise fine-tuning approach to train domain-specific models. Experimental results demonstrate that leveraging a dataset of over seventeen million time series from diverse domains, we obtain a one-for-all imputation model which outperforms existing domain-specific models across various datasets and missing patterns. Additionally, we find that NuwaTS can be generalized to other time series tasks such as forecasting. Our codes are available at https://github.com/Chengyui/NuwaTS.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15317">https://arxiv.org/abs/2405.15317</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces NuwaTS, a framework for general time series imputation leveraging Pre-trained Language Model (PLM). Besides, it emphasizes on the adaptability to different missing patterns and its generalization to time series tasks such as forecasting which are part of your interest in new deep learning methods and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.08362" target="_blank">Mean-Field Microcanonical Gradient Descent</a></h3>
            <a href="https://arxiv.org/html/2403.08362v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.08362v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Marcus H\"aggbom, Morten Karlsmark, Joakim And\'en</p>
            <p><strong>Summary:</strong> arXiv:2403.08362v2 Announce Type: replace-cross 
Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.08362">https://arxiv.org/abs/2403.08362</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper presents a new model, the mean-field microcanonical gradient descent, which is used in the context of financial time series, therefore making it relevant to your interest in new methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12234" target="_blank">Joint Prediction Regions for time-series models</a></h3>
            
            <p><strong>Authors:</strong> Eshant English</p>
            <p><strong>Summary:</strong> arXiv:2405.12234v2 Announce Type: replace-cross 
Abstract: Machine Learning algorithms are notorious for providing point predictions but not prediction intervals. There are many applications where one requires confidence in predictions and prediction intervals. Stringing together, these intervals give rise to joint prediction regions with the desired significance level. It is an easy task to compute Joint Prediction regions (JPR) when the data is IID. However, the task becomes overly difficult when JPR is needed for time series because of the dependence between the observations. This project aims to implement Wolf and Wunderli's method for constructing JPRs and compare it with other methods (e.g. NP heuristic, Joint Marginals). The method under study is based on bootstrapping and is applied to different datasets (Min Temp, Sunspots), using different predictors (e.g. ARIMA and LSTM). One challenge of applying the method under study is to derive prediction standard errors for models, it cannot be obtained analytically. A novel method to estimate prediction standard error for different predictors is also devised. Finally, the method is applied to a synthetic dataset to find empirical averages and empirical widths and the results from the Wolf and Wunderli paper are consolidated. The experimental results show a narrowing of width with strong predictors like neural nets, widening of width with increasing forecast horizon H and decreasing significance level alpha, controlling the width with parameter k in K-FWE, and loss of information using Joint Marginals.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12234">https://arxiv.org/abs/2405.12234</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper fits into the category of 'time-series' as it discusses methods for prediction in time series models, which could align with your interest in forecasting. It also explores the use of deep learning methods like LSTM for predictions. However, because it doesn't specifically address foundation models, multimodal models, or transformer-like models, the relevance might be slightly less than perfect.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16510" target="_blank">Meta-Task Planning for Language Agents</a></h3>
            <a href="https://arxiv.org/html/2405.16510v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16510v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang, Yong Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.16510v1 Announce Type: cross 
Abstract: The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim40\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\sim14\%$, showing the immense potential of integrating LLM with multi-agent systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16510">https://arxiv.org/abs/2405.16510</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper aligns well with your interests in large-language-model-based agents. It specifically discusses 'Meta-Task Planning,' a normative method for collaborative LLM-based multi-agent systems that simplifies complex task planning by performing it in a hierarchical manner, which might help you in developing LLMs for software and browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.16843" target="_blank">Do LLM Agents Have Regret? A Case Study in Online Learning and Games</a></h3>
            
            <p><strong>Authors:</strong> Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.16843v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.16843">https://arxiv.org/abs/2403.16843</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it discusses the performance metrics of Large Language Models' (LLMs) decision-making capabilities, specifically in online learning and game theory settings. These insights could extend to autonomous agents used for computer automation and software control. Furthermore, the paper presents novel methods to train these models for effective interactions, aligning with your preference for papers that propose new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09220" target="_blank">ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.09220v2/extracted/5622500/Fig/C1/Graph.png" target="_blank"><img src="https://arxiv.org/html/2405.09220v2/extracted/5622500/Fig/C1/Graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.09220v2 Announce Type: replace 
Abstract: In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks." Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis. Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent. Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09220">https://arxiv.org/abs/2405.09220</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it studies the planning capability of transformer-based language models which aligns with your interest in agents based on large-language models. The research theoretically and empirically investigates how these models can be used for planning tasks, a concept which could potentially be extended to controlling software or automating computer tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15877" target="_blank">Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications</a></h3>
            <a href="https://arxiv.org/html/2405.15877v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15877v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yang Li, Changsheng Zhao, Hyungtak Lee, Ernie Chang, Yangyang Shi, Vikas Chandra</p>
            <p><strong>Summary:</strong> arXiv:2405.15877v1 Announce Type: new 
Abstract: Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15877">https://arxiv.org/abs/2405.15877</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Even though this paper does not directly mention agent behavior, it discusses a method for compressing large language models. This could be beneficial in scenarios where large language models are employed to control software or web browsers as it might enhance their speed and efficiency.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15943" target="_blank">Transformers represent belief state geometry in their residual stream</a></h3>
            <a href="https://arxiv.org/html/2405.15943v1/extracted/5619344/mess3_overview.png" target="_blank"><img src="https://arxiv.org/html/2405.15943v1/extracted/5619344/mess3_overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Adam S. Shai, Sarah E. Marzen, Lucas Teixeira, Alexander Gietelink Oldenziel, Paul M. Riechers</p>
            <p><strong>Summary:</strong> arXiv:2405.15943v1 Announce Type: new 
Abstract: What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a framework connecting the structure of training data to the computational structure and representations that transformers use to carry out their behavior.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15943">https://arxiv.org/abs/2405.15943</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though not explicitly about large language model-based agents, the paper studies how transformers (often used in large language models) learn and represents belief states during training. This could be pertinent to better understanding the capabilities of such models in control/automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16265" target="_blank">MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</a></h3>
            <a href="https://arxiv.org/html/2405.16265v1/extracted/5620426/figs/archi.png" target="_blank"><img src="https://arxiv.org/html/2405.16265v1/extracted/5620426/figs/archi.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Boxing Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.16265v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method called MindStar (M*), which treats reasoning tasks as search problems. This method utilizes a step-wise reasoning approach to navigate the tree space. To enhance search efficiency, we propose two tree-search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16265">https://arxiv.org/abs/2405.16265</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper describes a method (MindStar) that enhances the reasoning abilities of Large Language Models (LLMs) specifically for maths. This could be relevant to your interest in LLM-agents, as it delves into how to improve and control the capabilities of such models. However, it doesn't specifically discuss controlling software or web browsers, hence it isn't a perfect match to your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16325" target="_blank">SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.16325v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16325v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, Maryam Mehri Dehnavi</p>
            <p><strong>Summary:</strong> arXiv:2405.16325v1 Announce Type: new 
Abstract: We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference. In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass. SLoPe accelerates the training and inference of models with billions of parameters up to $1.14\times$ and $1.34\times$ respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to $0.77\times$ and $0.51\times$ for training and inference respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16325">https://arxiv.org/abs/2405.16325</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> As the method described is focused on improving the efficiency of large language models (LLMs), you might find it relevant. Even though it doesn't explicitly mention using LLMs to control software or web browsers, the improvements in accuracy and speed discussed could be very beneficial in those applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16406" target="_blank">SpinQuant -- LLM quantization with learned rotations</a></h3>
            
            <p><strong>Authors:</strong> Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort</p>
            <p><strong>Summary:</strong> arXiv:2405.16406v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Recent findings suggest that rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures, and find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant that optimizes (or learns) the rotation matrices with Cayley optimization on a small validation set. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-2 7B/LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by 30.2%/34.1% relative to QuaRot.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16406">https://arxiv.org/abs/2405.16406</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the quantization of Large Language Models - a process that can be crucial to manipulating and controlling such models in various applications including software and web browsers control. However, it doesn't speak directly to the specific application of large language models in controlling software and web browsers, so the score is not the highest possible.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16436" target="_blank">Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer</a></h3>
            <a href="https://arxiv.org/html/2405.16436v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16436v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, Zhaoran Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.16436v1 Announce Type: new 
Abstract: Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. Here, the reward penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16436">https://arxiv.org/abs/2405.16436</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to your interests as it discusses about aligning large language models (LLM) with human preferences, which is related to controlling software and automation. However, it does not specifically mention controlling web browsers or software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16444" target="_blank">CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion</a></h3>
            <a href="https://arxiv.org/html/2405.16444v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16444v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang</p>
            <p><strong>Summary:</strong> arXiv:2405.16444v1 Announce Type: new 
Abstract: Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized.
  This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16444">https://arxiv.org/abs/2405.16444</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses Large Language Models and improving their performance, but it doesn't involve using these models to directly perform actions like controlling software or web browsers as per your interest, hence did not get a perfect score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16450" target="_blank">Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search</a></h3>
            <a href="https://arxiv.org/html/2405.16450v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16450v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun Chen, Shao-Hua Sun</p>
            <p><strong>Summary:</strong> arXiv:2405.16450v1 Announce Type: new 
Abstract: Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to consistently improve the programs. Experimental results in the Karel domain demonstrate the superior effectiveness and efficiency of our LLM-GS framework. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16450">https://arxiv.org/abs/2405.16450</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper discusses leveraging large language models (LLMs) to enhance the efficiency of program-environment interactions. It's directly applicable to computer automation using LLMs and describes a proposed novel method, LLM-Guided Search, that could be significant in the context of controlling software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16581" target="_blank">On Bits and Bandits: Quantifying the Regret-Information Trade-off</a></h3>
            <a href="https://arxiv.org/html/2405.16581v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16581v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor</p>
            <p><strong>Summary:</strong> arXiv:2405.16581v1 Announce Type: new 
Abstract: In interactive decision-making tasks, information can be acquired by direct interactions, through receiving indirect feedback, and from external knowledgeable sources. We examine the trade-off between the information an agent accumulates and the regret it suffers. We show that information from external sources, measured in bits, can be traded off for regret, measured in reward. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We then generalize a variety of interactive decision-making tasks with external information to a new setting. Using this setting, we introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. These lower bounds also prove the near-optimality of Thompson sampling for Bayesian problems. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16581">https://arxiv.org/abs/2405.16581</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper pertains to the use of large language models in question-answering tasks, which aligns with your interest in agents based on large-language models. However, the specific use case of controlling software or web browsers is not clearly outlined, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16587" target="_blank">Cost-Effective Online Multi-LLM Selection with Versatile Reward Models</a></h3>
            <a href="https://arxiv.org/html/2405.16587v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16587v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, John C. S. Lui</p>
            <p><strong>Summary:</strong> arXiv:2405.16587v1 Announce Type: new 
Abstract: With the rapid advancement of large language models (LLMs), the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs. To tackle these challenges, we introduce the \textit{C2MAB-V}, a \underline{C}ost-effective \underline{C}ombinatorial \underline{M}ulti-armed \underline{B}andit with \underline{V}ersatile reward models for optimal LLM selection and usage. This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration. With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models. Based on our designed online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks. The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback. Theoretically, we prove that \textit{C2MAB-V} offers strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases. Empirically, we show that \textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16587">https://arxiv.org/abs/2405.16587</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the employment of multiple large language models for optimally completing tasks which aligns with your interest in 'agents based on large-language models'. However, the focus on cost efficiency and bandits may not be directly related to your specified subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16671" target="_blank">Mixture of Experts Using Tensor Products</a></h3>
            <a href="https://arxiv.org/html/2405.16671v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16671v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhan Su, Fengran Mo, Prayag Tiwari, Benyou Wang, Jian-Yun Nie, Jakob Grue Simonsen</p>
            <p><strong>Summary:</strong> arXiv:2405.16671v1 Announce Type: new 
Abstract: In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously. However, the training signals from different tasks can interfere with one another, potentially leading to \textit{negative transfer}. To mitigate this, we investigate if modular language models can facilitate positive transfer and systematic generalization. Specifically, we propose a novel modular language model (\texttt{TensorPoly}), that balances parameter efficiency with nuanced routing methods. For \textit{modules}, we reparameterize Low-Rank Adaptation (\texttt{LoRA}) by employing an entangled tensor through the use of tensor product operations and name the resulting approach \texttt{TLoRA}. For \textit{routing function}, we tailor two innovative routing functions according to the granularity: \texttt{TensorPoly-I} which directs to each rank within the entangled tensor while \texttt{TensorPoly-II} offers a finer-grained routing approach targeting each order of the entangled tensor. The experimental results from the multi-task T0-benchmark demonstrate that: 1) all modular LMs surpass the corresponding dense approaches, highlighting the potential of modular language models to mitigate negative inference in multi-task learning and deliver superior outcomes. 2) \texttt{TensorPoly-I} achieves higher parameter efficiency in adaptation and outperforms other modular LMs, which shows the potential of our approach in multi-task transfer learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16671">https://arxiv.org/abs/2405.16671</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the potential of modular language models, in multi-task learning which aligns with your interest in large language models, although it doesn't directly focus on control of softwares or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16755" target="_blank">CHESS: Contextual Harnessing for Efficient SQL Synthesis</a></h3>
            <a href="https://arxiv.org/html/2405.16755v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16755v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, Amin Saberi</p>
            <p><strong>Summary:</strong> arXiv:2405.16755v1 Announce Type: new 
Abstract: Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions. We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries. To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases. Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size. Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance. Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16755">https://arxiv.org/abs/2405.16755</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interest in large-language models controlling software. It demonstrates the use of large language models to convert natural language questions into SQL queries, thus displaying an application of computers' automation through language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16820" target="_blank">Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings</a></h3>
            <a href="https://arxiv.org/html/2405.16820v1/extracted/5622437/teaser.png" target="_blank"><img src="https://arxiv.org/html/2405.16820v1/extracted/5622437/teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Robert Wolfe, Isaac Slaughter, Bin Han, Bingbing Wen, Yiwei Yang, Lucas Rosenblatt, Bernease Herman, Eva Brown, Zening Qu, Nic Weber, Bill Howe</p>
            <p><strong>Summary:</strong> arXiv:2405.16820v1 Announce Type: new 
Abstract: The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.
  We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16820">https://arxiv.org/abs/2405.16820</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the feasibility of using smaller, open-weight models in place of larger language models like GPT-4-Turbo to control software for different tasks. This fits well with your interest in using large language models for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16833" target="_blank">Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16833v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16833v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang</p>
            <p><strong>Summary:</strong> arXiv:2405.16833v1 Announce Type: new 
Abstract: While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16833">https://arxiv.org/abs/2405.16833</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses fine-tuning methods for large language models (LLMs) which falls within your interest in agents based on LLMs. Although it isn't explicitly about controlling software or web browsers, the insight gained from the efficient use of LoRA on LLMs can be valuable in the mentioned context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16845" target="_blank">On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability</a></h3>
            
            <p><strong>Authors:</strong> Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, Chongxuan Li</p>
            <p><strong>Summary:</strong> arXiv:2405.16845v1 Announce Type: new 
Abstract: Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks. Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL. Namely, the forward pass of the trained transformer is equivalent to optimizing an inner objective function in-context. However, whether the practical non-convex training dynamics will converge to the ideal mesa-optimizer is still unclear. Towards filling this gap, we investigate the non-convex dynamics of a one-layer linear causal self-attention model autoregressively trained by gradient flow, where the sequences are generated by an AR process $x_{t+1} = W x_t$. First, under a certain condition of data distribution, we prove that an autoregressively trained transformer learns $W$ by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context. It then applies the learned $\widehat{W}$ for next-token prediction, thereby verifying the mesa-optimization hypothesis. Next, under the same data conditions, we explore the capability limitations of the obtained mesa-optimizer. We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution. Besides, we conduct exploratory analyses beyond the first data condition and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem. Finally, our simulation results verify the theoretical results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16845">https://arxiv.org/abs/2405.16845</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on transformer models, related to your interest in large language models. It discusses about the in-context learning ability of transformers to address downstream tasks, which could be applicable in computer automation scenarios. Its emphasis on non-convex dynamics and training methods adds a new angle to the large language model-based agents' discussion.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17088" target="_blank">Phase Transitions in the Output Distribution of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.17088v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17088v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Julian Arnold, Flemming Holtorf, Frank Sch\"afer, Niels L\"orch</p>
            <p><strong>Summary:</strong> arXiv:2405.17088v1 Announce Type: new 
Abstract: In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17088">https://arxiv.org/abs/2405.17088</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the behavior of large language models, which aligns with your interests in large language models as agents. While it does not directly discuss the use of these models to control software or web browsers, the exploration into their behavior can provide greater insight into their capabilities, which may be applicable to your interest in the areas of software and browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17216" target="_blank">Autoformalizing Euclidean Geometry</a></h3>
            <a href="https://arxiv.org/html/2405.17216v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17216v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si</p>
            <p><strong>Summary:</strong> arXiv:2405.17216v1 Announce Type: new 
Abstract: Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable. Euclidean geometry provides an interesting and controllable domain for studying autoformalization. In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs). One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. We also provide automatic semantic evaluation for autoformalized theorem statements. We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant. Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems. The data and code are available at https://github.com/loganrjmurphy/LeanEuclid.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17216">https://arxiv.org/abs/2405.17216</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper primarily details the utilization of Large Language Models (LLMs) for an autoformalization task. Although it does not directly cover the use of LLMs for controlling software or web browsers, it demonstrates the intricacies of using LLMs to autoformalize Euclidean geometry theorems, which could potentially extend to automation in computer programs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17233" target="_blank">CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.17233v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17233v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoyu Wang, Bei Liu, Hang Shao, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian</p>
            <p><strong>Summary:</strong> arXiv:2405.17233v1 Announce Type: new 
Abstract: Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17233">https://arxiv.org/abs/2405.17233</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models. It discusses CLAQ, a new form of LLM quantization that could have various applications to controlling software and web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17258" target="_blank">$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning</a></h3>
            <a href="https://arxiv.org/html/2405.17258v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17258v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Runqian Wang, Soumya Ghosh, David Cox, Diego Antognini, Aude Oliva, Rogerio Feris, Leonid Karlinsky</p>
            <p><strong>Summary:</strong> arXiv:2405.17258v1 Announce Type: new 
Abstract: Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17258">https://arxiv.org/abs/2405.17258</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper proposes a new method for transferable parameter efficient finetuning (Trans-LoRA) which utilizes large language models in creating synthetic data generators. Thus, it lies within your interest of using large language models for automation and control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17346" target="_blank">Prompt Optimization with Human Feedback</a></h3>
            <a href="https://arxiv.org/html/2405.17346v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17346v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</p>
            <p><strong>Summary:</strong> arXiv:2405.17346v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization. However, previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances. Our code can be found at \url{https://github.com/xqlin98/APOHF}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17346">https://arxiv.org/abs/2405.17346</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models, specifically in the subtopic of using large language models to control software. It talks about optimizing the input prompts that are given to these models and improving their performance based on human feedback. Though it may not directly deal with computer automation or web browser control, understanding how to optimize prompts can enhance the applicability of these models in various control scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17382" target="_blank">ReMoDetect: Reward Models Recognize Aligned LLM's Generations</a></h3>
            <a href="https://arxiv.org/html/2405.17382v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17382v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hyunseok Lee, Jihoon Tack, Jinwoo Shin</p>
            <p><strong>Summary:</strong> arXiv:2405.17382v1 Announce Type: new 
Abstract: The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results. Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17382">https://arxiv.org/abs/2405.17382</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'Agents based on large-language models' as it discusses and proposes reward models for detecting Large Language Models' text generations. Although it does not directly address the subjects of using LLMs to control software or web browsers, the concepts and methods it discusses could potentially be useful in that context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15793" target="_blank">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></h3>
            
            <p><strong>Authors:</strong> John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press</p>
            <p><strong>Summary:</strong> arXiv:2405.15793v1 Announce Type: cross 
Abstract: Software engineering is a challenging task requiring proficiency in both code generation and interacting with computers. In this paper, we introduce SWE-agent, an autonomous system that uses a language model to interact with a computer to solve software engineering tasks. We show that a custom-built agent-computer interface (ACI) greatly enhances the ability of an agent to create and edit code files, navigate entire repositories and execute programs. On SWE-bench, SWE-agent is able to solve 12.5% of issues, compared to the previous best of 3.8% achieved with retrieval-augmented generation (RAG). We explore how ACI design impacts an agent's behavior and performance, and provide insights on effective design.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15793">https://arxiv.org/abs/2405.15793</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'Agents based on large-language models' as it introduces SWE-agent, an autonomous system that uses a language model to interact with a computer to solve software engineering tasks. Although it doesn't specifically mention controlling web browsers, the concept of using large language models for computer automation is a major theme in this paper.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15821" target="_blank">Reinforcing Language Agents via Policy Optimization with Action Decomposition</a></h3>
            <a href="https://arxiv.org/html/2405.15821v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15821v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Muning Wen, Ziyu Wan, Weinan Zhang, Jun Wang, Ying Wen</p>
            <p><strong>Summary:</strong> arXiv:2405.15821v1 Announce Type: cross 
Abstract: Language models as intelligent agents push the boundaries of sequential decision-making agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments. However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human's prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces. Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15821">https://arxiv.org/abs/2405.15821</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper involves the use of Language models as intelligent agents for decision making. Although it does not specifically mention controlling software or web browsers, it addresses reinforcement learning to align agents' knowledge with environments. This represents a foundational method related to your interests in agent-based language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15842" target="_blank">Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation</a></h3>
            <a href="https://arxiv.org/html/2405.15842v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15842v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Boyuan Chen, Mingzhi Zhu, Brendan Dolan-Gavitt, Muhammad Shafique, Siddharth Garg</p>
            <p><strong>Summary:</strong> arXiv:2405.15842v1 Announce Type: cross 
Abstract: The rapid development of large language models (LLMs) has led to significant advancements in code completion tasks. While larger models have higher accuracy, they also cost much more to run. Meanwhile, model cascading has been proven effective to conserve computational resources while enhancing accuracy in LLMs on natural language generation tasks. It generates output with the smallest model in a set, and only queries the larger models when it fails to meet predefined quality criteria. However, this strategy has not been used in code completion tasks, primarily because assessing the quality of code completions differs substantially from assessing natural language, where the former relies heavily on the functional correctness. To address this, we propose letting each model generate and execute a set of test cases for their solutions, and use the test results as the cascading threshold. We show that our model cascading strategy reduces computational costs while increases accuracy compared to generating the output with a single model. We also introduce a heuristics to determine the optimal combination of the number of solutions, test cases, and test lines each model should generate, based on the budget. Compared to speculative decoding, our method works on black-box models, having the same level of cost-accuracy trade-off, yet providing much more choices based on the server's budget. Ours is the first work to optimize cost-accuracy trade-off for LLM code generation with model cascading.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15842">https://arxiv.org/abs/2405.15842</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models for code generation, which is a part of software control, one of your interests. It describes an optimization method for large language model based code generation, which falls under the general theme of computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16057" target="_blank">SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16057v1/" target="_blank"><img src="https://arxiv.org/html/2405.16057v1/" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xudong Lu, Aojun Zhou, Yuhui Xu, Renrui Zhang, Peng Gao, Hongsheng Li</p>
            <p><strong>Summary:</strong> arXiv:2405.16057v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16057">https://arxiv.org/abs/2405.16057</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to large language models and how to optimize their parameters for fine-tuning purposes, which is crucial for deploying these models in software and web browser control scenarios in an efficient manner. However, it doesn't directly address the application in control scenarios, hence the score is not the maximum.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16122" target="_blank">Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars</a></h3>
            
            <p><strong>Authors:</strong> Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</p>
            <p><strong>Summary:</strong> arXiv:2405.16122v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of in-context learning (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the instruction, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods. To address these challenges, we propose a novel method named EASE, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars while accounting for exemplar ordering. Our EASE can efficiently find an ordered set of exemplars that performs well for all test queries from a given task, thereby eliminating test-time computation. Importantly, EASE can be readily extended to jointly optimize both the exemplars and the instruction. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of EASE over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest. Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16122">https://arxiv.org/abs/2405.16122</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though the paper does not directly relate to control of software or web browsers, it does extend the capabilities of Large Language Models (LLMs) by proposing a novel method, EASE, for the optimization of prompt exemplars. This enhances the performance of LLMs, and hence, could have implications for LLM-controlled systems and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16236" target="_blank">A statistical framework for weak-to-strong generalization</a></h3>
            
            <p><strong>Authors:</strong> Seamus Somerstep, Felipe Maia Polo, Moulinath Banerjee, Ya'acov Ritov, Mikhail Yurochkin, Yuekai Sun</p>
            <p><strong>Summary:</strong> arXiv:2405.16236v1 Announce Type: cross 
Abstract: Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach with three LLM alignment tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16236">https://arxiv.org/abs/2405.16236</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses a method of aligning Large Language Models (LLMs) with human feedback, implying potential uses in the control of software or automation. However, it doesn't directly address controlling specific applications (such as web browsers) which is why it doesn't earn a full score of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16282" target="_blank">Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16282v1/extracted/5619955/Figures/Flowchart.png" target="_blank"><img src="https://arxiv.org/html/2405.16282v1/extracted/5619955/Figures/Flowchart.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, Ali Emami</p>
            <p><strong>Summary:</strong> arXiv:2405.16282v1 Announce Type: cross 
Abstract: As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16282">https://arxiv.org/abs/2405.16282</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper looks at the self-assessed confidence of Large Language Models. Although it does not directly address control of software or web browsers, it is informative for understanding the reliability of LLM's outputs, which is pertinent to their use in any form of automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16388" target="_blank">Multi-Reference Preference Optimization for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16388v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16388v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hung Le, Quan Tran, Dung Nguyen, Kien Do, Saloni Mittal, Kelechi Ogueji, Svetha Venkatesh</p>
            <p><strong>Summary:</strong> arXiv:2405.16388v1 Announce Type: cross 
Abstract: How can Large Language Models (LLMs) be aligned with human intentions and values? A typical solution is to gather human preference on model outputs and finetune the LLMs accordingly while ensuring that updates do not deviate too far from a reference model. Recent approaches, such as direct preference optimization (DPO), have eliminated the need for unstable and sluggish reinforcement learning optimization by introducing close-formed supervised losses. However, a significant limitation of the current approach is its design for a single reference model only, neglecting to leverage the collective power of numerous pretrained LLMs. To overcome this limitation, we introduce a novel closed-form formulation for direct preference optimization using multiple reference models. The resulting algorithm, Multi-Reference Preference Optimization (MRPO), leverages broader prior knowledge from diverse reference models, substantially enhancing preference learning capabilities compared to the single-reference DPO. Our experiments demonstrate that LLMs finetuned with MRPO generalize better in various preference data, regardless of data scarcity or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior performance in several downstream natural language processing tasks such as GSM8K and TruthfulQA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16388">https://arxiv.org/abs/2405.16388</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents Multi-Reference Preference Optimization (MRPO), a method that improves the alignment of Large Language Models (LLM) with human intentions. Although the work does not directly discuss controlling software or browsers, the finetuning of LLMs is often a fundamental step towards building automated systems or creating agents based on these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16413" target="_blank">Augmented Risk Prediction for the Onset of Alzheimer's Disease from Electronic Health Records with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16413v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16413v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiankun Wang, Sumyeong Ahn, Taykhoom Dalal, Xiaodan Zhang, Weishen Pan, Qiannan Zhang, Bin Chen, Hiroko H. Dodge, Fei Wang, Jiayu Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.16413v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel. Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \& Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16413">https://arxiv.org/abs/2405.16413</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not directly propose a new method, it does demonstrate the use of large language models with machine learning for prediction tasks by leveraging electronic health records data, which aligns with your interest in applications of large-language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16455" target="_blank">On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization</a></h3>
            <a href="https://arxiv.org/html/2405.16455v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16455v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, Weijie J. Su</p>
            <p><strong>Summary:</strong> arXiv:2405.16455v1 Announce Type: cross 
Abstract: Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF) -- the predominant approach for aligning LLMs with human preferences through a reward model -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16455">https://arxiv.org/abs/2405.16455</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the alignment of large language models with human preferences, which might influence how large language models could be used for controlling software or automations. However, it does not explicitly mention controlling software, web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16661" target="_blank">RLSF: Reinforcement Learning via Symbolic Feedback</a></h3>
            <a href="https://arxiv.org/html/2405.16661v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16661v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Piyush Jha, Prithwish Jana, Arnav Arora, Vijay Ganesh</p>
            <p><strong>Summary:</strong> arXiv:2405.16661v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have had a dramatic impact on various sub-fields of AI, most notably on natural language understanding tasks. However, there is widespread agreement that the logical reasoning capabilities of contemporary LLMs are, at best, fragmentary (i.e., may work well on some problem instances but fail dramatically on others). While traditional LLM fine-tuning approaches (e.g., those that use human feedback) do address this problem to some degree, they suffer from many issues, including unsound black-box reward models, difficulties in collecting preference data, and sparse scalar reward values.
  To address these challenges, we propose a new training/fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which is aimed at enhancing the reasoning capabilities of LLMs. In the RLSF setting, the LLM that is being trained/fine-tuned is considered as the RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, algebra systems). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. The ability of RLSF-based training/fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above. Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on two different applications, namely, program synthesis from natural language pseudo-code to programming language (C++) and solving the Game of 24.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16661">https://arxiv.org/abs/2405.16661</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the enhancement of reasoning capabilities in Large Language Models (LLMs) through a training/fine-tuning paradigm, which potentially contributes to the development of LLM-based intelligent agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16700" target="_blank">Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs</a></h3>
            <a href="https://arxiv.org/html/2405.16700v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16700v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mustafa Shukor, Matthieu Cord</p>
            <p><strong>Summary:</strong> arXiv:2405.16700v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the building block for Large Multimodal Models, yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation aiming to understand their generalization beyond textual inputs.
  Findings. Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations, and complete translation to textual tokens does not exist. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite being different, (3) perceptual and textual tokens are implicitly aligned inside LLMs, we call this the implicit multimodal alignment (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture.
  Implications. (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations, revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (e.g. in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork that works well across a wide range of multimodal tasks. Paper code: https://github.com/mshukor/ima-lmms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16700">https://arxiv.org/abs/2405.16700</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interests, particularly in the area of large-language models and their application for understanding multimodal inputs. However, it does not directly cover controlling software or web browsers using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16714" target="_blank">Crafting Interpretable Embeddings by Asking LLMs Questions</a></h3>
            <a href="https://arxiv.org/html/2405.16714v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16714v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vinamra Benara, Chandan Singh, John X. Morris, Richard Antonello, Ion Stoica, Alexander G. Huth, Jianfeng Gao</p>
            <p><strong>Summary:</strong> arXiv:2405.16714v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.
  We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16714">https://arxiv.org/abs/2405.16714</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models (LLMs) and provides insights into how LLMs are used for interpreting fMRI voxel responses to language stimuli. However, it does not discuss using LLMs to control software or web browsers, which are topics of particular interest to you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16766" target="_blank">Reframing the Relationship in Out-of-Distribution Detection</a></h3>
            <a href="https://arxiv.org/html/2405.16766v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16766v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> YuXiao Lee, Xiaofeng Cao</p>
            <p><strong>Summary:</strong> arXiv:2405.16766v1 Announce Type: cross 
Abstract: The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation. The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence. Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability. Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process. These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships. This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs. Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16766">https://arxiv.org/abs/2405.16766</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper outlines the use of large language models as intermediary agents in the task of Out-of-Distribution detection. This directly relates to your interest on agents based on large language models, specifically on using large language models for task automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16802" target="_blank">AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation</a></h3>
            <a href="https://arxiv.org/html/2405.16802v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16802v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo</p>
            <p><strong>Summary:</strong> arXiv:2405.16802v1 Announce Type: cross 
Abstract: In this work, we propose a novel method named \textbf{Auto}mated Process Labeling via \textbf{C}onfidence \textbf{V}ariation (\textbf{\textsc{AutoCV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. Our approach begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the confidence variations learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. Subsequently, we demonstrate that the process annotations generated by \textsc{AutoCV} can improve the accuracy of the verification model in selecting the correct answer from multiple outputs generated by LLMs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of \textsc{AutoCV} is available at \url{https://github.com/rookie-joe/AUTOCV}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16802">https://arxiv.org/abs/2405.16802</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. It proposes a new method, 'AutoCV', to enhance the reasoning capabilities of LLMs, contributing to their ability to effectively control software and perform computer automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17039" target="_blank">BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation</a></h3>
            <a href="https://arxiv.org/html/2405.17039v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17039v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengxing Jia, Pengyuan Wang, Ziniu Li, Yi-Chen Li, Zhilong Zhang, Nan Tang, Yang Yu</p>
            <p><strong>Summary:</strong> arXiv:2405.17039v1 Announce Type: cross 
Abstract: Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task. This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17039">https://arxiv.org/abs/2405.17039</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a new model for controllable language generation using LLMs, which is focused on your interest in the third category. While it doesn't specifically mention controlling software or web browsers, it could be applicable to broader automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17044" target="_blank">Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models</a></h3>
            <a href="https://arxiv.org/html/2405.17044v1/extracted/5623066/Fig1_SemNet.png" target="_blank"><img src="https://arxiv.org/html/2405.17044v1/extracted/5623066/Fig1_SemNet.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xuemei Gu, Mario Krenn</p>
            <p><strong>Summary:</strong> arXiv:2405.17044v1 Announce Type: cross 
Abstract: Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4. We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest. This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph. We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas. This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17044">https://arxiv.org/abs/2405.17044</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not deal specifically with your subtopics of controlling software or web browsers, it does present the implementation of a Large Language Model (GPT-4) for the generation of research ideas. It could therefore be relevant to your interests in the broader application of Large Language Models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17047" target="_blank">Interpretable Robotic Manipulation from Language</a></h3>
            <a href="https://arxiv.org/html/2405.17047v1/extracted/5623259/img/Voxelization.png" target="_blank"><img src="https://arxiv.org/html/2405.17047v1/extracted/5623259/img/Voxelization.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Boyuan Zheng, Jianlong Zhou, Fang Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.17047v1 Announce Type: cross 
Abstract: Humans naturally employ linguistic instructions to convey knowledge, a process that proves significantly more complex for machines, especially within the context of multitask robotic manipulation environments. Natural language, moreover, serves as the primary medium through which humans acquire new knowledge, presenting a potentially intuitive bridge for translating concepts understandable by humans into formats that can be learned by machines. In pursuit of facilitating this integration, we introduce an explainable behavior cloning agent, named Ex-PERACT, specifically designed for manipulation tasks. This agent is distinguished by its hierarchical structure, which incorporates natural language to enhance the learning process. At the top level, the model is tasked with learning a discrete skill code, while at the bottom level, the policy network translates the problem into a voxelized grid and maps the discretized actions to voxel grids. We evaluate our method across eight challenging manipulation tasks utilizing the RLBench benchmark, demonstrating that Ex-PERACT not only achieves competitive policy performance but also effectively bridges the gap between human instructions and machine execution in complex environments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17047">https://arxiv.org/abs/2405.17047</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper dives into the concept of using natural language to control robotic manipulation tasks, a form of software control. It's not using large language models, which is why it does not achieve a 5, but it's on a topic that could be insightful for your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17202" target="_blank">Efficient multi-prompt evaluation of LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.17202v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17202v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Felipe Maia Polo, Ronald Xu, Lucas Weber, M\'irian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin</p>
            <p><strong>Summary:</strong> arXiv:2405.17202v1 Announce Type: cross 
Abstract: Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17202">https://arxiv.org/abs/2405.17202</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to your interests given that it deals with efficient evaluation of Large Language Models (LLMs), which is essential in developing LLM-based agents. However, it does not specifically cover the control of the software or web browsers with the use of LLMs, thus a score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17234" target="_blank">Benchmarking General Purpose In-Context Learning</a></h3>
            <a href="https://arxiv.org/html/2405.17234v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17234v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fan Wang, Chuan Lin, Yang Cao, Yu Kang</p>
            <p><strong>Summary:</strong> arXiv:2405.17234v1 Announce Type: cross 
Abstract: In-context learning (ICL) capabilities is becoming increasingly appealing towards building general intelligence. Taking this concept one step further, we draw a parallel to humans and many animals, who inherit primarily learning capabilities but refine their memory and acquire diverse skills and knowledge through extensive lifelong experiences. This parallel inspires our approach to general purpose in-context learning (GPICL). This paper introduces two lightweight but insightful benchmarks specifically crafted to train and evaluate GPICL functionalities. Each benchmark encompasses a wide range of diverse tasks characterized by generation and interaction, minimal transferable knowledge, and long-term dependency. These features present significant challenges for models that primarily rely on context or interactions to enhance their proficiency. We hope that these benchmarks will not only advance research in GPICL but also contribute significantly to the broader field of general intelligence.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17234">https://arxiv.org/abs/2405.17234</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large-language models and their use in software control. It looks at the concept of in-context learning in general intelligence and its application in diverse tasks, which is central to computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17264" target="_blank">On the Noise Robustness of In-Context Learning for Text Generation</a></h3>
            <a href="https://arxiv.org/html/2405.17264v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17264v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei</p>
            <p><strong>Summary:</strong> arXiv:2405.17264v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the "noisy" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17264">https://arxiv.org/abs/2405.17264</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses the impact of 'noisy' data on large language models (LLMs), proposing a method to counteract this. Even though it doesn't discuss the control of software or web browser, it provides useful insights about language models, which falls under your interest in agents based on large-language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17372" target="_blank">BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction</a></h3>
            <a href="https://arxiv.org/html/2405.17372v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17372v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, Chun Jason Xue</p>
            <p><strong>Summary:</strong> arXiv:2405.17372v1 Announce Type: cross 
Abstract: Simulating realistic interactions among traffic agents is crucial for efficiently validating the safety of autonomous driving systems. Existing leading simulators primarily use an encoder-decoder structure to encode the historical trajectories for future simulation. However, such a paradigm complicates the model architecture, and the manual separation of history and future trajectories leads to low data utilization. To address these challenges, we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a decoder-only, autoregressive architecture designed to simulate the sequential motion of multiple agents. Crucially, our approach discards the traditional separation between "history" and "future," treating each time step as the "current" one, resulting in a simpler, more parameter- and data-efficient design that scales seamlessly with data and computation. Additionally, we introduce the Next-Patch Prediction Paradigm (NP3), which enables models to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. BehaviorGPT ranks first across several metrics on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in multi-agent and agent-map interactions. We outperformed state-of-the-art models with a realism score of 0.741 and improved the minADE metric to 1.540, with an approximately 91.6% reduction in model parameters.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17372">https://arxiv.org/abs/2405.17372</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper proposes Behavior Generative Pre-trained Transformers (BehaviorGPT), a new approach to controlling agents based on large language models. Although it specifically targets autonomous driving, the underlying method can potentially be applied to general software or web-browser control, meeting your interest in new methods for agents based on large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2304.06701" target="_blank">Learning Personalized Decision Support Policies</a></h3>
            <a href="https://arxiv.org/html/2304.06701v2/extracted/5623746/figures/figure1_v7.png" target="_blank"><img src="https://arxiv.org/html/2304.06701v2/extracted/5623746/figures/figure1_v7.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Umang Bhatt, Valerie Chen, Katherine M. Collins, Parameswaran Kamalaruban, Emma Kallina, Adrian Weller, Ameet Talwalkar</p>
            <p><strong>Summary:</strong> arXiv:2304.06701v2 Announce Type: replace 
Abstract: Individual human decision-makers may benefit from different forms of support to improve decision outcomes, but when each form of support will yield better outcomes? In this work, we posit that personalizing access to decision support tools can be an effective mechanism for instantiating the appropriate use of AI assistance. Specifically, we propose the general problem of learning a decision support policy that, for a given input, chooses which form of support to provide to decision-makers for whom we initially have no prior information. We develop $\texttt{Modiste}$, an interactive tool to learn personalized decision support policies. $\texttt{Modiste}$ leverages stochastic contextual bandit techniques to personalize a decision support policy for each decision-maker and supports extensions to the multi-objective setting to account for auxiliary objectives like the cost of support. We find that personalized policies outperform offline policies, and, in the cost-aware setting, reduce the incurred cost with minimal degradation to performance. Our experiments include various realistic forms of support (e.g., expert consensus and predictions from a large language model) on vision and language tasks. Our human subject experiments validate our computational experiments, demonstrating that personalization can yield benefits in practice for real users, who interact with $\texttt{Modiste}$.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2304.06701">https://arxiv.org/abs/2304.06701</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses an interactive tool, Modiste, that uses a large language model for personalized decision support. Despite not directly addressing software or browser control, it contributes to the understanding of how large language models can be utilized in personalized systems, which is a crucial aspect of agent-based models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.16338" target="_blank">Think Before You Act: Decision Transformers with Working Memory</a></h3>
            <a href="https://arxiv.org/html/2305.16338v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.16338v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, Jie Fu</p>
            <p><strong>Summary:</strong> arXiv:2305.16338v2 Announce Type: replace 
Abstract: Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.16338">https://arxiv.org/abs/2305.16338</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper deals with decision-making agents based on transformers, which is a subfield of large language models. The described working memory module methodology could potentially be applied to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.11489" target="_blank">Text2Reward: Reward Shaping with Language Models for Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2309.11489v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2309.11489v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</p>
            <p><strong>Summary:</strong> arXiv:2309.11489v3 Announce Type: replace 
Abstract: Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/ .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.11489">https://arxiv.org/abs/2309.11489</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, 'Text2Reward: Reward Shaping with Language Models for Reinforcement Learning' should be relevant to your interests because it discusses using large language models for reinforcement learning and control purposes, directly addressing your subtopics of using large language models to control softwares and for computer automation. Although it's not directly about controlling web browsers or proposing new methods, it does present an innovative approach to generating reward functions using large language models, making it a potentially valuable source of information.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.05204" target="_blank">Towards Optimizing with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.05204v3/extracted/5622858/figs/framework.png" target="_blank"><img src="https://arxiv.org/html/2310.05204v3/extracted/5622858/figs/framework.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, Shou-De Lin</p>
            <p><strong>Summary:</strong> arXiv:2310.05204v3 Announce Type: replace 
Abstract: In this work, we conduct an assessment of the optimization capabilities of LLMs across various tasks and data sizes. Each of these tasks corresponds to unique optimization domains, and LLMs are required to execute these tasks with interactive prompting. That is, in each optimization step, the LLM generates new solutions from the past generated solutions with their values, and then the new solutions are evaluated and considered in the next optimization step. Additionally, we introduce three distinct metrics for a comprehensive assessment of task performance from various perspectives. These metrics offer the advantage of being applicable for evaluating LLM performance across a broad spectrum of optimization tasks and are less sensitive to variations in test samples. By applying these metrics, we observe that LLMs exhibit strong optimization capabilities when dealing with small-sized samples. However, their performance is significantly influenced by factors like data size and values, underscoring the importance of further research in the domain of optimization tasks for LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.05204">https://arxiv.org/abs/2310.05204</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly addresses the optimization capabilities of Large Language Models (LLMs), a major topic of interest for you. It provides an assessment of LLMs' performance across various tasks and interactive prompting, which could be applicable to your interest in computer automation using LLMs. However, it doesn't specifically address controlling software or web browsers, hence the 4 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.06387" target="_blank">Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations</a></h3>
            <a href="https://arxiv.org/html/2310.06387v3/extracted/5617951/figs/illustration.png" target="_blank"><img src="https://arxiv.org/html/2310.06387v3/extracted/5617951/figs/illustration.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, Yisen Wang</p>
            <p><strong>Summary:</strong> arXiv:2310.06387v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable success in various tasks, yet their safety and the risk of generating harmful content remain pressing concerns. In this paper, we delve into the potential of In-Context Learning (ICL) to modulate the alignment of LLMs. Specifically, we propose the In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs, and the In-Context Defense (ICD) which bolsters model resilience through examples that demonstrate refusal to produce harmful responses. We offer theoretical insights to elucidate how a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs. Through extensive experiments, we demonstrate the efficacy of ICA and ICD in respectively elevating and mitigating the success rates of jailbreaking prompts. Our findings illuminate the profound influence of ICL on LLM behavior, opening new avenues for improving the safety of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.06387">https://arxiv.org/abs/2310.06387</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the usage of Large Language Models in potential tasks related to the alignment and safety of LLMs. Though it doesn't directly involve controlling software or web browsers, it discusses relevant points around safety alignments that could be influential to the application of LLMs in computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08566" target="_blank">Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining</a></h3>
            
            <p><strong>Authors:</strong> Licong Lin, Yu Bai, Song Mei</p>
            <p><strong>Summary:</strong> arXiv:2310.08566v2 Announce Type: replace 
Abstract: Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08566">https://arxiv.org/abs/2310.08566</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the use of large transformers in reinforcement learning and decision making processes, which is closely connected with your interest in the use of large language models for controlling software and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.18491" target="_blank">Publicly-Detectable Watermarking for Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.18491v2/extracted/5601625/Figures/generate_quality_scores.png" target="_blank"><img src="https://arxiv.org/html/2310.18491v2/extracted/5601625/Figures/generate_quality_scores.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang</p>
            <p><strong>Summary:</strong> arXiv:2310.18491v2 Announce Type: replace 
Abstract: We present a highly detectable, trustless watermarking scheme for LLMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LLM output using rejection sampling. We prove that our scheme is cryptographically correct, sound, and distortion-free. We make novel uses of error-correction techniques to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and make empirical measurements over open models in the 2.7B to 70B parameter range. Our experiments suggest that our formal claims are met in practice.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.18491">https://arxiv.org/abs/2310.18491</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though the paper does not directly discuss controlling software or web browsers, it does seem to significantly deal with the functionality of large language models, thus it could still offer valuable insights to your interests in this domain.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.06353" target="_blank">Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes</a></h3>
            <a href="https://arxiv.org/html/2312.06353v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.06353v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng</p>
            <p><strong>Summary:</strong> arXiv:2312.06353v5 Announce Type: replace 
Abstract: Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioritizing perturbations with greater impact on model accuracy. Experiments across six scenarios with various LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in both communication efficiency and new task generalization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.06353">https://arxiv.org/abs/2312.06353</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though the paper does not align with all subtopics in your list, it contains relevant information on fine-tuning large language models which can be applicable in controlling software or automating tasks. The paper introduces a new method (FedKSeed), which could potentially be extended or applied in the areas of your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.05821" target="_blank">Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents</a></h3>
            <a href="https://arxiv.org/html/2401.05821v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.05821v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Quentin Delfosse, Sebastian Sztwiertnia, Mark Rothermel, Wolfgang Stammer, Kristian Kersting</p>
            <p><strong>Summary:</strong> arXiv:2401.05821v3 Announce Type: replace 
Abstract: Goal misalignment, reward sparsity and difficult credit assignment are only a few of the many issues that make it difficult for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep neural networks impedes the inclusion of domain experts for inspecting the model and revising suboptimal policies. To this end, we introduce *Successive Concept Bottleneck Agents* (SCoBots), that integrate consecutive concept bottleneck (CB) layers. In contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks. Our experimental results provide evidence of SCoBots' competitive performances, but also of their potential for domain experts to understand and regularize their behavior. Among other things, SCoBots enabled us to identify a previously unknown misalignment problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus result in more human-aligned RL agents. Our code is available at https://github.com/k4ntz/SCoBots .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.05821">https://arxiv.org/abs/2401.05821</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to your interests as it discusses in depth about reinforcement learning agents and their alignment - this could be extended to understand how large language models may be used to control software or web browsers. However, it does not focus exclusively on large language models, so the score is not the maximum.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00854" target="_blank">SymbolicAI: A framework for logic-based approaches combining generative models and solvers</a></h3>
            <a href="https://arxiv.org/html/2402.00854v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.00854v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter</p>
            <p><strong>Summary:</strong> arXiv:2402.00854v3 Announce Type: replace 
Abstract: We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the "Vector Embedding for Relational Trajectory Evaluation through Cross-similarity", or VERTEX score for short. The framework codebase and benchmark are linked below.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00854">https://arxiv.org/abs/2402.00854</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines SymbolicAI, a framework that integrates Large Language Models (LLMs) for accomplishing tasks based on language instructions. The relevance of this paper to your interests lies in the fact that it discusses how LLMs can be used for automation and controlling software, key elements of your specified subtopics within the 'llm-agents' category.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02314" target="_blank">Selecting Large Language Model to Fine-tune via Rectified Scaling Law</a></h3>
            <a href="https://arxiv.org/html/2402.02314v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02314v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang</p>
            <p><strong>Summary:</strong> arXiv:2402.02314v2 Announce Type: replace 
Abstract: The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02314">https://arxiv.org/abs/2402.02314</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a method for selecting large language models for fine-tuning, which relates to your interest in the used of large language models for tasks like computer automation, even though it's not directly tied to the specified tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.05445" target="_blank">Accurate LoRA-Finetuning Quantization of LLMs via Information Retention</a></h3>
            
            <p><strong>Authors:</strong> Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno</p>
            <p><strong>Summary:</strong> arXiv:2402.05445v2 Announce Type: replace 
Abstract: The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05445">https://arxiv.org/abs/2402.05445</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores information retention in quantized large language models (LLMs), which is relevant to your interest in agents based on LLMs. While it may not directly mention software control or browser control, the aspects discussed can be vital for computer automation using LLM.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.05643" target="_blank">Improving Token-Based World Models with Parallel Observation Prediction</a></h3>
            <a href="https://arxiv.org/html/2402.05643v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.05643v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor</p>
            <p><strong>Summary:</strong> arXiv:2402.05643v3 Announce Type: replace 
Abstract: Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at \url{https://github.com/leor-c/REM}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05643">https://arxiv.org/abs/2402.05643</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a novel method for training agents that use large language models to understand and act upon their environment. This aligns with your interest in agents based on large-language models, although it does not specifically target controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.05785" target="_blank">Limits of Transformer Language Models on Learning to Compose Algorithms</a></h3>
            
            <p><strong>Authors:</strong> Jonathan Thomm, Aleksandar Terzic, Giacomo Camposampiero, Michael Hersche, Bernhard Sch\"olkopf, Abbas Rahimi</p>
            <p><strong>Summary:</strong> arXiv:2402.05785v3 Announce Type: replace 
Abstract: We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.05785">https://arxiv.org/abs/2402.05785</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models, focusing on their application in discrete tasks and code generation. Settings like code generation could very well include scenarios such as controlling software. However, it does not explicitly discuss software or browser control, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.07630" target="_blank">G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</a></h3>
            <a href="https://arxiv.org/html/2402.07630v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.07630v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</p>
            <p><strong>Summary:</strong> arXiv:2402.07630v3 Announce Type: replace 
Abstract: Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote{Our codes and datasets are available at: \url{https://github.com/XiaoxinHe/G-Retriever}}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.07630">https://arxiv.org/abs/2402.07630</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems to be highly relevant to your interest in large-language models, more specifically in using them to ''chat with your graph.' The method allows for textual replies and pertinent graph parts highlighting, which hints to an advanced form of software control. However, there isn't a direct connection to using large language models for web browsers control or computer automation, which is why I rated it a 4, not a 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11867" target="_blank">LoRA Training in the NTK Regime has No Spurious Local Minima</a></h3>
            <a href="https://arxiv.org/html/2402.11867v2/extracted/5624093/1.png" target="_blank"><img src="https://arxiv.org/html/2402.11867v2/extracted/5624093/1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Uijeong Jang, Jason D. Lee, Ernest K. Ryu</p>
            <p><strong>Summary:</strong> arXiv:2402.11867v2 Announce Type: replace 
Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11867">https://arxiv.org/abs/2402.11867</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper on LoRA fine-tuning of LLMs does not explicitly describe an agent-based application, it gives insights into the fine-tuning process that could be applied when using large language models to control software or automate computers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.13516" target="_blank">ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.13516v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.13516v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun</p>
            <p><strong>Summary:</strong> arXiv:2402.13516v3 Announce Type: replace 
Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, activation sparsity has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces a simple and effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity while maintaining comparable performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along the multi-stage sine curves. This can enhance activation sparsity and mitigate performance degradation by avoiding radical shifts in activation distributions. With ProSparse, we obtain high sparsity of 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size MiniCPM-1B, respectively, achieving comparable performance to their original Swish-activated versions. These present the most sparsely activated models among open-source LLaMA versions and competitive end-size models, considerably surpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference acceleration experiments further demonstrate the significant practical acceleration potential of LLMs with higher activation sparsity, obtaining up to 4.52$\times$ inference speedup.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.13516">https://arxiv.org/abs/2402.13516</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces a new method named 'ProSparse' to enhance the efficiency of large language models, which could be beneficial in controlling software and automating computers. However, the paper doesn't directly address controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15390" target="_blank">Explorations of Self-Repair in Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.15390v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.15390v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cody Rushing, Neel Nanda</p>
            <p><strong>Summary:</strong> arXiv:2402.15390v2 Announce Type: replace 
Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15390">https://arxiv.org/abs/2402.15390</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large-language model based agents, as it discusses the concept of 'self-repair' in large language models, which could potentially be important when developing autonomous agents to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.16354" target="_blank">Language-guided Skill Learning with Temporal Variational Inference</a></h3>
            <a href="https://arxiv.org/html/2402.16354v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.16354v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris, Nicolas Le Roux, Marc-Alexandre C\^ot\'e, Xingdi Yuan</p>
            <p><strong>Summary:</strong> arXiv:2402.16354v2 Announce Type: replace 
Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.16354">https://arxiv.org/abs/2402.16354</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems to be highly relevant to your interest in 'agents based on large-language models'. The study makes use of large language models (LLMs) for skill discovery. Although it's not specifically about controlling software or web browsers, it does have an application in a simulation environment which indicates relevance to computer automation. However, it gets a 4 instead of 5 because it doesn't explicitly meet your exact subtopic interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.16902" target="_blank">PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA</a></h3>
            <a href="https://arxiv.org/html/2402.16902v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.16902v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu</p>
            <p><strong>Summary:</strong> arXiv:2402.16902v2 Announce Type: replace 
Abstract: With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA retains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.16902">https://arxiv.org/abs/2402.16902</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper introduces a new parameter-efficient finetuning method for large language models, which could be relevant to your interest in automation using LLMs. However, the paper does not directly address controlling software or web browsers using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.17919" target="_blank">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a></h3>
            <a href="https://arxiv.org/html/2403.17919v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.17919v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.17919v3 Announce Type: replace 
Abstract: The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freezes most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17919">https://arxiv.org/abs/2403.17919</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it provides new method (LISA) for memory efficient fine-tuning of Large Language Models (LLMs). Although it does not directly address LLM control of software or web browsers, the principles and techniques could potentially apply. However, no direct application to your specific subtopics mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00675" target="_blank">Self-Play Preference Optimization for Language Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.00675v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00675v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, Quanquan Gu</p>
            <p><strong>Summary:</strong> arXiv:2405.00675v3 Announce Type: replace 
Abstract: Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \textit{Self-play Probabilistic Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys a theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53\% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00675">https://arxiv.org/abs/2405.00675</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large-language models. It discusses a new method for language model alignment, which could potentially be useful for controlling software or automation. However, it doesn't directly discuss controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.12288" target="_blank">The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</a></h3>
            <a href="https://arxiv.org/html/2309.12288v4/extracted/5621782/figures/Experiment_2_explainer.png" target="_blank"><img src="https://arxiv.org/html/2309.12288v4/extracted/5621782/figures/Experiment_2_explainer.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans</p>
            <p><strong>Summary:</strong> arXiv:2309.12288v4 Announce Type: replace-cross 
Abstract: We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if "A is B" occurs, "B is A" is more likely to occur. It is worth noting, however, that if "A is B" appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of Abyssal Melodies" and showing that they fail to correctly answer "Who composed Abyssal Melodies?". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter.
  Code available at: https://github.com/lukasberglund/reversal_curse.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.12288">https://arxiv.org/abs/2309.12288</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not explicitly propose a new use for LLMs in controlling software or web browsers, it provides significant insight into their limitations and potential areas for improvement, particularly in the context of understanding and producing relational information. This can be a stepping stone while researching effective means of automating computer systems using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.02124" target="_blank">Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</a></h3>
            
            <p><strong>Authors:</strong> Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin Deng</p>
            <p><strong>Summary:</strong> arXiv:2310.02124v3 Announce Type: replace-cross 
Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\footnote{\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.02124">https://arxiv.org/abs/2310.02124</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models used as agents. It delves into the collaboration mechanisms of these models, thereby providing insight into their application for software control and automation. The topic doesn't strictly cover web browser or specific software control, hence not a perfect 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.17972" target="_blank">Self-Infilling Code Generation</a></h3>
            
            <p><strong>Authors:</strong> Lin Zheng, Jianbo Yuan, Zhi Zhang, Hongxia Yang, Lingpeng Kong</p>
            <p><strong>Summary:</strong> arXiv:2311.17972v3 Announce Type: replace-cross 
Abstract: This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our proposed decoding process is effective in enhancing both regularity and quality across several code generation benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.17972">https://arxiv.org/abs/2311.17972</a></p>
            <p><strong>Category:</strong> cs.PL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'llm-agents' category as it focuses on enhancing the controlling mechanism of code generation through large language models. It may, therefore, provide key insights for software and web browsers control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.07887" target="_blank">Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models</a></h3>
            <a href="https://arxiv.org/html/2312.07887v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.07887v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Junhao Zheng, Shengjie Qiu, Qianli Ma</p>
            <p><strong>Summary:</strong> arXiv:2312.07887v4 Announce Type: replace-cross 
Abstract: Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods and requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.07887">https://arxiv.org/abs/2312.07887</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it focuses on incorporating pre-trained language models (a type of large language model) for incremental learning. Although not specifically about control of software or browsers, it includes strategies about preventing forgetting in language models that could be applicable to controlling automated agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.10897" target="_blank">Generalized Category Discovery with Large Language Models in the Loop</a></h3>
            <a href="https://arxiv.org/html/2312.10897v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.10897v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenbin An, Wenkai Shi, Feng Tian, Haonan Lin, QianYing Wang, Yaqiang Wu, Mingxiang Cai, Luyan Wang, Yan Chen, Haiping Zhu, Ping Chen</p>
            <p><strong>Summary:</strong> arXiv:2312.10897v2 Announce Type: replace-cross 
Abstract: Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at https://github.com/Lackel/LOOP.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.10897">https://arxiv.org/abs/2312.10897</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls in the category of using large language models in different areas, which is one of your interests. It presents a novel framework that involves large language models in the training loop, which helps in categorizing the data. This may not directly show usage of large language models for controlling software or browsers, but it provides an innovative approach as to how large language models can improve tasks related to categorization and predicting clusters. As you're interested in newer methods, this might be worth your time.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.05268" target="_blank">AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning</a></h3>
            <a href="https://arxiv.org/html/2401.05268v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.05268v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2401.05268v4 Announce Type: replace-cross 
Abstract: Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others. Code will be available at https://github.com/zjunlp/AutoAct.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.05268">https://arxiv.org/abs/2401.05268</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper fits your interest in 'Agents based on large-language models'. Specifically, it offers a novel perspective on utilising large language models for QA systems, which may have broader implications for computer automation with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08699" target="_blank">Unsupervised Evaluation of Code LLMs with Round-Trip Correctness</a></h3>
            <a href="https://arxiv.org/html/2402.08699v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08699v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin</p>
            <p><strong>Summary:</strong> arXiv:2402.08699v2 Announce Type: replace-cross 
Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08699">https://arxiv.org/abs/2402.08699</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your interests in the use of large language models in the realm of software control and automation. It proposes a new method to evaluate code large language models (LLMs), and while it does not directly deal with LLMs controlling software, the evaluation method mentioned might be of use to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12146" target="_blank">Enabling Weak LLMs to Judge Response Reliability via Meta Ranking</a></h3>
            <a href="https://arxiv.org/html/2402.12146v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12146v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2402.12146v2 Announce Type: replace-cross 
Abstract: Despite the strong performance of large language models (LLMs) across a wide range of tasks, they still have reliability issues. Previous studies indicate that strong LLMs like GPT-4-turbo excel in evaluating the reliability of responses from LLMs, but face efficiency and local deployment issues. Thus, to enable weak LLMs to effectively assess the reliability of LLM responses, we propose a novel cross-query-comparison-based method called $\textit{Meta Ranking}$ (MR). Unlike previous few-shot methods that solely based on in-context learning capabilities in LLMs, MR assesses reliability by pairwisely ranking the target query-response pair with multiple reference query-response pairs. We found that MR is highly effective in error detection for LLM responses, where weak LLMs, such as Phi-2, could surpass strong baselines like GPT-3.5-turbo, requiring only five reference samples and significantly improving efficiency. We further demonstrate that MR can enhance strong LLMs' performance in two practical applications: model cascading and instruction tuning. In model cascading, we combine open- and closed-source LLMs to achieve performance comparable to GPT-4-turbo with lower costs. In instruction tuning, we use MR for iterative training data filtering, significantly reducing data processing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with fewer training tokens. These results underscore the high potential of MR in both efficiency and effectiveness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12146">https://arxiv.org/abs/2402.12146</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses large language models and focuses on their application in assessing the reliability of responses. Even though it doesn't talk directly about controlling software or web browsers, it can provide valuable insights into improving the efficiency and effectiveness of large language models, which may be relevant for developing intelligent agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12847" target="_blank">Instruction-tuned Language Models are Better Knowledge Learners</a></h3>
            <a href="https://arxiv.org/html/2402.12847v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12847v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer</p>
            <p><strong>Summary:</strong> arXiv:2402.12847v2 Announce Type: replace-cross 
Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12847">https://arxiv.org/abs/2402.12847</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper proposes a new method (i.e., pre-instruction-tuning) to improve the knowledge absorption of LLMs from new documents, which might be useful for controlling software and web browsers in an effective manner. However, it does not explicitly mention automation or control tasks directly.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14807" target="_blank">A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</a></h3>
            <a href="https://arxiv.org/html/2402.14807v3/extracted/5622044/figures/teaser.png" target="_blank"><img src="https://arxiv.org/html/2402.14807v3/extracted/5622044/figures/teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe</p>
            <p><strong>Summary:</strong> arXiv:2402.14807v3 Announce Type: replace-cross 
Abstract: Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations. We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14807">https://arxiv.org/abs/2402.14807</a></p>
            <p><strong>Category:</strong> cs.MA</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper showcases the use of Large Language Models in a dynamic decision-making setting, indicating a method for controlling software using these models. However, it does not explicitly discuss web browsers or general computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14992" target="_blank">tinyBenchmarks: evaluating LLMs with fewer examples</a></h3>
            <a href="https://arxiv.org/html/2402.14992v2/extracted/5622037/mmlu_leaderboard_performance_individual.png" target="_blank"><img src="https://arxiv.org/html/2402.14992v2/extracted/5622037/mmlu_leaderboard_performance_individual.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin</p>
            <p><strong>Summary:</strong> arXiv:2402.14992v2 Announce Type: replace-cross 
Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14992">https://arxiv.org/abs/2402.14992</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents areas relevant to your interests on large language models, specifically evaluation strategies on their performance. Although it does not directly discuss controlling software or web browsers, the exploration of how to assess LLMs accurately and efficiently might be beneficial to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00254" target="_blank">RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation</a></h3>
            <a href="https://arxiv.org/html/2405.00254v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00254v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chanwoo Park, Mingyang Liu, Dingwen Kong, Kaiqing Zhang, Asuman Ozdaglar</p>
            <p><strong>Summary:</strong> arXiv:2405.00254v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00254">https://arxiv.org/abs/2405.00254</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents new methods for dealing with heterogeneous feedback when fine-tuning large-language models for alignment with human values. It seems related to your interest in large language model-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08755" target="_blank">Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach</a></h3>
            <a href="https://arxiv.org/html/2405.08755v2/extracted/5621091/figures/image_1.png" target="_blank"><img src="https://arxiv.org/html/2405.08755v2/extracted/5621091/figures/image_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Syed Mhamudul Hasan, Alaa M. Alotaibi, Sajedul Talukder, Abdur R. Shahid</p>
            <p><strong>Summary:</strong> arXiv:2405.08755v2 Announce Type: replace-cross 
Abstract: With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of Large Language Models (LLMs), represents a promising paradigm for enhancing cybersecurity on resource-constrained edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08755">https://arxiv.org/abs/2405.08755</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it talks about the deployment of large language models onto edge devices (a form of software control), with an interesting application in cybersecurity.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.09719" target="_blank">Spectral Editing of Activations for Large Language Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.09719v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.09719v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen</p>
            <p><strong>Summary:</strong> arXiv:2405.09719v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.09719">https://arxiv.org/abs/2405.09719</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it focuses on improving the behavior of large language models, which is fundamental when controlling software or automating tasks. The proposed method could potentially be used to enhance the effectiveness of large language models in such roles.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11299" target="_blank">The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving</a></h3>
            <a href="https://arxiv.org/html/2405.11299v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11299v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, Xusheng Chen, Yizhou Shan</p>
            <p><strong>Summary:</strong> arXiv:2405.11299v2 Announce Type: replace-cross 
Abstract: We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11299">https://arxiv.org/abs/2405.11299</a></p>
            <p><strong>Category:</strong> cs.DB</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models serving, specifically in improving serving context length which could potentially apply to controlling software and web browsers. However, the paper seems to focus more on the optimization and performance of LLMs rather than proposing new methods for using them in specific applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12213" target="_blank">Octo: An Open-Source Generalist Robot Policy</a></h3>
            <a href="https://arxiv.org/html/2405.12213v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12213v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong>  Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine</p>
            <p><strong>Summary:</strong> arXiv:2405.12213v2 Announce Type: replace-cross 
Abstract: Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12213">https://arxiv.org/abs/2405.12213</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language model agents. It discusses the use of a transformer-based policy (which could be considered a large language model) for controlling robots. Although it's not specifically about controlling software or web browsers, it might give some insight on programming automated tasks using language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12856" target="_blank">LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</a></h3>
            
            <p><strong>Authors:</strong> James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud</p>
            <p><strong>Summary:</strong> arXiv:2405.12856v2 Announce Type: replace-cross 
Abstract: Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12856">https://arxiv.org/abs/2405.12856</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it discusses the utilization of Large Language Models (LLMs) in making probabilistic predictions, which aligns with your interest in using LLMs to control software and for computer automation. However, it doesn't specifically address control of web browsers or software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14314" target="_blank">Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</a></h3>
            <a href="https://arxiv.org/html/2405.14314v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14314v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li</p>
            <p><strong>Summary:</strong> arXiv:2405.14314v2 Announce Type: replace-cross 
Abstract: Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://read-llm.github.io/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14314">https://arxiv.org/abs/2405.14314</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper proposes a new method of grounding large language models in multi-agent systems, which is relevant to your interest in agents based on these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14767" target="_blank">FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.14767v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14767v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Christina Dan Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.14767v2 Announce Type: replace-cross 
Abstract: As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \url{https://github.com/AI4Finance-Foundation/FinRobot}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14767">https://arxiv.org/abs/2405.14767</a></p>
            <p><strong>Category:</strong> q-fin.ST</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not focus on web browser or software control, it does present a novel application of large language models in finance, which is a type of computer automation using large language models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16507" target="_blank">Causal Concept Embedding Models: Beyond Causal Opacity in Deep Learning</a></h3>
            <a href="https://arxiv.org/html/2405.16507v1/extracted/5618823/figs/h_dsprites.png" target="_blank"><img src="https://arxiv.org/html/2405.16507v1/extracted/5618823/figs/h_dsprites.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gabriele Dominici, Pietro Barbiero, Mateo Espinosa Zarlenga, Alberto Termine, Martin Gjoreski, Marc Langheinrich</p>
            <p><strong>Summary:</strong> arXiv:2405.16507v1 Announce Type: new 
Abstract: Causal opacity denotes the difficulty in understanding the "hidden" causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design. The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16507">https://arxiv.org/abs/2405.16507</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper's primary focus lies on the intersection of causality and machine learning, a key area you mentioned as an interest. It specifically deals with causal representation learning and discovery within the context of deep learning models, which directly aligns with your subtopics and overall interest in new method development over applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.01412" target="_blank">Causal Temporal Regime Structure Learning</a></h3>
            
            <p><strong>Authors:</strong> Abdellah Rahmani, Pascal Frossard</p>
            <p><strong>Summary:</strong> arXiv:2311.01412v2 Announce Type: replace 
Abstract: We address the challenge of structure learning from multivariate time series that are characterized by a sequence of different, unknown regimes. We introduce a new optimization-based method (CASTOR), that concurrently learns the Directed Acyclic Graph (DAG) for each regime and determine the number of regimes along with their sequential arrangement. Through the optimization of a score function via an expectation maximization (EM) algorithm, CASTOR alternates between learning the regime indices (Expectation step) and inferring causal relationships in each regime (Maximization step). We further prove the identifiability of regimes and DAGs within the CASTOR framework. We conduct extensive experiments and show that our method consistently outperforms causal discovery models across various settings (linear and nonlinear causal relationships) and datasets (synthetic and real data).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.01412">https://arxiv.org/abs/2311.01412</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in causality and machine learning. It discusses a new method named CASTOR for causal discovery from time series data across different regimes, thereby aligning with your interest in causal discovery. The method is new and it outperforms other causal discovery models, aligning with your preference for papers that propose new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2112.13398" target="_blank">Long Story Short: Omitted Variable Bias in Causal Machine Learning</a></h3>
            <a href="https://arxiv.org/html/2112.13398v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2112.13398v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Victor Chernozhukov, Carlos Cinelli, Whitney Newey, Amit Sharma, Vasilis Syrgkanis</p>
            <p><strong>Summary:</strong> arXiv:2112.13398v5 Announce Type: replace-cross 
Abstract: We develop a general theory of omitted variable bias for a wide range of common causal parameters, including (but not limited to) averages of potential outcomes, average treatment effects, average causal derivatives, and policy effects from covariate shifts. Our theory applies to nonparametric models, while naturally allowing for (semi-)parametric restrictions (such as partial linearity) when such assumptions are made. We show how simple plausibility judgments on the maximum explanatory power of omitted variables are sufficient to bound the magnitude of the bias, thus facilitating sensitivity analysis in otherwise complex, nonlinear models. Finally, we provide flexible and efficient statistical inference methods for the bounds, which can leverage modern machine learning algorithms for estimation. These results allow empirical researchers to perform sensitivity analyses in a flexible class of machine-learned causal models using very simple, and interpretable, tools. We demonstrate the utility of our approach with two empirical examples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2112.13398">https://arxiv.org/abs/2112.13398</a></p>
            <p><strong>Category:</strong> econ.EM</p>
            <p><strong>Interest score:</strong> 4.5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper dives deeply into the topic of causality, specifically, it discusses omitted variable bias in causal machine learning models. This aligns with your interest in causal representation learning and causal discovery. The paper also implies using machine learning tools which align with your general field of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16069" target="_blank">IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark</a></h3>
            <a href="https://arxiv.org/html/2405.16069v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16069v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fredrik D. Johansson</p>
            <p><strong>Summary:</strong> arXiv:2405.16069v1 Announce Type: new 
Abstract: Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16069">https://arxiv.org/abs/2405.16069</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper focuses on evaluating observational estimators of causal effects, proposing a singular strategy for turning observational data into sequential structural causal models. It's highly relevant to your subtopic of causal discovery. However, it does not specifically mention the use of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16130" target="_blank">Automating the Selection of Proxy Variables of Unmeasured Confounders</a></h3>
            
            <p><strong>Authors:</strong> Feng Xie, Zhengming Chen, Shanshan Luo, Wang Miao, Ruichu Cai, Zhi Geng</p>
            <p><strong>Summary:</strong> arXiv:2405.16130v1 Announce Type: new 
Abstract: Recently, interest has grown in the use of proxy variables of unobserved confounding for inferring the causal effect in the presence of unmeasured confounders from observational data. One difficulty inhibiting the practical use is finding valid proxy variables of unobserved confounding to a target causal effect of interest. These proxy variables are typically justified by background knowledge. In this paper, we investigate the estimation of causal effects among multiple treatments and a single outcome, all of which are affected by unmeasured confounders, within a linear causal model, without prior knowledge of the validity of proxy variables. To be more specific, we first extend the existing proxy variable estimator, originally addressing a single unmeasured confounder, to accommodate scenarios where multiple unmeasured confounders exist between the treatments and the outcome. Subsequently, we present two different sets of precise identifiability conditions for selecting valid proxy variables of unmeasured confounders, based on the second-order statistics and higher-order statistics of the data, respectively. Moreover, we propose two data-driven methods for the selection of proxy variables and for the unbiased estimation of causal effects. Theoretical analysis demonstrates the correctness of our proposed algorithms. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16130">https://arxiv.org/abs/2405.16130</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses the important topic of inferring causal effects in the presence of unmeasured confounders, which is directly related to your interest in causal discovery. It proposes new data-driven methods for the selection of proxy variables and for unbiased estimation of causal effects, aligning with your interest in new methods within the field of causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16219" target="_blank">Deep Causal Generative Models with Property Control</a></h3>
            <a href="https://arxiv.org/html/2405.16219v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16219v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qilong Zhao, Shiyu Wang, Guangji Bai, Bo Pan, Zhaohui Qin, Liang Zhao</p>
            <p><strong>Summary:</strong> arXiv:2405.16219v1 Announce Type: new 
Abstract: Generating data with properties of interest by external users while following the right causation among its intrinsic factors is important yet has not been well addressed jointly. This is due to the long-lasting challenge of jointly identifying key latent variables, their causal relations, and their correlation with properties of interest, as well as how to leverage their discoveries toward causally controlled data generation. To address these challenges, we propose a novel deep generative framework called the Correlation-aware Causal Variational Auto-encoder (C2VAE). This framework simultaneously recovers the correlation and causal relationships between properties using disentangled latent vectors. Specifically, causality is captured by learning the causal graph on latent variables through a structural causal model, while correlation is learned via a novel correlation pooling algorithm. Extensive experiments demonstrate C2VAE's ability to accurately recover true causality and correlation, as well as its superiority in controllable data generation compared to baseline models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16219">https://arxiv.org/abs/2405.16219</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper falls under your 'Causality and machine learning' interest as it focuses on causality in data generation processes. It suggests a new framework C2VAE that uncovers both causality and correlation in data while providing controlled data generation – all fundamental aspects of causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16225" target="_blank">Local Causal Structure Learning in the Presence of Latent Variables</a></h3>
            
            <p><strong>Authors:</strong> Feng Xie, Zheng Li, Peng Wu, Yan Zeng, Chunchen Liu, Zhi Geng</p>
            <p><strong>Summary:</strong> arXiv:2405.16225v1 Announce Type: new 
Abstract: Discovering causal relationships from observational data, particularly in the presence of latent variables, poses a challenging problem. While current local structure learning methods have proven effective and efficient when the focus lies solely on the local relationships of a target variable, they operate under the assumption of causal sufficiency. This assumption implies that all the common causes of the measured variables are observed, leaving no room for latent variables. Such a premise can be easily violated in various real-world applications, resulting in inaccurate structures that may adversely impact downstream tasks. In light of this, our paper delves into the primary investigation of locally identifying potential parents and children of a target from observational data that may include latent variables. Specifically, we harness the causal information from m-separation and V-structures to derive theoretical consistency results, effectively bridging the gap between global and local structure learning. Together with the newly developed stop rules, we present a principled method for determining whether a variable is a direct cause or effect of a target. Further, we theoretically demonstrate the correctness of our approach under the standard causal Markov and faithfulness conditions, with infinite samples. Experimental results on both synthetic and real-world data validate the effectiveness and efficiency of our approach.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16225">https://arxiv.org/abs/2405.16225</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This research paper is focused on the discovery of causal relationships from observational data, which is one of the subtopics from your causality category. Despite not directly referencing the use of large language models, it delves into causal discovery considering latent variables, a challenge in current methods, and thus might provide valuable insights for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16489" target="_blank">Causal-Aware Graph Neural Architecture Search under Distribution Shifts</a></h3>
            <a href="https://arxiv.org/html/2405.16489v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16489v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peiwen Li, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Jialong Wang, Yang Li, Wenwu Zhu</p>
            <p><strong>Summary:</strong> arXiv:2405.16489v1 Announce Type: new 
Abstract: Graph NAS has emerged as a promising approach for autonomously designing GNN architectures by leveraging the correlations between graphs and architectures. Existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. We propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with following challenges: how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, and how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments demonstrate that CARNAS achieves advanced out-of-distribution generalization ability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16489">https://arxiv.org/abs/2405.16489</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses discovering and exploiting the causal relationship between graphs and architectures in Graph Neural Architecture Search, a topic which lies under your interest in causal discovery in machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16718" target="_blank">Amortized Active Causal Induction with Deep Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2405.16718v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16718v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yashas Annadani, Panagiotis Tigas, Stefan Bauer, Adam Foster</p>
            <p><strong>Summary:</strong> arXiv:2405.16718v1 Announce Type: new 
Abstract: We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies. Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16718">https://arxiv.org/abs/2405.16718</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This research paper is relevant to your interest in causality and machine learning, as it presents a new method, Causal Amortized Active Structure Learning (CAASL), that actively selects interventions for better estimation of the underlying causal graph. However, it does not specifically mention the use of large language models in causal discovery, hence the score is 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16924" target="_blank">Demystifying amortized causal discovery with transformers</a></h3>
            <a href="https://arxiv.org/html/2405.16924v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16924v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Francesco Montagna, Max Cairney-Leeming, Dhanya Sridhar, Francesco Locatello</p>
            <p><strong>Summary:</strong> arXiv:2405.16924v1 Announce Type: new 
Abstract: Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16924">https://arxiv.org/abs/2405.16924</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper investigates CSIvA, a transformer-based model for causal discovery from observational data, which aligns with your interest in causal discovery and transformer models. While it does not propose new methods, it offers valuable insights on the assumptions behind this method.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17151" target="_blank">Smoke and Mirrors in Causal Downstream Tasks</a></h3>
            <a href="https://arxiv.org/html/2405.17151v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17151v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Riccardo Cadei, Lukas Lindorfer, Sylvia Cremer, Cordelia Schmid, Francesco Locatello</p>
            <p><strong>Summary:</strong> arXiv:2405.17151v1 Announce Type: new 
Abstract: Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where we assume binary effects that are recorded as high-dimensional images in a Randomized Controlled Trial (RCT). Despite being the simplest possible setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences. All code and data will be released.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17151">https://arxiv.org/abs/2405.17151</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in the causality topic. It explores the causal inference task of treatment effect estimation—an aspect closely related to causal discovery. The paper also discusses representation learning methods for answering causal questions, which ties into your interest in causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15956" target="_blank">CFGs: Causality Constrained Counterfactual Explanations using goal-directed ASP</a></h3>
            <a href="https://arxiv.org/html/2405.15956v1/extracted/5616295/Images/Figure_ICLP.drawio.png" target="_blank"><img src="https://arxiv.org/html/2405.15956v1/extracted/5616295/Images/Figure_ICLP.drawio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sopam Dasgupta, Joaqu\'in Arias, Elmer Salazar, Gopal Gupta</p>
            <p><strong>Summary:</strong> arXiv:2405.15956v1 Announce Type: cross 
Abstract: Machine learning models that automate decision-making are increasingly used in consequential areas such as loan approvals, pretrial bail approval, and hiring. Unfortunately, most of these models are black boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations require informing the individual of changes in the input attribute (s) that could be made to produce a desirable outcome. Our work focuses on the latter problem of generating counterfactual explanations by considering the causal dependencies between features. In this paper, we present the framework CFGs, CounterFactual Generation with s(CASP), which utilizes the goal-directed Answer Set Programming (ASP) system s(CASP) to automatically generate counterfactual explanations from models generated by rule-based machine learning algorithms in particular. We benchmark CFGs with the FOLD-SE model. Reaching the counterfactual state from the initial state is planned and achieved using a series of interventions. To validate our proposal, we show how counterfactual explanations are computed and justified by imagining worlds where some or all factual assumptions are altered/changed. More importantly, we show how CFGs navigates between these worlds, namely, go from our initial state where we obtain an undesired outcome to the imagined goal state where we obtain the desired decision, taking into account the causal relationships among features.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15956">https://arxiv.org/abs/2405.15956</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses counterfactual explanations in the context of machine learning models, with a focus on causal dependencies. It would be relevant to your interest in causal discovery within machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.05771" target="_blank">Hacking Task Confounder in Meta-Learning</a></h3>
            <a href="https://arxiv.org/html/2312.05771v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.05771v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jingyao Wang, Yi Ren, Zeen Song, Jianqi Zhang, Changwen Zheng, Wenwen Qiang</p>
            <p><strong>Summary:</strong> arXiv:2312.05771v4 Announce Type: replace 
Abstract: Meta-learning enables rapid generalization to new tasks by learning knowledge from various tasks. It is intuitively assumed that as the training progresses, a model will acquire richer knowledge, leading to better generalization performance. However, our experiments reveal an unexpected result: there is negative knowledge transfer between tasks, affecting generalization performance. To explain this phenomenon, we conduct Structural Causal Models (SCMs) for causal analysis. Our investigation uncovers the presence of spurious correlations between task-specific causal factors and labels in meta-learning. Furthermore, the confounding factors differ across different batches. We refer to these confounding factors as "Task Confounders". Based on these findings, we propose a plug-and-play Meta-learning Causal Representation Learner (MetaCRL) to eliminate task confounders. It encodes decoupled generating factors from multiple tasks and utilizes an invariant-based bi-level optimization mechanism to ensure their causality for meta-learning. Extensive experiments on various benchmark datasets demonstrate that our work achieves state-of-the-art (SOTA) performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.05771">https://arxiv.org/abs/2312.05771</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it not only discusses causal discovery, but also introduces a new approach, MetaCRL, that uses causality in meta-learning. Therefore, it might provide valuable insights into causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02644" target="_blank">Variational DAG Estimation via State Augmentation With Stochastic Permutations</a></h3>
            <a href="https://arxiv.org/html/2402.02644v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02644v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Edwin V. Bonilla, Pantelis Elinas, He Zhao, Maurizio Filippone, Vassili Kitsios, Terry O'Kane</p>
            <p><strong>Summary:</strong> arXiv:2402.02644v2 Announce Type: replace 
Abstract: Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach performs competitively when compared with a wide range of Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02644">https://arxiv.org/abs/2402.02644</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'Causal discovery' under the high-level topic of 'Causality and Machine Learning'. The paper presents a Bayesian approach for estimating the structure of a Bayesian network which is a key component of causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02672" target="_blank">Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach</a></h3>
            <a href="https://arxiv.org/html/2402.02672v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02672v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</p>
            <p><strong>Summary:</strong> arXiv:2402.02672v2 Announce Type: replace-cross 
Abstract: Estimation of conditional average treatment effects (CATEs) is an important topic in sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data owing to privacy concerns. To address this issue, we proposed data collaboration double machine learning, a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through simulations. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. Second, our method enables collaborative estimation between multiple time points and different parties. Third, our method performed equally or better than other methods in simulations using synthetic, semi-synthetic and real-world datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02672">https://arxiv.org/abs/2402.02672</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is centered around a critical theme in causality, which is the estimation of conditional average treatment effects. Although it might not specifically discuss the subtopics you listed, the method introduced can be significant for causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17042" target="_blank">Towards Generalizing Inferences from Trials to Target Populations</a></h3>
            
            <p><strong>Authors:</strong> Melody Y Huang, Harsh Parikh</p>
            <p><strong>Summary:</strong> arXiv:2402.17042v2 Announce Type: replace-cross 
Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop's discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17042">https://arxiv.org/abs/2402.17042</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper correlates with your interest in 'Causality and Machine Learning'. It discusses the generalizability and transportability of causal effects along with the application of causal inference methods, which can be applied to 'Causal Discovery' and 'Causal Representation Learning'</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.14786" target="_blank">RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model</a></h3>
            <a href="https://arxiv.org/html/2404.14786v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.14786v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenweu Zhu</p>
            <p><strong>Summary:</strong> arXiv:2404.14786v2 Announce Type: replace-cross 
Abstract: In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis. Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data. However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios. To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts. To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets. Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization. Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery. We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.14786">https://arxiv.org/abs/2404.14786</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper should be of interest as it proposes a new method for temporal causal discovery from interventional data, leveraging Large Language Models for the analysis of textual information.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 28, 2024 at 22:01:29</div></body></html>