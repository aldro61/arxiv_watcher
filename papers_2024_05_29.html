
            <html>
            <head>
                <title>Report Generated on May 29, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for May 29, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17458" target="_blank">Blood Glucose Control Via Pre-trained Counterfactual Invertible Neural Networks</a></h3>
            
            <p><strong>Authors:</strong> Jingchi Jiang, Rujia Shen, Boran Wang, Yi Guan</p>
            <p><strong>Summary:</strong> arXiv:2405.17458v1 Announce Type: new 
Abstract: Type 1 diabetes mellitus (T1D) is characterized by insulin deficiency and blood glucose (BG) control issues. The state-of-the-art solution for continuous BG control is reinforcement learning (RL), where an agent can dynamically adjust exogenous insulin doses in time to maintain BG levels within the target range. However, due to the lack of action guidance, the agent often needs to learn from randomized trials to understand misleading correlations between exogenous insulin doses and BG levels, which can lead to instability and unsafety. To address these challenges, we propose an introspective RL based on Counterfactual Invertible Neural Networks (CINN). We use the pre-trained CINN as a frozen introspective block of the RL agent, which integrates forward prediction and counterfactual inference to guide the policy updates, promoting more stable and safer BG control. Constructed based on interpretable causal order, CINN employs bidirectional encoders with affine coupling layers to ensure invertibility while using orthogonal weight normalization to enhance the trainability, thereby ensuring the bidirectional differentiability of network parameters. We experimentally validate the accuracy and generalization ability of the pre-trained CINN in BG prediction and counterfactual inference for action. Furthermore, our experimental results highlight the effectiveness of pre-trained CINN in guiding RL policy updates for more accurate and safer BG control.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17458">https://arxiv.org/abs/2405.17458</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is of interest as it discusses the use of Counterfactual Invertible Neural Networks (CINN), a causal discovery method, in guiding Reinforcement Learning for accurate prediction and inference. This aligns well with your interests in causality in machine learning, though it's more focused on application rather than proposing entirely new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17653" target="_blank">InversionView: A General-Purpose Method for Reading Information from Neural Activations</a></h3>
            <a href="https://arxiv.org/html/2405.17653v1/extracted/5623283/images/concept-illustration3.png" target="_blank"><img src="https://arxiv.org/html/2405.17653v1/extracted/5623283/images/concept-illustration3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinting Huang, Madhur Panwar, Navin Goyal, Michael Hahn</p>
            <p><strong>Summary:</strong> arXiv:2405.17653v1 Announce Type: new 
Abstract: The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present three case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17653">https://arxiv.org/abs/2405.17653</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper introduces a method for interpreting the information content of neural networks, a topic related to causal discovery. It's especially useful for understanding algorithms implemented by transformer models, which could be beneficial for your interest in causality. However, it's not directly focused on causal discovery or causal representation learning, hence the 4 score instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18206" target="_blank">Multi-CATE: Multi-Accurate Conditional Average Treatment Effect Estimation Robust to Unknown Covariate Shifts</a></h3>
            <a href="https://arxiv.org/html/2405.18206v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18206v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Christoph Kern, Michael Kim, Angela Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.18206v1 Announce Type: new 
Abstract: Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment. The method works in general for pseudo-outcome regression, such as the DR-learner. We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial. We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment. Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18206">https://arxiv.org/abs/2405.18206</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper delves into conditional average treatment effect estimation, a key component of causality in machine learning. It describes a new method for making this estimation robust to unknown covariate shifts, making it potentially useful for your interest in causal discovery and causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18314" target="_blank">Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm</a></h3>
            <a href="https://arxiv.org/html/2405.18314v1/extracted/5626908/figures/violin_bound_var5.png" target="_blank"><img src="https://arxiv.org/html/2405.18314v1/extracted/5626908/figures/violin_bound_var5.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mathieu Chevalley, Patrick Schwab, Arash Mehrjou</p>
            <p><strong>Summary:</strong> arXiv:2405.18314v1 Announce Type: new 
Abstract: Targeted and uniform interventions to a system are crucial for unveiling causal relationships. While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging. Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available. In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution. More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders. Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings. To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score. Intersort outperforms baselines (GIES, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field. Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18314">https://arxiv.org/abs/2405.18314</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in 'Causality and machine learning'. It discuses a new method, 'Intersort', for causal discovery by leveraging data from single-variable interventions. The key relevance to your interest is that it details a novel approach to modelling interventional datasets which could be invaluable for causal inference.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2106.11234" target="_blank">Instrumental Variable Estimation for Compositional Treatments</a></h3>
            
            <p><strong>Authors:</strong> Elisabeth Ailer, Christian L. M\"uller, Niki Kilbertus</p>
            <p><strong>Summary:</strong> arXiv:2106.11234v3 Announce Type: replace 
Abstract: Many scientific datasets are compositional in nature. Important biological examples include species abundances in ecology, cell-type compositions derived from single-cell sequencing data, and amplicon abundance data in microbiome research. Here, we provide a causal view on compositional data in an instrumental variable setting where the composition acts as the cause. First, we crisply articulate potential pitfalls for practitioners regarding the interpretation of compositional causes from the viewpoint of interventions and warn against attributing causal meaning to common summary statistics such as diversity indices in microbiome data analysis. We then advocate for and develop multivariate methods using statistical data transformations and regression techniques that take the special structure of the compositional sample space into account while still yielding scientifically interpretable results. In a comparative analysis on synthetic and real microbiome data we show the advantages and limitations of our proposal. We posit that our analysis provides a useful framework and guidance for valid and informative cause-effect estimation in the context of compositional data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2106.11234">https://arxiv.org/abs/2106.11234</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'Causality and Machine Learning' topic as it discusses causal views on compositional data, which could be linked with causal representation learning and discovery. The paper also emphasizes the development of methods for cause-effect estimation, relevant to your interest in new methods in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02644" target="_blank">Variational DAG Estimation via State Augmentation With Stochastic Permutations</a></h3>
            <a href="https://arxiv.org/html/2402.02644v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02644v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Edwin V. Bonilla, Pantelis Elinas, He Zhao, Maurizio Filippone, Vassili Kitsios, Terry O'Kane</p>
            <p><strong>Summary:</strong> arXiv:2402.02644v3 Announce Type: replace 
Abstract: Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach performs competitively when compared with a wide range of Bayesian and non-Bayesian benchmarks on a range of synthetic and real datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02644">https://arxiv.org/abs/2402.02644</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is relevant as it discusses about estimating the structure of a Bayesian network, which is used in causal discovery. It proposes a new bayesian approach to solve the task, a topic you are interested in. The score is 4 and not 5 because the paper's focus is not specifically on causal representation learning or using large models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16507" target="_blank">Causal Concept Embedding Models: Beyond Causal Opacity in Deep Learning</a></h3>
            <a href="https://arxiv.org/html/2405.16507v2/extracted/5625861/figs/h_dsprites.png" target="_blank"><img src="https://arxiv.org/html/2405.16507v2/extracted/5625861/figs/h_dsprites.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gabriele Dominici, Pietro Barbiero, Mateo Espinosa Zarlenga, Alberto Termine, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich</p>
            <p><strong>Summary:</strong> arXiv:2405.16507v2 Announce Type: replace 
Abstract: Causal opacity denotes the difficulty in understanding the "hidden" causal structure underlying a deep neural network's (DNN) reasoning. This leads to the inability to rely on and verify state-of-the-art DNN-based systems especially in high-stakes scenarios. For this reason, causal opacity represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Embedding Models (Causal CEMs), a class of interpretable models whose decision-making process is causally transparent by design. The results of our experiments show that Causal CEMs can: (i) match the generalization performance of causally-opaque models, (ii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness, and (iii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also accuracy of the explanation provided for a specific instance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16507">https://arxiv.org/abs/2405.16507</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper may be of interest to you as it introduces 'Causal Concept Embedding Models' which connects to your interest in 'Causal representation learning'. It also touches on the aspect of 'Causal discovery' by discussing the understanding of 'hidden' causal structure underlying a deep learning model.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.02931" target="_blank">Bivariate Causal Discovery using Bayesian Model Selection</a></h3>
            <a href="https://arxiv.org/html/2306.02931v2/extracted/5624587/model_samples.png" target="_blank"><img src="https://arxiv.org/html/2306.02931v2/extracted/5624587/model_samples.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Anish Dhir, Samuel Power, Mark van der Wilk</p>
            <p><strong>Summary:</strong> arXiv:2306.02931v2 Announce Type: replace-cross 
Abstract: Much of the causal discovery literature prioritises guaranteeing the identifiability of causal direction in statistical models. For structures within a Markov equivalence class, this requires strong assumptions which may not hold in real-world datasets, ultimately limiting the usability of these methods. Building on previous attempts, we show how to incorporate causal assumptions within the Bayesian framework. Identifying causal direction then becomes a Bayesian model selection problem. This enables us to construct models with realistic assumptions, and consequently allows for the differentiation between Markov equivalent causal structures. We analyse why Bayesian model selection works in situations where methods based on maximum likelihood fail. To demonstrate our approach, we construct a Bayesian non-parametric model that can flexibly model the joint distribution. We then outperform previous methods on a wide range of benchmark datasets with varying data generating assumptions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.02931">https://arxiv.org/abs/2306.02931</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> You should consider this paper as it is centrally focused on causal discovery. It not only provides a new perspective on the problem by implementing a Bayesian model selection framework but also manages to outperform previous methods. However, it doesn't significantly touch on the roles of large language models in causal discovery, which is why the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.02790" target="_blank">CausalCite: A Causal Formulation of Paper Citations</a></h3>
            <a href="https://arxiv.org/html/2311.02790v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.02790v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ishan Kumar, Zhijing Jin, Ehsan Mokhtarian, Siyuan Guo, Yuen Chen, Mrinmaya Sachan, Bernhard Sch\"olkopf</p>
            <p><strong>Summary:</strong> arXiv:2311.02790v3 Announce Type: replace-cross 
Abstract: Citation count of a paper is a commonly used proxy for evaluating the significance of a paper in the scientific community. Yet citation measures are widely criticized for failing to accurately reflect the true impact of a paper. Thus, we propose CausalCite, a new way to measure the significance of a paper by assessing the causal impact of the paper on its follow-up papers. CausalCite is based on a novel causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. TextMatch encodes each paper using text embeddings from large language models (LLMs), extracts similar samples by cosine similarity, and synthesizes a counterfactual sample as the weighted average of similar papers according to their similarity values. We demonstrate the effectiveness of CausalCite on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various subfields of AI. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of the quality of a paper. Our code is available at https://github.com/causalNLP/causal-cite.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.02790">https://arxiv.org/abs/2311.02790</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests in causality, particularly in 'Causal discovery', as it introduces 'CausalCite', a new approach to assess the impact of a published paper on its subsequent research. Furthermore, it incorporates large language models (LLMs) for producing text embeddings, which ties into your interest in 'Using large language models in causal discovery'. However, it does not propose a new method in causal representation learning, which is why it gets a score of 4 not 5.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08219" target="_blank">BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.08219v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08219v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai</p>
            <p><strong>Summary:</strong> arXiv:2402.08219v2 Announce Type: replace-cross 
Abstract: Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08219">https://arxiv.org/abs/2402.08219</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a novel method for adapting Large Language Models for specific tasks, which aligns with your interest in using large language models for computer automation and software control. The new method proposed in the paper, BBox-Adapter, also provides a solution to the challenges of transparency, privacy, and cost associated with adapting black-box LLMs via API services.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16510" target="_blank">Meta-Task Planning for Language Agents</a></h3>
            
            <p><strong>Authors:</strong> Cong Zhang, Derrick Goh Xin Deik, Dexun Li, Hao Zhang, Yong Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.16510v2 Announce Type: replace-cross 
Abstract: The rapid advancement of neural language models has sparked a new surge of intelligent agent research. Unlike traditional agents, large language model-based agents (LLM agents) have emerged as a promising paradigm for achieving artificial general intelligence (AGI) due to their superior reasoning and generalization capabilities. Effective planning is crucial for the success of LLM agents in real-world tasks, making it a highly pursued topic in the community. Current planning methods typically translate tasks into executable action sequences. However, determining a feasible or optimal sequence for complex tasks at fine granularity, which often requires compositing long chains of heterogeneous actions, remains challenging. This paper introduces Meta-Task Planning (MTP), a zero-shot methodology for collaborative LLM-based multi-agent systems that simplifies complex task planning by decomposing it into a hierarchy of subordinate tasks, or meta-tasks. Each meta-task is then mapped into executable actions. MTP was assessed on two rigorous benchmarks, TravelPlanner and API-Bank. Notably, MTP achieved an average $\sim40\%$ success rate on TravelPlanner, significantly higher than the state-of-the-art (SOTA) baseline ($2.92\%$), and outperforming $LLM_{api}$-4 with ReAct on API-Bank by $\sim14\%$, showing the immense potential of integrating LLM with multi-agent systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16510">https://arxiv.org/abs/2405.16510</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper aligns with your interests in agents based on large-language models (LLM agents). It outlines a new method for planning tasks for these agents, particularly in collaborative multi-agent systems - 'Meta-Task Planning (MTP)'. It shows potential in controlling software as it focuses on mapping meta-tasks into executable actions, thus contributing to the field of computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17470" target="_blank">Athena: Efficient Block-Wise Post-Training Quantization for Large Language Models Using Second-Order Matrix Derivative Information</a></h3>
            <a href="https://arxiv.org/html/2405.17470v1/extracted/5614308/figure/dis.png" target="_blank"><img src="https://arxiv.org/html/2405.17470v1/extracted/5614308/figure/dis.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yanshu Wang, Wenyang He, Tong Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.17470v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly advanced natural language processing tasks such as machine translation, text generation, and sentiment analysis. However, their large size, often consisting of billions of parameters, poses challenges for storage, computation, and deployment, particularly in resource-constrained environments like mobile devices and edge computing platforms. Effective compression and quantization techniques are crucial for addressing these issues, reducing memory footprint and computational requirements without significantly compromising performance. Traditional methods that uniformly map parameters to compressed spaces fail to account for the uneven distribution of parameters, leading to substantial accuracy loss. In this work, we propose Athena, a novel algorithm for efficient block-wise post-training quantization of LLMs. Athena leverages Second-Order Matrix Derivative Information to guide the quantization process using the curvature information of the loss landscape. By grouping parameters by columns or rows and iteratively optimizing the quantization process, Athena updates the model parameters and Hessian matrix to achieve significant compression while maintaining high accuracy. This makes Athena a practical solution for deploying LLMs in various settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17470">https://arxiv.org/abs/2405.17470</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It presents Athena, a novel algorithm for efficient block-wise post-training quantization of LLMs. While it doesn't deal with control of software or web browsers, or computer automation, it is still crucial in the overall goal of effectively deploying large language models in different contexts.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17477" target="_blank">OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning</a></h3>
            <a href="https://arxiv.org/html/2405.17477v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17477v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sheng Yue, Xingyuan Hua, Ju Ren, Sen Lin, Junshan Zhang, Yaoxue Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.17477v1 Announce Type: new 
Abstract: In this paper, we study offline-to-online Imitation Learning (IL) that pretrains an imitation policy from static demonstration data, followed by fast finetuning with minimal environmental interaction. We find the na\"ive combination of existing offline IL and online IL methods tends to behave poorly in this context, because the initial discriminator (often used in online IL) operates randomly and discordantly against the policy initialization, leading to misguided policy optimization and $\textit{unlearning}$ of pretraining knowledge. To overcome this challenge, we propose a principled offline-to-online IL method, named $\texttt{OLLIE}$, that simultaneously learns a near-expert policy initialization along with an $\textit{aligned discriminator initialization}$, which can be seamlessly integrated into online IL, achieving smooth and fast finetuning. Empirically, $\texttt{OLLIE}$ consistently and significantly outperforms the baseline methods in $\textbf{20}$ challenging tasks, from continuous control to vision-based domains, in terms of performance, demonstration efficiency, and convergence speed. This work may serve as a foundation for further exploration of pretraining and finetuning in the context of IL.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17477">https://arxiv.org/abs/2405.17477</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models and their applications for controlling software. While it does not mention specifically controlling browsers, the paper does discuss a method for improving policy learning, which is important for agent-based models. However, I give it a score of 4 because it is effectively about an improvement to a method, rather than proposing a completely new one.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17618" target="_blank">Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales</a></h3>
            <a href="https://arxiv.org/html/2405.17618v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17618v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ju-Seung Byun, Andrew Perrault</p>
            <p><strong>Summary:</strong> arXiv:2405.17618v1 Announce Type: new 
Abstract: Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17618">https://arxiv.org/abs/2405.17618</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may be relevant to your interests as it discusses methods for improving the performance of large language models using reinforcement learning, which could be used to control software or web browsers. However, it doesn't directly address these applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17627" target="_blank">Salutary Labeling with Zero Human Annotation</a></h3>
            <a href="https://arxiv.org/html/2405.17627v1/extracted/5613569/figures/moti_init_hticks.png" target="_blank"><img src="https://arxiv.org/html/2405.17627v1/extracted/5613569/figures/moti_init_hticks.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenxiao Xiao, Hongfu Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.17627v1 Announce Type: new 
Abstract: Active learning strategically selects informative unlabeled data points and queries their ground truth labels for model training. The prevailing assumption underlying this machine learning paradigm is that acquiring these ground truth labels will optimally enhance model performance. However, this assumption may not always hold true or maximize learning capacity, particularly considering the costly labor annotations required for ground truth labels. In contrast to traditional ground truth labeling, this paper proposes salutary labeling, which automatically assigns the most beneficial labels to the most informative samples without human annotation. Specifically, we utilize the influence function, a tool for estimating sample influence, to select newly added samples and assign their salutary labels by choosing the category that maximizes their positive influence. This process eliminates the need for human annotation. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our salutary labeling approach over traditional active learning strategies. Additionally, we provide several in-depth explorations and practical applications of large language model (LLM) fine-tuning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17627">https://arxiv.org/abs/2405.17627</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it discusses the practical applications of large language model fine-tuning, which could have implications for using large language models to control software and for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17767" target="_blank">Linguistic Collapse: Neural Collapse in (Large) Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.17767v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17767v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Robert Wu, Vardan Papyan</p>
            <p><strong>Summary:</strong> arXiv:2405.17767v1 Announce Type: new 
Abstract: Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored $\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as \textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\mathcal{NC}$. We find that $\mathcal{NC}$ properties that develop with scaling are linked to generalization. Moreover, there is evidence of some relationship between $\mathcal{NC}$ and generalization independent of scale. Our work therefore underscores the generality of $\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\mathcal{NC}$-related properties.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17767">https://arxiv.org/abs/2405.17767</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores studies on large language models (LLMs) and the phenomena of neural collapse. Although not entirely focused on automation or software control, it provides insight into the behaviors and properties of LLMs, which is fundamental to understanding how they could be tailored to control software and browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17849" target="_blank">I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, Chen Xu</p>
            <p><strong>Summary:</strong> arXiv:2405.17849v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs. We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17849">https://arxiv.org/abs/2405.17849</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses methods for accelerating the inference of large language models, which falls under your interest in agents based on large-language models. Although it does not specifically discuss controlling software or web browsers, the advancements made in this paper could be beneficial to those specific applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18039" target="_blank">Large Language Model-Driven Curriculum Design for Mobile Networks</a></h3>
            <a href="https://arxiv.org/html/2405.18039v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18039v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Omar Erak, Omar Alhussein, Shimaa Naser, Nouf Alabbasi, De Mi, Sami Muhaidat</p>
            <p><strong>Summary:</strong> arXiv:2405.18039v1 Announce Type: new 
Abstract: This paper proposes a novel framework that leverages large language models (LLMs) to automate curriculum design, thereby enhancing the application of reinforcement learning (RL) in mobile networks. As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges. Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks. To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization. However, curriculum design typically requires extensive domain knowledge and manual human effort. Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance. We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements. As a case study, we consider autonomous coordination and user association in mobile networks. Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18039">https://arxiv.org/abs/2405.18039</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large-language models and agents. It proposes a new method where large language models are used to automate curriculum design in the context of reinforcement learning for managing mobile networks. Although it doesn't directly talk about automating web browsers, it discusses an automation process in a different context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18218" target="_blank">FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.18218v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18218v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yang Zhang, Yawei Li, Xinpeng Wang, Qianli Shen, Barbara Plank, Bernd Bischl, Mina Rezaei, Kenji Kawaguchi</p>
            <p><strong>Summary:</strong> arXiv:2405.18218v1 Announce Type: new 
Abstract: Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To address these issues, we propose FinerCut, a new form of fine-grained layer pruning, which in contrast to prior work at the transformer block level, considers all self-attention and feed-forward network (FFN) layers within blocks as individual pruning candidates. FinerCut prunes layers whose removal causes minimal alternation to the model's output -- contributing to a new, lean, interpretable, and task-agnostic pruning method. Tested across 9 benchmarks, our approach retains 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed, all without fine-tuning or post-pruning reconstruction. Strikingly, we observe intriguing results with FinerCut: 42% (34 out of 80) of the self-attention layers in Llama3-70B can be removed while preserving 99% of its performance -- without additional fine-tuning after removal. Moreover, FinerCut provides a tool to inspect the types and locations of pruned layers, allowing to observe interesting pruning behaviors. For instance, we observe a preference for pruning self-attention layers, often at deeper consecutive decoder layers. We hope our insights inspire future efficient LLM architecture designs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18218">https://arxiv.org/abs/2405.18218</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models and their efficiency as it presents a technique for pruning layers in large transformer networks. While it does not directly discuss controlling software or web browsers, the insights gained from a more efficient architecture could be applicable to those fields.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18380" target="_blank">OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning</a></h3>
            <a href="https://arxiv.org/html/2405.18380v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18380v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pengxiang Li, Lu Yin, Xiaowei Gao, Shiwei Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.18380v1 Announce Type: new 
Abstract: The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors. We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained. Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs. To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner. By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning. Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18380">https://arxiv.org/abs/2405.18380</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents 'Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore)', a new method for memory-efficient fine-tuning of Large Language Models (LLMs). This could be relevant to your interests in agents based on large-language models, as it contributes a new method to optimize memory-performance trade-off in LLM pruning, which could impact their potential to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17438" target="_blank">An LLM-Tool Compiler for Fused Parallel Function Calling</a></h3>
            <a href="https://arxiv.org/html/2405.17438v1/extracted/5583056/llm-compiler-teaser.png" target="_blank"><img src="https://arxiv.org/html/2405.17438v1/extracted/5583056/llm-compiler-teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Simranjit Singh, Andreas Karatzas, Michael Fore, Iraklis Anagnostopoulos, Dimitrios Stamoulis</p>
            <p><strong>Summary:</strong> arXiv:2405.17438v1 Announce Type: cross 
Abstract: State-of-the-art sequential reasoning in Large Language Models (LLMs) has expanded the capabilities of Copilots beyond conversational tasks to complex function calling, managing thousands of API calls. However, the tendency of compositional prompting to segment tasks into multiple steps, each requiring a round-trip to the GPT APIs, leads to increased system latency and costs. Although recent advancements in parallel function calling have improved tool execution per API call, they may necessitate more detailed in-context instructions and task breakdown at the prompt level, resulting in higher engineering and production costs. Inspired by the hardware design principles of multiply-add (MAD) operations, which fuse multiple arithmetic operations into a single task from the compiler's perspective, we propose LLM-Tool Compiler, which selectively fuses similar types of tool operations under a single function at runtime, presenting them as a unified task to the LLM. This selective fusion inherently enhances parallelization and efficiency. Benchmarked on a large-scale Copilot platform, LLM-Tool Compiler achieves up to four times more parallel calls than existing methods, reducing token costs and latency by up to 40% and 12%, respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17438">https://arxiv.org/abs/2405.17438</a></p>
            <p><strong>Category:</strong> cs.PL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper deals with the concept of Large Language Models (LLMs) being used to control software more efficiently with reduced latency, which aligns closely with your interest in 'Agents based on large language models'. Nevertheless, while it involves LLM to enhance task execution, it doesn't explicitly mention about controlling browsers or automating computers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17587" target="_blank">RAGSys: Item-Cold-Start Recommender as RAG System</a></h3>
            <a href="https://arxiv.org/html/2405.17587v1/extracted/5624460/cosine_sim_vs_dpo.png" target="_blank"><img src="https://arxiv.org/html/2405.17587v1/extracted/5624460/cosine_sim_vs_dpo.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Emile Contal, Garrin McGoldrick</p>
            <p><strong>Summary:</strong> arXiv:2405.17587v1 Announce Type: cross 
Abstract: Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17587">https://arxiv.org/abs/2405.17587</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it discusses an application of Large Language Models, a specific focus of your interests in 'agents based on large language models'. However, while it focuses on using Large Language Models to optimize retrieval systems, it's not specific to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17743" target="_blank">ORLM: Training Large Language Models for Optimization Modeling</a></h3>
            <a href="https://arxiv.org/html/2405.17743v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17743v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.17743v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for complex Operations Research (OR) in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data will be available at \url{https://github.com/Cardinal-Operations/ORLM}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17743">https://arxiv.org/abs/2405.17743</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper corresponds to the 'llm-agents' tag from your interests. It discusses using Large Language Models (LLMs) for complex Operations Research, including aspects such as automating optimization modeling. Although it goes beyond controlling software or web browsers, it may provide valuable context for these specific tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17931" target="_blank">Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.17931v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17931v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.17931v1 Announce Type: cross 
Abstract: Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer. Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17931">https://arxiv.org/abs/2405.17931</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It discusses about Large Language Models (LLMs), alignment with human-centric values, and Reinforcement Learning from Human Feedback (RLHF), which is a paradigm in the field of machine learning that could be used for controlling software, among other applications. However, it does not explicitly mention controlling web browsers or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17969" target="_blank">Knowledge Circuits in Pretrained Transformers</a></h3>
            <a href="https://arxiv.org/html/2405.17969v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17969v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.17969v1 Announce Type: cross 
Abstract: The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, has allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17969">https://arxiv.org/abs/2405.17969</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it extensively discusses the functions and behaviors of large language models such as GPT2 and TinyLLAMA. It might provide useful insights regarding the use of large language models for controlling software and enabling computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18093" target="_blank">Pipette: Automatic Fine-grained Large Language Model Training Configurator for Real-World Clusters</a></h3>
            <a href="https://arxiv.org/html/2405.18093v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18093v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinkyu Yim, Jaeyong Song, Yerim Choi, Jaebeen Lee, Jaewon Jung, Hongsun Jang, Jinho Lee</p>
            <p><strong>Summary:</strong> arXiv:2405.18093v1 Announce Type: cross 
Abstract: Training large language models (LLMs) is known to be challenging because of the huge computational and memory capacity requirements. To address these issues, it is common to use a cluster of GPUs with 3D parallelism, which splits a model along the data batch, pipeline stage, and intra-layer tensor dimensions. However, the use of 3D parallelism produces the additional challenge of finding the optimal number of ways on each dimension and mapping the split models onto the GPUs. Several previous studies have attempted to automatically find the optimal configuration, but many of these lacked several important aspects. For instance, the heterogeneous nature of the interconnect speeds is often ignored. While the peak bandwidths for the interconnects are usually made equal, the actual attained bandwidth varies per link in real-world clusters. Combined with the critical path modeling that does not properly consider the communication, they easily fall into sub-optimal configurations. In addition, they often fail to consider the memory requirement per GPU, often recommending solutions that could not be executed. To address these challenges, we propose Pipette, which is an automatic fine-grained LLM training configurator for real-world clusters. By devising better performance models along with the memory estimator and fine-grained individual GPU assignment, Pipette achieves faster configurations that satisfy the memory constraints. We evaluated Pipette on large clusters to show that it provides a significant speedup over the prior art. The implementation of Pipette is available at https://github.com/yimjinkyu1/date2024_pipette.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18093">https://arxiv.org/abs/2405.18093</a></p>
            <p><strong>Category:</strong> cs.DC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses training large language models, which is directly related to your interest in agents based on large language models. While it does not directly discuss controlling software or automation, the techniques it describes could be used to improve those applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18208" target="_blank">A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.18208v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18208v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengxing Xie, Difan Zou</p>
            <p><strong>Summary:</strong> arXiv:2405.18208v1 Announce Type: cross 
Abstract: Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18208">https://arxiv.org/abs/2405.18208</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be of interest because it discusses the development of a human-like planning framework for Large Language Model agents, particularly focusing on planning tasks. The paper details strategies to improve the efficiency and effectiveness of these agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18369" target="_blank">PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework</a></h3>
            
            <p><strong>Authors:</strong> Eshaan Agarwal, Vivek Dani, Tanuja Ganu, Akshay Nambi</p>
            <p><strong>Summary:</strong> arXiv:2405.18369v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities. Central to their success is the concept of prompting, which guides model output generation. However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions. This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks. Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance. The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity. It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance. PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18369">https://arxiv.org/abs/2405.18369</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be very relevant for your interest in 'Agents based on large-language models', especially in terms of automation. It introduces PromptWizard, a framework that uses large language models for automatic task-tailored prompting, which could potentially be implemented for controlling software or web browsers. While it's not directly focused on control aspect, the automatic task optimization leveraging LLMs can contribute significantly to improving the efficiency of such control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18414" target="_blank">Don't Forget to Connect! Improving RAG with Graph-based Reranking</a></h3>
            <a href="https://arxiv.org/html/2405.18414v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18414v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, Anton Tsitsulin</p>
            <p><strong>Summary:</strong> arXiv:2405.18414v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18414">https://arxiv.org/abs/2405.18414</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper focuses on Large Language Model responses and a method to improve them called RAG, it doesn't explicitly cover controlling software or web browsers using large language models. However, the main interest area that it may relate to is in agent-based models. The approach could potentially be applied in automation scenarios using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.16617" target="_blank">Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model</a></h3>
            <a href="https://arxiv.org/html/2305.16617v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.16617v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng</p>
            <p><strong>Summary:</strong> arXiv:2305.16617v2 Announce Type: replace 
Abstract: The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.16617">https://arxiv.org/abs/2305.16617</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under the category of large-language models (LLMs). It discusses the detection of texts generated by LLMs and proposes a new model that improves efficiency, which could potentially be useful in controlling software and automating tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.05516" target="_blank">Stateful Large Language Model Serving with Pensieve</a></h3>
            <a href="https://arxiv.org/html/2312.05516v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.05516v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lingfan Yu, Jinyang Li</p>
            <p><strong>Summary:</strong> arXiv:2312.05516v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are wildly popular today and it is important to serve them efficiently. Existing LLM serving systems are stateless across requests. Consequently, when LLMs are used in the common setting of multi-turn conversations, a growing log of the conversation history must be processed alongside any request by the serving system at each turn, resulting in repeated processing.
  In this paper, we design Pensieve, a system optimized for multi-turn conversation LLM serving. Pensieve maintains the conversation state across requests by caching previously processed history to avoid duplicate processing. Pensieve's multi-tier caching strategy can utilize both GPU and CPU memory to efficiently store and retrieve cached data. Pensieve also generalizes the recent PagedAttention kernel to support attention between multiple input tokens with a GPU cache spread over non-contiguous memory. Our evaluation shows that Pensieve can achieve 13-58% more throughput compared to vLLM and TensorRT-LLM and significantly reduce latency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.05516">https://arxiv.org/abs/2312.05516</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper details a method of using Large Language Models (LLMs) to optimize multi-turn conversation serving systems. While it does not specifically address controlling software or web browsers, it proposes a new method of using LLMs in a complex task that requires sequencing and processing. It could potentially be applied in software control and automation settings, thus making it relevant to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02314" target="_blank">Selecting Large Language Model to Fine-tune via Rectified Scaling Law</a></h3>
            <a href="https://arxiv.org/html/2402.02314v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02314v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang</p>
            <p><strong>Summary:</strong> arXiv:2402.02314v3 Announce Type: replace 
Abstract: The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02314">https://arxiv.org/abs/2402.02314</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Agents based on large-language models'. It discusses the selection of a Large Language Model (LLM) for fine-tuning, an essential factor when designing agents built upon LLMs. It also focuses on predicting fine-tuning performance, which is a critical concern in implementing effective LLM agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11592" target="_blank">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</a></h3>
            <a href="https://arxiv.org/html/2402.11592v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11592v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen</p>
            <p><strong>Summary:</strong> arXiv:2402.11592v3 Announce Type: replace 
Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11592">https://arxiv.org/abs/2402.11592</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly relates to your interest in large language models, providing a solution for reducing memory costs during LLM fine-tuning, which is pertinent to their usage in software control and automation. Although it does not focus on controlling software or web browsers specifically, it holds relevance due to its focus on fine-tuning and memory efficiency of large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11867" target="_blank">LoRA Training in the NTK Regime has No Spurious Local Minima</a></h3>
            <a href="https://arxiv.org/html/2402.11867v3/extracted/5625608/1.png" target="_blank"><img src="https://arxiv.org/html/2402.11867v3/extracted/5625608/1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Uijeong Jang, Jason D. Lee, Ernest K. Ryu</p>
            <p><strong>Summary:</strong> arXiv:2402.11867v3 Announce Type: replace 
Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11867">https://arxiv.org/abs/2402.11867</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper delve into Low-rank adaptation (LoRA), a technique that can be used for fine-tuning large language models (LLMs). This could be particularly useful when designing agents based on LLMs. However, the paper does not specifically address using LLMs for computer automation, controlling software or web browsers. It more so talks about the theory behind LoRA which can be vital for improving the functionality of LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17453" target="_blank">DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning</a></h3>
            <a href="https://arxiv.org/html/2402.17453v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.17453v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang</p>
            <p><strong>Summary:</strong> arXiv:2402.17453v5 Announce Type: replace 
Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\% success rate in the development stage, while attaining 36\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17453">https://arxiv.org/abs/2402.17453</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses 'DS-Agent', an automated data science framework using large language models, which aligns with your interest in agents based on large language models. Particularly, it speaks to the theme of computer automation using large language models. While it doesn't explicitly discuss controlling web browsers or software, the automation of data science tasks might intersect with these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02948" target="_blank">PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.02948v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02948v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fanxu Meng, Zhaohui Wang, Muhan Zhang</p>
            <p><strong>Summary:</strong> arXiv:2404.02948v3 Announce Type: replace 
Abstract: To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\Delta W \in \mathbb{R}^{m \times n}$ through the product of two matrices $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA freezes the original model $W$ and updates the "Noise & Zero" adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \in \mathbb{R}^{m \times n}$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the "residual" parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02948">https://arxiv.org/abs/2404.02948</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about the fine-tuning of large language models, which is pertinent to the construction and improvement of agents based on large language models. However, it does not explicitly mention the application of these models in controlling software, web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.04102" target="_blank">ROPO: Robust Preference Optimization for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.04102v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.04102v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xize Liang, Chao Chen, Shuang Qiu, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye</p>
            <p><strong>Summary:</strong> arXiv:2404.04102v2 Announce Type: replace 
Abstract: Preference alignment is pivotal for empowering large language models (LLMs) to generate helpful and harmless responses. However, the performance of preference alignment is highly sensitive to the prevalent noise in the preference data. Recent efforts for this problem either marginally alleviate the impact of noise without the ability to actually reduce its presence, or rely on costly teacher LLMs prone to reward misgeneralization. To address these challenges, we propose the RObust Preference Optimization (ROPO) framework, an iterative alignment approach that integrates noise-tolerance and filtering of noisy samples without the aid of external models. Specifically, ROPO iteratively solves a constrained optimization problem, where we dynamically assign a quality-aware weight for each sample and constrain the sum of the weights to the number of samples we intend to retain. For noise-tolerant training and effective noise identification, we derive a robust loss by suppressing the gradients of samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is critical for distinguishing noisy samples from clean ones. Furthermore, inspired by our derived loss, we propose a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Experiments on three widely-used datasets with Mistral-7B and Llama-2-7B demonstrate that ROPO significantly outperforms existing preference alignment methods, with its superiority growing as the noise rate increases.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.04102">https://arxiv.org/abs/2404.04102</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models as it discusses the ROPO framework for preference alignment in LLMs, although building agents based on LLMs is not the main focus of the paper.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00747" target="_blank">Soft Preference Optimization: Aligning Language Models to Expert Distributions</a></h3>
            <a href="https://arxiv.org/html/2405.00747v3/extracted/5624571/Figures/win_rate_main_body.png" target="_blank"><img src="https://arxiv.org/html/2405.00747v3/extracted/5624571/Figures/win_rate_main_body.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Arsalan Sharifnassab, Sina Ghiassian, Saber Salehkaleybar, Surya Kanoria, Dale Schuurmans</p>
            <p><strong>Summary:</strong> arXiv:2405.00747v3 Announce Type: replace 
Abstract: We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's "softness" adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00747">https://arxiv.org/abs/2405.00747</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a method for aligning large language models with human preferences, which could be potentially applied to control software and enable automation. However, it doesn't directly talk about using large language models for software control or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14597" target="_blank">Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.14597v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14597v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Yifan Lu, Yerui Sun, Lin Ma, Yuchen Xie</p>
            <p><strong>Summary:</strong> arXiv:2405.14597v2 Announce Type: replace 
Abstract: We introduce Integer Scale, a novel post-training quantization scheme for large language models that effectively resolves the inference bottleneck in current fine-grained quantization approaches while maintaining similar accuracies. Integer Scale is a free lunch as it requires no extra calibration or fine-tuning which will otherwise incur additional costs. It can be used plug-and-play for most fine-grained quantization methods. Its integration results in at most 1.85x end-to-end speed boost over the original counterpart with comparable accuracy. Additionally, due to the orchestration of the proposed Integer Scale and fine-grained quantization, we resolved the quantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible performance degradation, and it comes with an end-to-end speed boost of 2.13x, and 2.31x compared with their FP16 versions respectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14597">https://arxiv.org/abs/2405.14597</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper introduces a post-training quantization method for large language models, which can significantly speed up their inference time. This could be relevant for any kind of software controlled by such models, including web browsers and other automation tools. Although it doesn't directly focus on new methods for controlling software or web browsers, it addresses one of the potential bottlenecks when using large language models for these purposes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17374" target="_blank">Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.17374v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17374v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau</p>
            <p><strong>Summary:</strong> arXiv:2405.17374v2 Announce Type: replace 
Abstract: Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17374">https://arxiv.org/abs/2405.17374</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While it does not strictly discuss new methods for using large language models to control software or web browsers, it delves into safety considerations which are undoubtedly important for automation applications. Understanding how to maintain model safety during fine-tuning could be useful when developing new methods to use such models for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04617" target="_blank">InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory</a></h3>
            <a href="https://arxiv.org/html/2402.04617v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04617v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun</p>
            <p><strong>Summary:</strong> arXiv:2402.04617v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \url{https://github.com/thunlp/InfLLM}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04617">https://arxiv.org/abs/2402.04617</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about expanding the capabilities of Large Language Models (LLMs) to understand and process extremely long sequences. In terms of your interest, it's indirect but fits the theme of computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09179" target="_blank">Instruction Backdoor Attacks Against Customized LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.09179v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.09179v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang</p>
            <p><strong>Summary:</strong> arXiv:2402.09179v3 Announce Type: replace-cross 
Abstract: The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 6 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose two defense strategies and demonstrate their effectiveness in reducing such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09179">https://arxiv.org/abs/2402.09179</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not propound the usage of large language models for controlling software or web browsers directly, it gives valuable insights into potential pitfalls (backdoor attacks) when implementing LLMs in customized applications. These findings can be extremely useful in building secure LLM-based agents for software control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10379" target="_blank">DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows</a></h3>
            <a href="https://arxiv.org/html/2402.10379v2/extracted/5624561/resources/figures/DataDreamerRecursive.png" target="_blank"><img src="https://arxiv.org/html/2402.10379v2/extracted/5624561/resources/figures/DataDreamerRecursive.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ajay Patel, Colin Raffel, Chris Callison-Burch</p>
            <p><strong>Summary:</strong> arXiv:2402.10379v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at https://github.com/datadreamer-dev/DataDreamer .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10379">https://arxiv.org/abs/2402.10379</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is very relevant for your interest in agents based on large-language models. The paper discusses an open-source tool, DataDreamer, that allows researchers to implement powerful workflows with large language models. Some of these workflows may include control or automation tasks, which directly align with your subtopics of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10958" target="_blank">Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts</a></h3>
            <a href="https://arxiv.org/html/2402.10958v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10958v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou</p>
            <p><strong>Summary:</strong> arXiv:2402.10958v2 Announce Type: replace-cross 
Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to leverage insights from a more varied set of prompts. Through empirical tests, including dialogue and summarization tasks, and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a superior ability to align LLMs with user preferences and to improve their adaptability during the training process. Our code can be viewed at https://github.com/yinyueqin/relative-preference-optimization</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10958">https://arxiv.org/abs/2402.10958</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in agents based on large language models. It presents a method called Relative Preference Optimization (RPO) that improves the alignment of large language models with user preferences. Although it doesn't specifically focus on controlling software or web browsers, the improvement in adaptability during training may be beneficial in these applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14809" target="_blank">CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</a></h3>
            <a href="https://arxiv.org/html/2402.14809v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14809v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang</p>
            <p><strong>Summary:</strong> arXiv:2402.14809v3 Announce Type: replace-cross 
Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14809">https://arxiv.org/abs/2402.14809</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper involves the study of Large Language Models (LLMs), and their ability to critique and rectify their reasoning, which may include controlling software or web browsers. Although it does not directly talk about controlling software or browsers, the research is relevant as it explores the reasoning capabilities of LLMs, which is crucial for their automation abilities.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.07378" target="_blank">SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression</a></h3>
            
            <p><strong>Authors:</strong> Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.07378v4 Announce Type: replace-cross 
Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.07378">https://arxiv.org/abs/2403.07378</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about large language model compression. Although it doesn't directly involve using large language models to control software or browsers, it could be relevant as efficient model compression would enable practical deployment of large language model agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.14472" target="_blank">Detoxifying Large Language Models via Knowledge Editing</a></h3>
            
            <p><strong>Authors:</strong> Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</p>
            <p><strong>Summary:</strong> arXiv:2403.14472v5 Announce Type: replace-cross 
Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.14472">https://arxiv.org/abs/2403.14472</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it not only discusses large language models, but it also addresses modifying their behavior, which could be applicable in controlling software or other types of automation. However, it does not focus specifically on your subtopics of control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.18969" target="_blank">A Survey on Large Language Models from Concept to Implementation</a></h3>
            
            <p><strong>Authors:</strong> Chen Wang, Jin Zhao, Jiaqi Gong</p>
            <p><strong>Summary:</strong> arXiv:2403.18969v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of Transformer-based LLMs in practical applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.18969">https://arxiv.org/abs/2403.18969</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper provides a comprehensive survey on the uses of large language models, highlighting their applications in various sectors. Although it does not specifically mention control of software or web browsers, it covers areas like code interpretation and the construction of interactive systems which align with your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16802" target="_blank">AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation</a></h3>
            <a href="https://arxiv.org/html/2405.16802v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16802v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo</p>
            <p><strong>Summary:</strong> arXiv:2405.16802v2 Announce Type: replace-cross 
Abstract: In this work, we propose a novel method named \textbf{Auto}mated Process Labeling via \textbf{C}onfidence \textbf{V}ariation (\textbf{\textsc{AutoCV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps. Our approach begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward. We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches. We experimentally validate that the confidence variations learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps. Subsequently, we demonstrate that the process annotations generated by \textsc{AutoCV} can improve the accuracy of the verification model in selecting the correct answer from multiple outputs generated by LLMs. Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning. The source code of \textsc{AutoCV} is available at \url{https://github.com/rookie-joe/AUTOCV}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16802">https://arxiv.org/abs/2405.16802</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on improving the reasoning capabilities of large language models, a topic related to your interest in agent-based large language models. However, it does not explicitly detail methods for controlling software or web browsers, hence a score of 4 instead of 5.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17478" target="_blank">ROSE: Register Assisted General Time Series Forecasting with Decomposed Frequency Learning</a></h3>
            <a href="https://arxiv.org/html/2405.17478v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17478v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yihang Wang, Yuying Qiu, Peng Chen, Kai Zhao, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</p>
            <p><strong>Summary:</strong> arXiv:2405.17478v1 Announce Type: new 
Abstract: With the increasing collection of time series data from various domains, there arises a strong demand for general time series forecasting models pre-trained on a large number of time-series datasets to support a variety of downstream prediction tasks. Enabling general time series forecasting faces two challenges: how to obtain unified representations from multi-domian time series data, and how to capture domain-specific features from time series data across various domains for adaptive transfer in downstream tasks. To address these challenges, we propose a Register Assisted General Time Series Forecasting Model with Decomposed Frequency Learning (ROSE), a novel pre-trained model for time series forecasting. ROSE employs Decomposed Frequency Learning for the pre-training task, which decomposes coupled semantic and periodic information in time series with frequency-based masking and reconstruction to obtain unified representations across domains. We also equip ROSE with a Time Series Register, which learns to generate a register codebook to capture domain-specific representations during pre-training and enhances domain-adaptive transfer by selecting related register tokens on downstream tasks. After pre-training on large-scale time series data, ROSE achieves state-of-the-art forecasting performance on 8 real-world benchmarks. Remarkably, even in few-shot scenarios, it demonstrates competitive or superior performance compared to existing methods trained with full data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17478">https://arxiv.org/abs/2405.17478</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces ROSE, a new pre-trained model developed specifically for time series forecasting. It is relevant to your interests as it proposes a new method for time series that includes unified representations across various time series domains via decomposed frequency learning and a Register to capture domain-specific representations. This paper also discusses large-scale time series data which aligns with your interest in dataset for training foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17951" target="_blank">Efficient Time Series Processing for Transformers and State-Space Models through Token Merging</a></h3>
            <a href="https://arxiv.org/html/2405.17951v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17951v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Leon G\"otz, Marcel Kollovieh, Stephan G\"unnemann, Leo Schwinn</p>
            <p><strong>Summary:</strong> arXiv:2405.17951v1 Announce Type: new 
Abstract: Transformer architectures have shown promising results in time series processing. However, despite recent advances in subquadratic attention mechanisms or state-space models, processing very long sequences still imposes significant computational requirements. Token merging, which involves replacing multiple tokens with a single one calculated as their linear combination, has shown to considerably improve the throughput of vision transformer architectures while maintaining accuracy. In this work, we go beyond computer vision and perform the first investigations of token merging in time series analysis on both time series transformers and state-space models. To effectively scale token merging to long sequences, we introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, adjusting the computational complexity from linear to quadratic based on the neighborhood size. Our comprehensive empirical evaluation demonstrates that token merging offers substantial computational benefits with minimal impact on accuracy across various models and datasets. On the recently proposed Chronos foundation model, we achieve accelerations up to 5400% with only minor accuracy degradations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17951">https://arxiv.org/abs/2405.17951</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it introduces a new method for implementing transformers efficiently in time series analysis. It fits specifically under your subtopic 'New transformer-like models for time series'. The paper also addresses the challenge of processing long sequences, which is essential in the field of time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18036" target="_blank">ForecastGrapher: Redefining Multivariate Time Series Forecasting with Graph Neural Networks</a></h3>
            <a href="https://arxiv.org/html/2405.18036v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18036v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wanlin Cai, Kun Wang, Hao Wu, Xiaoxu Chen, Yuankai Wu</p>
            <p><strong>Summary:</strong> arXiv:2405.18036v1 Announce Type: new 
Abstract: The challenge of effectively learning inter-series correlations for multivariate time series forecasting remains a substantial and unresolved problem. Traditional deep learning models, which are largely dependent on the Transformer paradigm for modeling long sequences, often fail to integrate information from multiple time series into a coherent and universally applicable model. To bridge this gap, our paper presents ForecastGrapher, a framework reconceptualizes multivariate time series forecasting as a node regression task, providing a unique avenue for capturing the intricate temporal dynamics and inter-series correlations. Our approach is underpinned by three pivotal steps: firstly, generating custom node embeddings to reflect the temporal variations within each series; secondly, constructing an adaptive adjacency matrix to encode the inter-series correlations; and thirdly, augmenting the GNNs' expressive power by diversifying the node feature distribution. To enhance this expressive power, we introduce the Group Feature Convolution GNN (GFC-GNN). This model employs a learnable scaler to segment node features into multiple groups and applies one-dimensional convolutions with different kernel lengths to each group prior to the aggregation phase. Consequently, the GFC-GNN method enriches the diversity of node feature distribution in a fully end-to-end fashion. Through extensive experiments and ablation studies, we show that ForecastGrapher surpasses strong baselines and leading published techniques in the domain of multivariate time series forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18036">https://arxiv.org/abs/2405.18036</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant as it proposes a new deep learning method, ForecastGrapher, for time series forecasting. It introduces new approach for learning inter-series correlations in multivariate time series forecasting that might appeal to your interest in new foundation models and transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18165" target="_blank">Time Series Representation Models</a></h3>
            <a href="https://arxiv.org/html/2405.18165v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18165v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Robert Leppich, Vanessa Borst, Veronika Lesch, Samuel Kounev</p>
            <p><strong>Summary:</strong> arXiv:2405.18165v1 Announce Type: new 
Abstract: Time series analysis remains a major challenge due to its sparse characteristics, high dimensionality, and inconsistent data quality. Recent advancements in transformer-based techniques have enhanced capabilities in forecasting and imputation; however, these methods are still resource-heavy, lack adaptability, and face difficulties in integrating both local and global attributes of time series. To tackle these challenges, we propose a new architectural concept for time series analysis based on introspection. Central to this concept is the self-supervised pretraining of Time Series Representation Models (TSRMs), which once learned can be easily tailored and fine-tuned for specific tasks, such as forecasting and imputation, in an automated and resource-efficient manner. Our architecture is equipped with a flexible and hierarchical representation learning process, which is robust against missing data and outliers. It can capture and learn both local and global features of the structure, semantics, and crucial patterns of a given time series category, such as heart rate data. Our learned time series representation models can be efficiently adapted to a specific task, such as forecasting or imputation, without manual intervention. Furthermore, our architecture's design supports explainability by highlighting the significance of each input value for the task at hand. Our empirical study using four benchmark datasets shows that, compared to investigated state-of-the-art baseline methods, our architecture improves imputation and forecasting errors by up to 90.34% and 71.54%, respectively, while reducing the required trainable parameters by up to 92.43%. The source code is available at https://github.com/RobertLeppich/TSRM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18165">https://arxiv.org/abs/2405.18165</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper relates to your interests in 'New deep learning methods for time series', 'New foundation models for time series', and 'Datasets to train foundation models for time series'. It provides insights into a new concept for time series analysis and offers a self-supervised pretraining of Time Series Representation Models (TSRMs), aligning with your expressed interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.03589" target="_blank">TimeGPT-1</a></h3>
            <a href="https://arxiv.org/html/2310.03589v3/extracted/5624779/images/single_forecasting.png" target="_blank"><img src="https://arxiv.org/html/2310.03589v3/extracted/5624779/images/single_forecasting.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Azul Garza, Cristian Challu, Max Mergenthaler-Canseco</p>
            <p><strong>Summary:</strong> arXiv:2310.03589v3 Announce Type: replace 
Abstract: In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.03589">https://arxiv.org/abs/2310.03589</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents TimeGPT, a novel model for time series, and compares it with other traditional methods indicating its efficiency and performance. It falls under your interest of 'New foundation models for time series' and 'New deep learning methods for time series'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13575" target="_blank">PDMLP: Patch-based Decomposed MLP for Long-Term Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2405.13575v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.13575v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peiwang Tang, Weitai Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.13575v2 Announce Type: replace 
Abstract: Recent studies have attempted to refine the Transformer architecture to demonstrate its effectiveness in Long-Term Time Series Forecasting (LTSF) tasks. Despite surpassing many linear forecasting models with ever-improving performance, we remain skeptical of Transformers as a solution for LTSF. We attribute the effectiveness of these models largely to the adopted Patch mechanism, which enhances sequence locality to an extent yet fails to fully address the loss of temporal information inherent to the permutation-invariant self-attention mechanism. Further investigation suggests that simple linear layers augmented with the Patch mechanism may outperform complex Transformer-based LTSF models. Moreover, diverging from models that use channel independence, our research underscores the importance of cross-variable interactions in enhancing the performance of multivariate time series forecasting. The interaction information between variables is highly valuable but has been misapplied in past studies, leading to suboptimal cross-variable models. Based on these insights, we propose a novel and simple Patch-based Decomposed MLP (PDMLP) for LTSF tasks. Specifically, we employ simple moving averages to extract smooth components and noise-containing residuals from time series data, engaging in semantic information interchange through channel mixing and specializing in random noise with channel independence processing. The PDMLP model consistently achieves state-of-the-art results on several real-world datasets. We hope this surprising finding will spur new research directions in the LTSF field and pave the way for more efficient and concise solutions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13575">https://arxiv.org/abs/2405.13575</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper significantly covers your interests, as it introduces a new deep learning method, PDMLP, specifically designed for long-term time series forecasting. It also addresses the issue of interaction information between variables, which can be seen as proposing a new method for modeling multivariate time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13063" target="_blank">Aurora: A Foundation Model of the Atmosphere</a></h3>
            <a href="https://arxiv.org/html/2405.13063v2/extracted/5627057/figures/overview.png" target="_blank"><img src="https://arxiv.org/html/2405.13063v2/extracted/5627057/figures/overview.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cristian Bodnar, Wessel P. Bruinsma, Ana Lucic, Megan Stanley, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Anna Vaughan, Jayesh K. Gupta, Kit Tambiratnam, Alex Archibald, Elizabeth Heider, Max Welling, Richard E. Turner, Paris Perdikaris</p>
            <p><strong>Summary:</strong> arXiv:2405.13063v2 Announce Type: replace-cross 
Abstract: Deep learning foundation models are revolutionizing many facets of science by leveraging vast amounts of data to learn general-purpose representations that can be adapted to tackle diverse downstream tasks. Foundation models hold the promise to also transform our ability to model our planet and its subsystems by exploiting the vast expanse of Earth system data. Here we introduce Aurora, a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data. Aurora leverages the strengths of the foundation modelling approach to produce operational forecasts for a wide variety of atmospheric prediction problems, including those with limited training data, heterogeneous variables, and extreme events. In under a minute, Aurora produces 5-day global air pollution predictions and 10-day high-resolution weather forecasts that outperform state-of-the-art classical simulation tools and the best specialized deep learning models. Taken together, these results indicate that foundation models can transform environmental forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13063">https://arxiv.org/abs/2405.13063</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interests as it introduces Aurora, a large-scale foundation model trained on diverse weather and climate data for time series forecasting. It proposes a new deep learning model for time series and uses a dataset for its training, fitting your criteria perfectly.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17766" target="_blank">SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals</a></h3>
            <a href="https://arxiv.org/html/2405.17766v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17766v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rahul Thapa, Bryan He, Magnus Ruud Kjaer, Hyatt Moore, Gauri Ganjoo, Emmanuel Mignot, James Zou</p>
            <p><strong>Summary:</strong> arXiv:2405.17766v1 Announce Type: new 
Abstract: Sleep is a complex physiological process evaluated through various modalities recording electrical brain, cardiac, and respiratory activities. We curate a large polysomnography dataset from over 14,000 participants comprising over 100,000 hours of multi-modal sleep recordings. Leveraging this extensive dataset, we developed SleepFM, the first multi-modal foundation model for sleep analysis. We show that a novel leave-one-out approach for contrastive learning significantly improves downstream task performance compared to representations from standard pairwise contrastive learning. A logistic regression model trained on SleepFM's learned embeddings outperforms an end-to-end trained convolutional neural network (CNN) on sleep stage classification (macro AUROC 0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned embeddings achieve 48% top-1 average accuracy in retrieving the corresponding recording clips of other modalities from 90,000 candidates. This work demonstrates the value of holistic multi-modal sleep modeling to fully capture the richness of sleep recordings. SleepFM is open source and available at https://github.com/rthapa84/sleepfm-codebase.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17766">https://arxiv.org/abs/2405.17766</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it introduces SleepFM, a new model for time series analysis in the context of sleep data. It's a multi-modal model, which aligns with your interest, but you might find it less relevant as it's focused on application in sleep analysis rather than proposing purely theoretical methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17898" target="_blank">FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction</a></h3>
            <a href="https://arxiv.org/html/2405.17898v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17898v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang</p>
            <p><strong>Summary:</strong> arXiv:2405.17898v1 Announce Type: new 
Abstract: The objective of traffic prediction is to accurately forecast and analyze the dynamics of transportation patterns, considering both space and time. However, the presence of distribution shift poses a significant challenge in this field, as existing models struggle to generalize well when faced with test data that significantly differs from the training distribution. To tackle this issue, this paper introduces a simple and universal spatio-temporal prompt-tuning framework-FlashST, which adapts pre-trained models to the specific characteristics of diverse downstream datasets, improving generalization in diverse traffic prediction scenarios. Specifically, the FlashST framework employs a lightweight spatio-temporal prompt network for in-context learning, capturing spatio-temporal invariant knowledge and facilitating effective adaptation to diverse scenarios. Additionally, we incorporate a distribution mapping mechanism to align the data distributions of pre-training and downstream data, facilitating effective knowledge transfer in spatio-temporal forecasting. Empirical evaluations demonstrate the effectiveness of our FlashST across different spatio-temporal prediction tasks using diverse urban datasets. Code is available at https://github.com/HKUDS/FlashST.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17898">https://arxiv.org/abs/2405.17898</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper would be relevant as it introduces a new framework (FlashST) for traffic prediction which accounts for spatio-temporal variability. It thus aligns to your interest in novel models for time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17444" target="_blank">Towards Gradient-based Time-Series Explanations through a SpatioTemporal Attention Network</a></h3>
            <a href="https://arxiv.org/html/2405.17444v1/extracted/5603959/figures/overall_flow.png" target="_blank"><img src="https://arxiv.org/html/2405.17444v1/extracted/5603959/figures/overall_flow.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Min Hun Lee</p>
            <p><strong>Summary:</strong> arXiv:2405.17444v1 Announce Type: cross 
Abstract: In this paper, we explore the feasibility of using a transformer-based, spatiotemporal attention network (STAN) for gradient-based time-series explanations. First, we trained the STAN model for video classifications using the global and local views of data and weakly supervised labels on time-series data (i.e. the type of an activity). We then leveraged a gradient-based XAI technique (e.g. saliency map) to identify salient frames of time-series data. According to the experiments using the datasets of four medically relevant activities, the STAN model demonstrated its potential to identify important frames of videos.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17444">https://arxiv.org/abs/2405.17444</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new transformer-like model (SpatioTemporal Attention Network, STAN) for time-series data. It is also relevant to your interest in multimodal methods for time series as it uses both global and local views of data. However, the focus is more on explanation rather than forecasting, hence the rating of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17455" target="_blank">WeatherFormer: A Pretrained Encoder Model for Learning Robust Weather Representations from Small Datasets</a></h3>
            
            <p><strong>Authors:</strong> Adib Hasan, Mardavij Roozbehani, Munther Dahleh</p>
            <p><strong>Summary:</strong> arXiv:2405.17455v1 Announce Type: cross 
Abstract: This paper introduces WeatherFormer, a transformer encoder-based model designed to learn robust weather features from minimal observations. It addresses the challenge of modeling complex weather dynamics from small datasets, a bottleneck for many prediction tasks in agriculture, epidemiology, and climate science. WeatherFormer was pretrained on a large pretraining dataset comprised of 39 years of satellite measurements across the Americas. With a novel pretraining task and fine-tuning, WeatherFormer achieves state-of-the-art performance in county-level soybean yield prediction and influenza forecasting. Technical innovations include a unique spatiotemporal encoding that captures geographical, annual, and seasonal variations, adapting the transformer architecture to continuous weather data, and a pretraining strategy to learn representations that are robust to missing weather features. This paper for the first time demonstrates the effectiveness of pretraining large transformer encoder models for weather-dependent applications across multiple domains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17455">https://arxiv.org/abs/2405.17455</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new transformer-like model, WeatherFormer, for predictive tasks related to weather. Considering it's about modeling complex weather dynamics leveraging a transformer-based model, it aligns with your interest in 'new transformer-like models for time series'. However, it doesn't quite delve into the other explicit subtopics you mentioned, hence it scores 4 not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17516" target="_blank">Time Elastic Neural Networks</a></h3>
            
            <p><strong>Authors:</strong> Pierre-Fran\c{c}ois Marteau (EXPRESSION)</p>
            <p><strong>Summary:</strong> arXiv:2405.17516v1 Announce Type: cross 
Abstract: We introduce and detail an atypical neural network architecture, called time elastic neural network (teNN), for multivariate time series classification. The novelty compared to classical neural network architecture is that it explicitly incorporates time warping ability, as well as a new way of considering attention. In addition, this architecture is capable of learning a dropout strategy, thus optimizing its own architecture.Behind the design of this architecture, our overall objective is threefold: firstly, we are aiming at improving the accuracy of instance based classification approaches that shows quite good performances as far as enough training data is available. Secondly we seek to reduce the computational complexity inherent to these methods to improve their scalability. Ideally, we seek to find an acceptable balance between these first two criteria. And finally, we seek to enhance the explainability of the decision provided by this kind of neural architecture.The experiment demonstrates that the stochastic gradient descent implemented to train a teNN is quite effective. To the extent that the selection of some critical meta-parameters is correct, convergence is generally smooth and fast.While maintaining good accuracy, we get a drastic gain in scalability by first reducing the required number of reference time series, i.e. the number of teNN cells required. Secondly, we demonstrate that, during the training process, the teNN succeeds in reducing the number of neurons required within each cell. Finally, we show that the analysis of the activation and attention matrices as well as the reference time series after training provides relevant information to interpret and explain the classification results.The comparative study that we have carried out and which concerns around thirty diverse and multivariate datasets shows that the teNN obtains results comparable to those of the state of the art, in particular similar to those of a network mixing LSTM and CNN architectures for example.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17516">https://arxiv.org/abs/2405.17516</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new neural network architecture for time series classification, which is in line with your interest in new deep learning methods for time series. Though it doesn't specifically mention forecasting or transformer-like models, it mentions implementing time warping and attention in a new way, which might be insightful for you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.02646" target="_blank">SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting</a></h3>
            <a href="https://arxiv.org/html/2312.02646v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.02646v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaobei Zou, Luolin Xiong, Yang Tang, J\"urgen Kurths</p>
            <p><strong>Summary:</strong> arXiv:2312.02646v3 Announce Type: replace 
Abstract: Spatio-temporal forecasting in various domains, like traffic prediction and weather forecasting, is a challenging endeavor, primarily due to the difficulties in modeling propagation dynamics and capturing high-dimensional interactions among nodes. Despite the significant strides made by graph-based networks in spatio-temporal forecasting, there remain two pivotal factors closely related to forecasting performance that need further consideration: time delays in propagation dynamics and multi-scale high-dimensional interactions. In this work, we present a Series-Aligned Multi-Scale Graph Learning (SAMSGL) framework, aiming to enhance forecasting performance. In order to handle time delays in spatial interactions, we propose a series-aligned graph convolution layer to facilitate the aggregation of non-delayed graph signals, thereby mitigating the influence of time delays for the improvement in accuracy. To understand global and local spatio-temporal interactions, we develop a spatio-temporal architecture via multi-scale graph learning, which encompasses two essential components: multi-scale graph structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale graph structure learning includes a global graph structure to learn both delayed and non-delayed node embeddings, as well as a local one to learn node variations influenced by neighboring factors. The Graph-FC blocks synergistically fuse spatial and temporal information to boost prediction accuracy. To evaluate the performance of SAMSGL, we conduct experiments on meteorological and traffic forecasting datasets, which demonstrate its effectiveness and superiority.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.02646">https://arxiv.org/abs/2312.02646</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses Series-Aligned Multi-Scale Graph Learning (SAMSGL), a new method for spatio-temporal forecasting which could be seen as a deep learning method for time series. However, it is more focused on handling the intricacies of time delays and high-dimensional interactions in this context, rather than a pure time series forecasting context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.04234" target="_blank">Graph Convolutions Enrich the Self-Attention in Transformers!</a></h3>
            <a href="https://arxiv.org/html/2312.04234v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.04234v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee, Nathaniel Trask, Noseong Park</p>
            <p><strong>Summary:</strong> arXiv:2312.04234v3 Announce Type: replace 
Abstract: Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph regression, speech recognition, and code classification.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.04234">https://arxiv.org/abs/2312.04234</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Although this paper does not introduce a method specifically for time series, it introduces a new self-attention mechanism from a graph signal processing (GSP) perspective that improves the performance of transformers across various fields. This might include time-series modeling, making it potentially relevant and interesting for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.09793" target="_blank">PatchAD: A Lightweight Patch-based MLP-Mixer for Time Series Anomaly Detection</a></h3>
            <a href="https://arxiv.org/html/2401.09793v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.09793v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</p>
            <p><strong>Summary:</strong> arXiv:2401.09793v5 Announce Type: replace 
Abstract: Anomaly detection in time series analysis is a pivotal task, yet it poses the challenge of discerning normal and abnormal patterns in label-deficient scenarios. While prior studies have largely employed reconstruction-based approaches, which limits the models' representational capacities. Moreover, existing deep learning-based methods are not sufficiently lightweight. Addressing these issues, we present PatchAD, our novel, highly efficient multiscale patch-based MLP-Mixer architecture that utilizes contrastive learning for representation extraction and anomaly detection. With its four distinct MLP Mixers and innovative dual project constraint module, PatchAD mitigates potential model degradation and offers a lightweight solution, requiring only $3.2$MB. Its efficacy is demonstrated by state-of-the-art results across $9$ datasets sourced from different application scenarios, outperforming over $30$ comparative algorithms. PatchAD significantly improves the classical F1 score by $50.5\%$, the Aff-F1 score by $7.8\%$, and the AUC by $10.0\%$. The code is publicly available. \url{https://github.com/EmorZz1G/PatchAD}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.09793">https://arxiv.org/abs/2401.09793</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a novel deep learning-based method for time series analysis, specifically for anomaly detection, which you may find interesting although it's not explicitly about forecasting. It also introduces a lightweight architecture, which may be of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13937" target="_blank">DyGPrompt: Learning Feature and Time Prompts on Dynamic Graphs</a></h3>
            <a href="https://arxiv.org/html/2405.13937v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.13937v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xingtong Yu, Zhenghao Liu, Yuan Fang, Xinming Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.13937v3 Announce Type: replace 
Abstract: Dynamic graphs are pervasive in the real world, modeling dynamic relations between objects across various fields. For dynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a mainstream technique, which are generally pre-trained on the link prediction task, leaving a significant gap from the objectives of downstream tasks such as node classification. To bridge the gap, prompt-based learning has gained traction on graphs. However, existing efforts focus on static graphs, neglecting the evolution of dynamic graphs. In this paper, we propose DyGPrompt, a novel pre-training and prompting framework for dynamic graph modeling. First, we design dual prompts to address the gap in both task objectives and dynamic variations across pre-training and downstream tasks. Second, we recognize that node and time features mutually characterize each other, and propose dual condition-nets to model the evolving node-time patterns in downstream tasks. Finally, we thoroughly evaluate and analyze DyGPrompt through extensive experiments on three public datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13937">https://arxiv.org/abs/2405.13937</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new pre-training and prompting framework for dynamic graph modeling which appears to be a novel approach to time series. However, it does not seem to specifically address the use of foundation models or transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13956" target="_blank">Attention as an RNN</a></h3>
            <a href="https://arxiv.org/html/2405.13956v2/extracted/5625307/figs/many_to_one_Att-RNN.png" target="_blank"><img src="https://arxiv.org/html/2405.13956v2/extracted/5625307/figs/many_to_one_Att-RNN.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Yoshua Bengio, Greg Mori</p>
            <p><strong>Summary:</strong> arXiv:2405.13956v2 Announce Type: replace 
Abstract: The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13956">https://arxiv.org/abs/2405.13956</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents a new deep learning approach for sequence tasks including time-series forecasting. It introduces an attention-based module that is both trainable in parallel and is memory-efficient for inference, which could potentially be beneficial for large-scale time series forecasting.</p>
        </div>
        </div><div class='timestamp'>Report generated on May 29, 2024 at 21:49:09</div></body></html>