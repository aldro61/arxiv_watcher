
            <html>
            <head>
                <title>Report Generated on June 03, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for June 03, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20482" target="_blank">Leveraging Structure Between Environments: Phylogenetic Regularization Incentivizes Disentangled Representations</a></h3>
            <a href="https://arxiv.org/html/2405.20482v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20482v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Elliot Layne, Jason Hartford, S\'ebastien Lachapelle, Mathieu Blanchette, Dhanya Sridhar</p>
            <p><strong>Summary:</strong> arXiv:2405.20482v1 Announce Type: new 
Abstract: Many causal systems such as biological processes in cells can only be observed indirectly via measurements, such as gene expression. Causal representation learning -- the task of correctly mapping low-level observations to latent causal variables -- could advance scientific understanding by enabling inference of latent variables such as pathway activation. In this paper, we develop methods for inferring latent variables from multiple related datasets (environments) and tasks. As a running example, we consider the task of predicting a phenotype from gene expression, where we often collect data from multiple cell types or organisms that are related in known ways. The key insight is that the mapping from latent variables driven by gene expression to the phenotype of interest changes sparsely across closely related environments. To model sparse changes, we introduce Tree-Based Regularization (TBR), an objective that minimizes both prediction error and regularizes closely related environments to learn similar predictors. We prove that under assumptions about the degree of sparse changes, TBR identifies the true latent variables up to some simple transformations. We evaluate the theory empirically with both simulations and ground-truth gene expression data. We find that TBR recovers the latent causal variables better than related methods across these settings, even under settings that violate some assumptions of the theory.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20482">https://arxiv.org/abs/2405.20482</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper speaks directly to the subtopic of 'Causal representation learning'. It develops new methods for inferring latent variables from multiple related datasets (environments) which could be a significant interest for you. It involves learning the causal system indirectly via measurements like gene expression, which falls under the ambit of causal representation learning. However, it doesn't talk directly about large language models in causal discovery, hence a 4 and not a 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21012" target="_blank">G-Transformer for Conditional Average Potential Outcome Estimation over Time</a></h3>
            <a href="https://arxiv.org/html/2405.21012v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.21012v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Konstantin Hess, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</p>
            <p><strong>Summary:</strong> arXiv:2405.21012v1 Announce Type: new 
Abstract: Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task suffer from either (a) bias or (b) large variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model designed for unbiased, low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21012">https://arxiv.org/abs/2405.21012</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> While the paper doesn't focus on machine learning directly, it considers the use of a new neural model (G-Transformer) for estimating potential outcomes over time, which is an aspect of causal discovery. Hence, this might be relevant to your interests in 'causality'. It however doesn't deal with large scale language models or explicit representation learning that you are interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20404" target="_blank">XPrompt:Explaining Large Language Model's Generation via Joint Prompt Attribution</a></h3>
            <a href="https://arxiv.org/html/2405.20404v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20404v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin</p>
            <p><strong>Summary:</strong> arXiv:2405.20404v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of elucidating and explaining the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, XPrompt, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both faithfulness and efficiency of our framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20404">https://arxiv.org/abs/2405.20404</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper seems to align with your interest in 'Causality and Machine Learning', especially in the area of 'causal representation learning' and 'using large language models in causal discovery.' It explores the causality between input and output pairs in Large Language Models (LLMs) and introduces XPrompt, a framework for generating counterfactual explanations based on joint prompt attribution. Though it does not propose a new method, it may offer valuable insights for your research in this field.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.11130" target="_blank">Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable</a></h3>
            
            <p><strong>Authors:</strong> Yuta Kawakami, Manabu Kuroki, Jin Tian</p>
            <p><strong>Summary:</strong> arXiv:2401.11130v2 Announce Type: replace 
Abstract: There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we study conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. Notably, CAPCE is identifiable under a weaker assumption than required by a commonly used measure for estimating heterogeneous causal effects of continuous treatment. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.11130">https://arxiv.org/abs/2401.11130</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses the identification and estimation of conditional average partial causal effects, which clearly fits into your criteria of interest in causal representation learning and causal discovery. Also, it seems like the paper presents new methods for estimating the heterogeneous causal effects of continuous treatment, which might interest you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16069" target="_blank">IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark</a></h3>
            <a href="https://arxiv.org/html/2405.16069v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16069v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fredrik D. Johansson</p>
            <p><strong>Summary:</strong> arXiv:2405.16069v2 Announce Type: replace 
Abstract: Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the \tt IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16069">https://arxiv.org/abs/2405.16069</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it discusses causal effects and methods, aligns with your interest in 'Causal discovery' and 'Causal representation learning'. However, it does not specifically mention the use of machine learning or large language models.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></h3>
            <a href="https://arxiv.org/html/2312.00752v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.00752v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Albert Gu, Tri Dao</p>
            <p><strong>Summary:</strong> arXiv:2312.00752v2 Announce Type: replace 
Abstract: Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest as it proposes a new 'Mamba' model for time series with significant improvements in efficiency and scalability. Furthermore, it addresses subtopics of your interest like new foundation models for time series and multimodal deep learning models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20516" target="_blank">WaveCastNet: An AI-enabled Wavefield Forecasting Framework for Earthquake Early Warning</a></h3>
            <a href="https://arxiv.org/html/2405.20516v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20516v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dongwei Lyu, Rie Nakata, Pu Ren, Michael W. Mahoney, Arben Pitarka, Nori Nakata, N. Benjamin Erichson</p>
            <p><strong>Summary:</strong> arXiv:2405.20516v1 Announce Type: new 
Abstract: Large earthquakes can be destructive and quickly wreak havoc on a landscape. To mitigate immediate threats, early warning systems have been developed to alert residents, emergency responders, and critical infrastructure operators seconds to a minute before seismic waves arrive. These warnings provide time to take precautions and prevent damage. The success of these systems relies on fast, accurate predictions of ground motion intensities, which is challenging due to the complex physics of earthquakes, wave propagation, and their intricate spatial and temporal interactions. To improve early warning, we propose a novel AI-enabled framework, WaveCastNet, for forecasting ground motions from large earthquakes. WaveCastNet integrates a novel convolutional Long Expressive Memory (ConvLEM) model into a sequence to sequence (seq2seq) forecasting framework to model long-term dependencies and multi-scale patterns in both space and time. WaveCastNet, which shares weights across spatial and temporal dimensions, requires fewer parameters compared to more resource-intensive models like transformers and thus, in turn, reduces inference times. Importantly, WaveCastNet also generalizes better than transformer-based models to different seismic scenarios, including to more rare and critical situations with higher magnitude earthquakes. Our results using simulated data from the San Francisco Bay Area demonstrate the capability to rapidly predict the intensity and timing of destructive ground motions. Importantly, our proposed approach does not require estimating earthquake magnitudes and epicenters, which are prone to errors using conventional approaches; nor does it require empirical ground motion models, which fail to capture strongly heterogeneous wave propagation effects.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20516">https://arxiv.org/abs/2405.20516</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'New deep learning methods for time series'. While it doesn't address all your subtopics, it introduces a model called 'WaveCastNet' which applies deep learning (seq2seq and ConvLEM) for wavefield forecasting in the context of earthquakes. Although the application is not general forecasting, the methodology might be insightful.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20761" target="_blank">Share Your Secrets for Privacy! Confidential Forecasting with Vertical Federated Learning</a></h3>
            <a href="https://arxiv.org/html/2405.20761v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20761v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aditya Shankar, Lydia Y. Chen, J\'er\'emie Decouchant, Dimitra Gkorou, Rihan Hai</p>
            <p><strong>Summary:</strong> arXiv:2405.20761v1 Announce Type: new 
Abstract: Vertical federated learning (VFL) is a promising area for time series forecasting in industrial applications, such as predictive maintenance and machine control. Critical challenges to address in manufacturing include data privacy and over-fitting on small and noisy datasets during both training and inference. Additionally, to increase industry adaptability, such forecasting models must scale well with the number of parties while ensuring strong convergence and low-tuning complexity. We address those challenges and propose 'Secret-shared Time Series Forecasting with VFL' (STV), a novel framework that exhibits the following key features: i) a privacy-preserving algorithm for forecasting with SARIMAX and autoregressive trees on vertically partitioned data; ii) serverless forecasting using secret sharing and multi-party computation; iii) novel N-party algorithms for matrix multiplication and inverse operations for direct parameter optimization, giving strong convergence with minimal hyperparameter tuning complexity. We conduct evaluations on six representative datasets from public and industry-specific contexts. Our results demonstrate that STV's forecasting accuracy is comparable to those of centralized approaches. They also show that our direct optimization can outperform centralized methods, which include state-of-the-art diffusion models and long-short-term memory, by 23.81% on forecasting accuracy. We also conduct a scalability analysis by examining the communication costs of direct and iterative optimization to navigate the choice between the two. Code and appendix are available: https://github.com/adis98/STV</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20761">https://arxiv.org/abs/2405.20761</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in 'Time series and deep learning', specifically dealing with forecasting. Although it does not seem to introduce a novel deep learning method, it does present a new framework 'STV' for vertical federated learning in the context of time series forecasting, which would be beneficial to understand recent trends in the field.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21060" target="_blank">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</a></h3>
            
            <p><strong>Authors:</strong> Tri Dao, Albert Gu</p>
            <p><strong>Summary:</strong> arXiv:2405.21060v1 Announce Type: new 
Abstract: While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21060">https://arxiv.org/abs/2405.21060</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> While this paper might not overtly mention time-series, the methods discussed within it, specifically the deep-learning architecture (Mamba-2), can be employed for time-series forecasting. The stated efficiency and competitiveness indicate it's a new method to consider in the realm of deep learning for time-series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20348" target="_blank">Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2405.20348v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20348v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shengchao Chen, Guodong Long, Jing Jiang, Chengqi Zhang</p>
            <p><strong>Summary:</strong> arXiv:2405.20348v1 Announce Type: cross 
Abstract: This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variables modeling. We present LM-Weather, a generic approach to taming PLMs, that have learned massive sequential knowledge from the universe of natural language databases, to acquire an immediate capability to obtain highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness. During communication between clients and the server, low-rank-based transmission is performed to effectively fuse the global knowledge among devices while maintaining high communication efficiency and ensuring privacy. Experiments on real-wold dataset show that LM-Weather outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales). We provide extensive and in-depth analyses experiments, which verify that LM-Weather can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20348">https://arxiv.org/abs/2405.20348</a></p>
            <p><strong>Category:</strong> physics.ao-ph</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your 'time series and deep learning' interest, particularly in relation to 'New foundation models for time series' and 'Datasets to train foundation models for time series'. It introduces a new model, LM-Weather, for meteorological time series forecasting and discusses the used dataset. However, the paper's main focus seems to be meteorology application, which might be less in line with your preference for new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20799" target="_blank">Rough Transformers: Lightweight Continuous-Time Sequence Modelling with Path Signatures</a></h3>
            
            <p><strong>Authors:</strong> Fernando Moreno-Pino, \'Alvaro Arroyo, Harrison Waldon, Xiaowen Dong, \'Alvaro Cartea</p>
            <p><strong>Summary:</strong> arXiv:2405.20799v1 Announce Type: cross 
Abstract: Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequence-based recurrent models struggle. To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose \textit{multi-view signature attention}, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20799">https://arxiv.org/abs/2405.20799</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it presents a new transformer-like model (Rough Transformer) for handling time series data, specifically addressing the challenge of long-range dependencies and non-uniform intervals.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2301.06650" target="_blank">Enhancing Deep Traffic Forecasting Models with Dynamic Regression</a></h3>
            <a href="https://arxiv.org/html/2301.06650v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2301.06650v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vincent Zhihao Zheng, Seongjin Choi, Lijun Sun</p>
            <p><strong>Summary:</strong> arXiv:2301.06650v2 Announce Type: replace 
Abstract: Deep learning models for traffic forecasting often assume the residual is independent and isotropic across time and space. This assumption simplifies loss functions such as mean absolute error, but real-world residual processes often exhibit significant autocorrelation and structured spatiotemporal correlation. This paper introduces a dynamic regression (DR) framework to enhance existing spatiotemporal traffic forecasting models by incorporating structured learning for the residual process. We assume the residual of the base model (i.e., a well-developed traffic forecasting model) follows a matrix-variate seasonal autoregressive (AR) model, which is seamlessly integrated into the training process through the redesign of the loss function. Importantly, the parameters of the DR framework are jointly optimized alongside the base model. We evaluate the effectiveness of the proposed framework on state-of-the-art (SOTA) deep traffic forecasting models using both speed and flow datasets, demonstrating improved performance and providing interpretable AR coefficients and spatiotemporal covariance matrices.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2301.06650">https://arxiv.org/abs/2301.06650</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is about improving deep learning models for traffic forecasting, which falls under your interest in 'new deep learning methods for time series'. While it's not about a completely new model, it enhances existing models with a new approach, dynamic regression.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.14906" target="_blank">BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition</a></h3>
            <a href="https://arxiv.org/html/2308.14906v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.14906v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shikai Fang, Qingsong Wen, Yingtao Luo, Shandian Zhe, Liang Sun</p>
            <p><strong>Summary:</strong> arXiv:2308.14906v3 Announce Type: replace 
Abstract: In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with different patterns. We apply a group of Gaussian Processes (GPs) with different kernels as functional priors to fit the factors. For computational efficiency, we further convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE), and developing a scalable algorithm for online inference. The proposed method can not only handle imputation over arbitrary time stamps, but also offer uncertainty quantification and interpretability for the downstream application. We evaluate our method on both synthetic and real-world datasets.We release the code at {https://github.com/xuangu-fang/BayOTIDE}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.14906">https://arxiv.org/abs/2308.14906</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper could be relevant because it proposes a new method for handling time-series data, specifically focusing on imputation for missing values. It also addresses issues related to irregular time-stamps and streaming data, which fall under time-series analysis. It doesn't directly mention deep learning or forecasting though, which is why it doesn't score a 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18886" target="_blank">A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</a></h3>
            <a href="https://arxiv.org/html/2404.18886v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.18886v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao Liu, Bin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen</p>
            <p><strong>Summary:</strong> arXiv:2404.18886v2 Announce Type: replace 
Abstract: The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18886">https://arxiv.org/abs/2404.18886</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper reviews diffusion models in time series and spatio-temporal data, but it goes into detail on categorized methods and includes various applications. It is an extensive review rather than proposing a novel method but could provide a solid foundation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18669" target="_blank">Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities</a></h3>
            <a href="https://arxiv.org/html/2405.18669v2/extracted/5635126/zipper_new_figure.png" target="_blank"><img src="https://arxiv.org/html/2405.18669v2/extracted/5635126/zipper_new_figure.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vicky Zayats, Peter Chen, Melissa Ferrari, Dirk Padfield</p>
            <p><strong>Summary:</strong> arXiv:2405.18669v2 Announce Type: replace 
Abstract: Integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges. Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities.
  We propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In our experiments fusing speech and text modalities, we show the proposed architecture performs very competitively in scenarios with limited aligned text-speech data. We also showcase the flexibility of our model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text). In cross-modal tasks such as automatic speech recognition (ASR) where the output modality is text, we show that freezing the text backbone results in negligible performance degradation. In cross-modal tasks such as text-to-speech generation (TTS) where the output modality is speech, we show that using a pre-trained speech backbone results in superior performance to the baseline.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18669">https://arxiv.org/abs/2405.18669</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper 'Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities' may be of interest to you as it discusses a new multimodal deep learning model for integrating different generative foundation models. Although it does not seem to directly discuss time series, the multi-tower decoder architecture presented could potentially be applied to multi-modal time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01000" target="_blank">Multivariate Probabilistic Time Series Forecasting with Correlated Errors</a></h3>
            <a href="https://arxiv.org/html/2402.01000v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.01000v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vincent Zhihao Zheng, Lijun Sun</p>
            <p><strong>Summary:</strong> arXiv:2402.01000v3 Announce Type: replace-cross 
Abstract: Accurately modeling the correlation structure of errors is essential for reliable uncertainty quantification in probabilistic time series forecasting. Recent deep learning models for multivariate time series have developed efficient parameterizations for time-varying contemporaneous covariance, but they often assume temporal independence of errors for simplicity. However, real-world data frequently exhibit significant error autocorrelation and cross-lag correlation due to factors such as missing covariates. In this paper, we present a plug-and-play method that learns the covariance structure of errors over multiple steps for autoregressive models with Gaussian-distributed errors. To achieve scalable inference and computational efficiency, we model the contemporaneous covariance using a low-rank-plus-diagonal parameterization and characterize cross-covariance through a group of independent latent temporal processes. The learned covariance matrix can be used to calibrate predictions based on observed residuals. We evaluate our method on probabilistic models built on RNN and Transformer architectures, and the results confirm the effectiveness of our approach in enhancing predictive accuracy and uncertainty quantification without significantly increasing the parameter size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01000">https://arxiv.org/abs/2402.01000</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a novel method to improve uncertainty quantification in probabilistic time series forecasting using deep learning models like RNN and Transformer. However, it's more on improving an existing method rather than proposing an entirely new deep learning method for time series. Still, it might be of interest due to its focus on a crucial aspect of time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.01371" target="_blank">eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling</a></h3>
            <a href="https://arxiv.org/html/2403.01371v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.01371v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Matthew Dowling, Yuan Zhao, Il Memming Park</p>
            <p><strong>Summary:</strong> arXiv:2403.01371v2 Announce Type: replace-cross 
Abstract: State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data. State-of-the-art probabilistic approaches are often able to scale to large problems at the cost of flexibility of the variational posterior or expressivity of the dynamics model. However, those consolidations can be detrimental if the ultimate goal is to learn a generative model capable of explaining the spatiotemporal structure of the data and making accurate forecasts. We introduce a low-rank structured variational autoencoding framework for nonlinear Gaussian state-space graphical models capable of capturing dense covariance structures that are important for learning dynamical systems with predictive capabilities. Our inference algorithm exploits the covariance structures that arise naturally from sample based approximate Gaussian message passing and low-rank amortized posterior updates -- effectively performing approximate variational smoothing with time complexity scaling linearly in the state dimensionality. In comparisons with other deep state-space model architectures our approach consistently demonstrates the ability to learn a more predictive generative model. Furthermore, when applied to neural physiological recordings, our approach is able to learn a dynamical system capable of forecasting population spiking and behavioral correlates from a small portion of single trials.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.01371">https://arxiv.org/abs/2403.01371</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper introduces a new method for learning dynamical systems from data with a focus on making accurate forecasts, thus falling under 'New deep learning methods for time series.' Although the paper does not specifically discuss multimodal or transformer-like models, the proposed approach may offer new perspectives on time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.04234" target="_blank">Graph Convolutions Enrich the Self-Attention in Transformers!</a></h3>
            <a href="https://arxiv.org/html/2312.04234v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.04234v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee, Nathaniel Trask, Noseong Park</p>
            <p><strong>Summary:</strong> arXiv:2312.04234v4 Announce Type: replace 
Abstract: Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph regression, speech recognition, and code classification.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.04234">https://arxiv.org/abs/2312.04234</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 3.5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper designs self-attention in transformers from a graph signal processing perspective, which may be relevant for your interest in time series and transformer models. However, the paper does not primarily focus on time series forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.02905" target="_blank">Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers</a></h3>
            <a href="https://arxiv.org/html/2310.02905v2/extracted/5630776/figure/performance-profile-20.png" target="_blank"><img src="https://arxiv.org/html/2310.02905v2/extracted/5630776/figure/performance-profile-20.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</p>
            <p><strong>Summary:</strong> arXiv:2310.02905v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions. Our code is available at https://github.com/xqlin98/INSTINCT.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.02905">https://arxiv.org/abs/2310.02905</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant because it discusses a new instruction optimization method for Large Language Model, which directly relates to automating tasks using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15793" target="_blank">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></h3>
            
            <p><strong>Authors:</strong> John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press</p>
            <p><strong>Summary:</strong> arXiv:2405.15793v2 Announce Type: replace-cross 
Abstract: Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15793">https://arxiv.org/abs/2405.15793</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to the 'llm-agents' topic as it discusses using language models for automating tasks in software engineering. This is aligned with your interest in using large language models for controlling software and computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20692" target="_blank">In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought</a></h3>
            <a href="https://arxiv.org/html/2405.20692v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20692v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sili Huang, Jifeng Hu, Hechang Chen, Lichao Sun, Bo Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.20692v1 Announce Type: new 
Abstract: In-context learning is a promising approach for offline reinforcement learning (RL) to handle online tasks, which can be achieved by providing task prompts. Recent works demonstrated that in-context RL could emerge with self-improvement in a trial-and-error manner when treating RL tasks as an across-episodic sequential prediction problem. Despite the self-improvement not requiring gradient updates, current works still suffer from high computational costs when the across-episodic sequence increases with task horizons. To this end, we propose an In-context Decision Transformer (IDT) to achieve self-improvement in a high-level trial-and-error manner. Specifically, IDT is inspired by the efficient hierarchical structure of human decision-making and thus reconstructs the sequence to consist of high-level decisions instead of low-level actions that interact with environments. As one high-level decision can guide multi-step low-level actions, IDT naturally avoids excessively long sequences and solves online tasks more efficiently. Experimental results show that IDT achieves state-of-the-art in long-horizon tasks over current in-context RL methods. In particular, the online evaluation time of our IDT is \textbf{36$\times$} times faster than baselines in the D4RL benchmark and \textbf{27$\times$} times faster in the Grid World benchmark.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20692">https://arxiv.org/abs/2405.20692</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant for your interest in 'Agents based on large-language models'. It discusses an application of in-context reinforcement learning, which can be a strategy for controlling software with large language models. Even though it's not a direct application on large language models, it provides insights on a method for efficient decision-making which might be useful in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20835" target="_blank">Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.20835v1/extracted/5634473/images/nonsensical/GPTQ_nonsensical_perplexity.jpg" target="_blank"><img src="https://arxiv.org/html/2405.20835v1/extracted/5634473/images/nonsensical/GPTQ_nonsensical_perplexity.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Davide Paglieri, Saurabh Dash, Tim Rockt\"aschel, Jack Parker-Holder</p>
            <p><strong>Summary:</strong> arXiv:2405.20835v1 Announce Type: new 
Abstract: Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various notable open-source LLMs. Calibration sets are crucial for evaluating activation magnitudes and identifying outliers, which can distort the quantization range and negatively impact performance. Our analysis reveals a marked contrast in quantization effectiveness across models. The older OPT model, which much of the quantization literature is based on, shows significant performance deterioration and high susceptibility to outliers with varying calibration sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity to outliers and stable activations. These findings suggest a shift in PTQ strategies might be needed. As advancements in pre-training methods reduce the relevance of outliers, there is an emerging need to reassess the fundamentals of current quantization literature. The emphasis should pivot towards optimizing inference speed, rather than primarily focusing on outlier preservation, to align with the evolving characteristics of state-of-the-art LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20835">https://arxiv.org/abs/2405.20835</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is focused on the efficiency of Large Language Models (LLMs), which are of your interest. Although it doesn't delve into the use of LLMs for controlling software or web browsers, or computer automation, it does discuss the functioning and optimization of LLMs, providing useful insights for your goals in using and developing LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20971" target="_blank">Amortizing intractable inference in diffusion models for vision, language, and control</a></h3>
            
            <p><strong>Authors:</strong> Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, Yoshua Bengio, Glen Berseth, Nikolay Malkin</p>
            <p><strong>Summary:</strong> arXiv:2405.20971v1 Announce Type: new 
Abstract: Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint or likelihood function $r(\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20971">https://arxiv.org/abs/2405.20971</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though this paper touches on several subjects, it talks about the application of large language models (LLMs) in areas such as language infilling and text-to-image generation, and thus falls under your 'Agents based on large-language models' interest. However, it does not specifically address LLMs for controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20973" target="_blank">LCQ: Low-Rank Codebook based Quantization for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.20973v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20973v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wen-Pu Cai, Wu-Jun Li</p>
            <p><strong>Summary:</strong> arXiv:2405.20973v1 Announce Type: new 
Abstract: Large language models~(LLMs) have recently demonstrated promising performance in many tasks. However, the high storage and computational cost of LLMs has become a challenge for deploying LLMs. Weight quantization has been widely used for model compression, which can reduce both storage and computational cost. Most existing weight quantization methods for LLMs use a rank-one codebook for quantization, which results in substantial accuracy loss when the compression ratio is high. In this paper, we propose a novel weight quantization method, called low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a low-rank codebook, the rank of which can be larger than one, for quantization. Experiments show that LCQ can achieve better accuracy than existing methods with a negligibly extra storage cost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20973">https://arxiv.org/abs/2405.20973</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a method to optimize Large Language Models, which can be relevant in the context of using such models for controlling software or web browsers. However, it does not explicitly mention these applications, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21018" target="_blank">Improved Techniques for Optimization-Based Jailbreaking on Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.21018v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.21018v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin</p>
            <p><strong>Summary:</strong> arXiv:2405.21018v1 Announce Type: new 
Abstract: Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed $\mathcal{I}$-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21018">https://arxiv.org/abs/2405.21018</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although it doesn't directly pertain to controlling software or browsers, this paper extensively discusses jailbreaking and security concerns related to Large Language Models (LLMs), which may be pertinent information to gripe in the context of developing and implementing agency with Large Language Models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21046" target="_blank">Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF</a></h3>
            
            <p><strong>Authors:</strong> Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, Alexander Rakhlin</p>
            <p><strong>Summary:</strong> arXiv:2405.21046v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has emerged as a central tool for language model alignment. We consider online exploration in RLHF, which exploits interactive access to human or AI feedback by deliberately encouraging the model to produce diverse, maximally informative responses. By allowing RLHF to confidently stray from the pre-trained model, online exploration offers the possibility of novel, potentially super-human capabilities, but its full potential as a paradigm for language model training has yet to be realized, owing to computational and statistical bottlenecks in directly adapting existing reinforcement learning techniques. We propose a new algorithm for online exploration in RLHF, Exploratory Preference Optimization (XPO), which is simple and practical -- a one-line change to (online) Direct Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the strongest known provable guarantees and promising empirical performance. XPO augments the DPO objective with a novel and principled exploration bonus, empowering the algorithm to explore outside the support of the initial model and human feedback data. In theory, we show that XPO is provably sample-efficient and converges to a near-optimal language model policy under natural exploration conditions, irrespective of whether the initial model has good coverage. Our analysis, which builds on the observation that DPO implicitly performs a form of $Q^{\star}$-approximation (or, Bellman error minimization), combines previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the perspective of KL-regularized Markov decision processes. Empirically, we find that XPO is more sample-efficient than non-exploratory DPO variants in a preliminary evaluation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21046">https://arxiv.org/abs/2405.21046</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language model-based agents. The proposed method, 'Exploratory Preference Optimization', could be potentially used to control software or enhance automation, as it focuses on optimizing language model policies through Reinforcement Learning from Human Feedback (RLHF). However, it is not explicitly about controlling software or web browsers with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20347" target="_blank">Small Language Models for Application Interactions: A Case Study</a></h3>
            <a href="https://arxiv.org/html/2405.20347v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20347v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Beibin Li, Yi Zhang, S\'ebastien Bubeck, Jeevan Pathuri, Ishai Menache</p>
            <p><strong>Summary:</strong> arXiv:2405.20347v1 Announce Type: cross 
Abstract: We study the efficacy of Small Language Models (SLMs) in facilitating application usage through natural language interactions. Our focus here is on a particular internal application used in Microsoft for cloud supply chain fulfilment. Our experiments show that small models can outperform much larger ones in terms of both accuracy and running time, even when fine-tuned on small datasets. Alongside these results, we also highlight SLM-based system design considerations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20347">https://arxiv.org/abs/2405.20347</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper is not about a large language model, it discusses the usage of small language models for application interactions, which is quite relevant to your interest in using large language models for software control. The paper could provide insight about the general trends and techniques in using language models for controlling applications, which might be beneficial when considering large language models specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20495" target="_blank">Transfer Q Star: Principled Decoding for LLM Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.20495v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20495v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, Furong Huang</p>
            <p><strong>Summary:</strong> arXiv:2405.20495v1 Announce Type: cross 
Abstract: Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\pi_{\texttt{sft}}}$ (derived from the reference $\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose Transfer $Q^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\rho_{\texttt{BL}}$ aligned with a baseline reward $\rho_{\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of Transfer $Q^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20495">https://arxiv.org/abs/2405.20495</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper addresses aligning foundation models for safe and trustworthy deployment, a key aspect for designing agents based on large language models. It proposes a new decoding method for better aligning, aligning via decoding, which could be related to controlling software or web browser automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20505" target="_blank">SPOT: Text Source Prediction from Originality Score Thresholding</a></h3>
            <a href="https://arxiv.org/html/2405.20505v1/extracted/5633531/figs/mistral_opt.png" target="_blank"><img src="https://arxiv.org/html/2405.20505v1/extracted/5633531/figs/mistral_opt.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Edouard Yvinec, Gabriel Kasser</p>
            <p><strong>Summary:</strong> arXiv:2405.20505v1 Announce Type: cross 
Abstract: The wide acceptance of large language models (LLMs) has unlocked new applications and social risks. Popular countermeasures aim at detecting misinformation, usually involve domain specific models trained to recognize the relevance of any information. Instead of evaluating the validity of the information, we propose to investigate LLM generated text from the perspective of trust. In this study, we define trust as the ability to know if an input text was generated by a LLM or a human. To do so, we design SPOT, an efficient method, that classifies the source of any, standalone, text input based on originality score. This score is derived from the prediction of a given LLM to detect other LLMs. We empirically demonstrate the robustness of the method to the architecture, training data, evaluation data, task and compression of modern LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20505">https://arxiv.org/abs/2405.20505</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the concept of identifying whether a text was generated by a large language model (LLM) or a human, which is relevant to your interest in agents based on large-language models. However, note that it does not directly cover the subtopics of using LLMs for controlling software or web browsers, or for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20551" target="_blank">EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.20551v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20551v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dorin Pomian, Abhiram Bellur, Malinda Dilhara, Zarina Kurbatova, Egor Bogomolov, Andrey Sokolov, Timofey Bryksin, Danny Dig</p>
            <p><strong>Summary:</strong> arXiv:2405.20551v1 Announce Type: cross 
Abstract: Excessively long methods, loaded with multiple responsibilities, are challenging to understand, debug, reuse, and maintain. The solution lies in the widely recognized Extract Method refactoring. While the application of this refactoring is supported in modern IDEs, recommending which code fragments to extract has been the topic of many research tools. However, they often struggle to replicate real-world developer practices, resulting in recommendations that do not align with what a human developer would do in real life. To address this issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to generate refactoring suggestions and subsequently validates, enhances, and ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected recommendation. In our extensive evaluation of 1,752 real-world refactorings that actually took place in open-source projects, EM-Assist's recall rate was 53.4% among its top-5 recommendations, compared to 39.4% for the previous best-in-class tool that relies solely on static analysis. Moreover, we conducted a usability survey with 18 industrial developers and 94.4% gave a positive rating.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20551">https://arxiv.org/abs/2405.20551</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Agents based on large-language models'. EM-Assist introduces a tool that uses LLMs for automated Extract Method refactoring, which is a form of controlling software. The tool generates and ranks suggestions, which is a novel method using LLMs for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20778" target="_blank">Improved Generation of Adversarial Examples Against Safety-aligned LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.20778v1/extracted/5625418/images/block.png" target="_blank"><img src="https://arxiv.org/html/2405.20778v1/extracted/5625418/images/block.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.20778v1 Announce Type: cross 
Abstract: Despite numerous efforts to ensure large language models (LLMs) adhere to safety standards and produce harmless content, some successes have been achieved in bypassing these restrictions, known as jailbreak attacks against LLMs. Adversarial prompts generated using gradient-based methods exhibit outstanding performance in performing jailbreak attacks automatically. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the white-box setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking black-box image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, i.e., Skip Gradient Method and Intermediate Level Attack, for improving the effectiveness of automatically generated adversarial examples against white-box LLMs. With appropriate adaptations, we inject these ideologies into gradient-based adversarial prompt generation processes and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that the developed combination achieves >30% absolute increase in attack success rates compared with GCG for attacking the Llama-2-7B-Chat model on AdvBench.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20778">https://arxiv.org/abs/2405.20778</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses large language models (LLMs) and introduces methods to improve the generation of adversarial prompts. Though it does not specifically tackle controlling software or web browsers, it is relevant to your interest as it involves developing methods to handle potential exploitation of LLMs, which might have wider implications on your area of control applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20797" target="_blank">Ovis: Structural Embedding Alignment for Multimodal Large Language Model</a></h3>
            <a href="https://arxiv.org/html/2405.20797v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20797v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye</p>
            <p><strong>Summary:</strong> arXiv:2405.20797v1 Announce Type: cross 
Abstract: Current Multimodal Large Language Models (MLLMs) typically integrate a pre-trained LLM with another pre-trained vision transformer through a connector, such as an MLP, endowing the LLM with visual capabilities. However, the misalignment between two embedding strategies in MLLMs -- the structural textual embeddings based on an embedding look-up table and the continuous embeddings generated directly by the vision encoder -- makes challenges for a more seamless fusion of visual and textual information. We propose Ovis, a novel MLLM architecture designed to structurally align visual and textual embeddings. Ovis integrates an additional learnable visual embedding table into the visual encoder's process. To capture rich visual semantics, each image patch indexes the visual embedding table multiple times, resulting in a final visual embedding that is a probabilistic combination of the indexed embeddings. This structural approach mirrors the method used for generating textual embeddings. Empirical evaluations on various multimodal benchmarks demonstrate that Ovis outperforms open-source MLLMs of similar parameter scales and even surpasses the proprietary model Qwen-VL-Plus overall. These results highlight the potential of Ovis' structured visual representation for advancing MLLM architectural design and promoting more effective multimodal learning. Both the source code and the training dataset of Ovis will be made publicly available.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20797">https://arxiv.org/abs/2405.20797</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper outlines a novel MLLM architecture, which appears to align with your interest in using large language models. However, it does not specifically discuss controlling software or web browsers, or computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20830" target="_blank">Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2405.20830v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20830v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yueqin Yin, Zhendong Wang, Yujia Xie, Weizhu Chen, Mingyuan Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.20830v1 Announce Type: cross 
Abstract: Traditional language model alignment methods, such as Direct Preference Optimization (DPO), are limited by their dependence on static, pre-collected paired preference data, which hampers their adaptability and practical applicability. To overcome this limitation, we introduce Self-Augmented Preference Optimization (SAPO), an effective and scalable training paradigm that does not require existing paired data. Building on the self-play concept, which autonomously generates negative responses, we further incorporate an off-policy learning pipeline to enhance data exploration and exploitation. Specifically, we employ an Exponential Moving Average (EMA) model in conjunction with a replay buffer to enable dynamic updates of response segments, effectively integrating real-time feedback with insights from historical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B models across benchmarks, including the Open LLM Leaderboard, IFEval, AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses established offline contrastive baselines, such as DPO and Odds Ratio Preference Optimization, and outperforms offline self-play methods like SPIN. Our code is available at https://github.com/yinyueqin/SAPO</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20830">https://arxiv.org/abs/2405.20830</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in 'Agents based on large-language models'. It describes a new method 'Self-Augmented Preference Optimization (SAPO)' for training language model based agents, making it highly relevant. However, it does not directly mention controlling software or web browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20974" target="_blank">SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</a></h3>
            <a href="https://arxiv.org/html/2405.20974v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20974v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</p>
            <p><strong>Summary:</strong> arXiv:2405.20974v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at \url{https://github.com/xu1868/SaySelf}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20974">https://arxiv.org/abs/2405.20974</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your research interests as it addresses large language models (LLMs), specifically teaching them to express confidence. Although it doesn't focus on controlling software or web browsers, it provides insights on the broader implications and the usages of LLMs, such as automation and generating accurate information.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21047" target="_blank">Grammar-Aligned Decoding</a></h3>
            <a href="https://arxiv.org/html/2405.21047v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.21047v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni</p>
            <p><strong>Summary:</strong> arXiv:2405.21047v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21047">https://arxiv.org/abs/2405.21047</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses grammar-aligned decoding, a decoding approach for Large Language Models (LLMs), and its potential applications for code generation. It should be relevant to your interest in agents based on large language models and computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.05660" target="_blank">Hypothesis Search: Inductive Reasoning with Language Models</a></h3>
            
            <p><strong>Authors:</strong> Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman</p>
            <p><strong>Summary:</strong> arXiv:2309.05660v2 Announce Type: replace 
Abstract: Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.05660">https://arxiv.org/abs/2309.05660</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large language models (LLMs) as it discusses using LLMs for problem-solving tasks, including improving their inductive reasoning abilities. Though it does not directly mention controlling software or web browsers, the advancements in LLMs' reasoning capabilities could contribute to their performance in such roles.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.03299" target="_blank">GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Haibo Jin, Ruoxi Chen, Andy Zhou, Yang Zhang, Haohan Wang</p>
            <p><strong>Summary:</strong> arXiv:2402.03299v4 Announce Type: replace 
Abstract: The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.03299">https://arxiv.org/abs/2402.03299</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses using large language models in a novel way for testing their safety adherences, which is relevant to designing and controlling large-language model-based agents, even though it isn't directly about controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04513" target="_blank">Online Cascade Learning for Efficient Inference over Streams</a></h3>
            <a href="https://arxiv.org/html/2402.04513v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04513v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat Chaudhuri</p>
            <p><strong>Summary:</strong> arXiv:2402.04513v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04513">https://arxiv.org/abs/2402.04513</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper could be of interest as it discusses Large Language Models (LLMs) in the capacity of answering complex queries and their application in stream processing, which is relevant to your interest in using LLMs for specific tasks such as controlling software or web browsers. However, it primarily focuses on the efficiency aspect of LLMs rather than automation, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.07043" target="_blank">A Tale of Tails: Model Collapse as a Change of Scaling Laws</a></h3>
            <a href="https://arxiv.org/html/2402.07043v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.07043v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, Julia Kempe</p>
            <p><strong>Summary:</strong> arXiv:2402.07043v2 Announce Type: replace 
Abstract: As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.07043">https://arxiv.org/abs/2402.07043</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models, particularly in regards to how these models evolve and potentially degrade when synthetic data is introduced into their training. Although it does not directly discuss controlling software or web browsers with these models, it provides significant insight into their performance and reliability.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08846" target="_blank">Experimental Design for Active Transductive Inference in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.08846v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08846v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Subhojyoti Mukherjee, Anusha Lalitha, Aniket Deshmukh, Ge Liu, Yifei Ma, Branislav Kveton</p>
            <p><strong>Summary:</strong> arXiv:2404.08846v2 Announce Type: replace 
Abstract: One emergent ability of large language models (LLMs) is that query-specific examples can be included in the prompt at inference time. In this work, we use active learning for adaptive prompt design and call it Active In-context Prompt Design (AIPD). We design the LLM prompt by adaptively choosing few-shot examples from a training set to optimize performance on a test set. The training examples are initially unlabeled and we obtain the label of the most informative ones, which maximally reduces uncertainty in the LLM prediction. We propose two algorithms, GO and SAL, which differ in how the few-shot examples are chosen. We analyze these algorithms in linear models: first GO and then use its equivalence with SAL. We experiment with many different tasks in small, medium-sized, and large language models; and show that GO and SAL outperform other methods for choosing few-shot examples in the LLM prompt at inference time.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08846">https://arxiv.org/abs/2404.08846</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper doesn't directly discuss software or browser control using Large Language Models but it does focus on the use of Large Language Models via Active In-context Prompt Design, which is closely related to your interest in agents using large language models. It proposes new methods for selecting few-shot examples in LLM prompting which could be useful for automation and control scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18239" target="_blank">SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning</a></h3>
            <a href="https://arxiv.org/html/2404.18239v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.18239v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu</p>
            <p><strong>Summary:</strong> arXiv:2404.18239v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18239">https://arxiv.org/abs/2404.18239</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models, and particularly their application in unlearning, an essential aspect of automation. However, it does not directly discuss controlling software or browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18641" target="_blank">Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning</a></h3>
            <a href="https://arxiv.org/html/2405.18641v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18641v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</p>
            <p><strong>Summary:</strong> arXiv:2405.18641v2 Announce Type: replace 
Abstract: Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. First time in the literature, we show that the jail-broken effect can be mitigated by separating states in the finetuning stage to optimize the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards consensus could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence. Empirically, our results on four downstream finetuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks. Code is available at \url{https://github.com/git-disl/Lisa}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18641">https://arxiv.org/abs/2405.18641</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the safety alignment of Large Language Models (LLM) and their use in controlling the harmful effect of data while fine tuning. This topic touches on the aspect of 'computer automation using large language models'. The paper introduces a method (Lisa) for better alignment performance while maintaining the LLM's accuracy which aligns with the topic of using LLM's for diverse control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.15255" target="_blank">Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM</a></h3>
            <a href="https://arxiv.org/html/2305.15255v4/extracted/5633737/figures/train_tf_11.png" target="_blank"><img src="https://arxiv.org/html/2305.15255v4/extracted/5633737/figures/train_tf_11.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich</p>
            <p><strong>Summary:</strong> arXiv:2305.15255v4 Announce Type: replace-cross 
Abstract: We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.15255">https://arxiv.org/abs/2305.15255</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in large language models and their applications. Specifically, it details a novel approach to adapt LLMs for spoken QA and speech continuation, which falls under the category of controlling software with LLM. However, it doesn't specifically discuss controlling web browsers or computer automation, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.15805" target="_blank">Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</a></h3>
            <a href="https://arxiv.org/html/2305.15805v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.15805v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas Hofmann</p>
            <p><strong>Summary:</strong> arXiv:2305.15805v3 Announce Type: replace-cross 
Abstract: Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\times$ increase in inference throughput and even greater memory savings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.15805">https://arxiv.org/abs/2305.15805</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is mainly focused on Large Language Models (LLMs) and proposes a novel approach that could be used in controlling software or other forms of automation. Even though it doesn't explicitly address the subtopics you mentioned, the novel approach in this paper could potentially be used in LLM based agents systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.01399" target="_blank">Learning to Model the World with Language</a></h3>
            <a href="https://arxiv.org/html/2308.01399v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.01399v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan</p>
            <p><strong>Summary:</strong> arXiv:2308.01399v2 Announce Type: replace-cross 
Abstract: To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language -- language like "this button turns on the TV" or "I put the bowls away" -- that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.01399">https://arxiv.org/abs/2308.01399</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents the design of an agent named Dynalang that leverages diverse language and future prediction mechanisms to predict the behavior of the environment and select actions. While the paper does not specifically mention software or browser control, the use of large language models and the potential application in diverse environments makes it highly relevant to your interests in large-language model based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09615" target="_blank">API Pack: A Massive Multi-Programming Language Dataset for API Call Generation</a></h3>
            <a href="https://arxiv.org/html/2402.09615v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.09615v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda</p>
            <p><strong>Summary:</strong> arXiv:2402.09615v3 Announce Type: replace-cross 
Abstract: We introduce API Pack, a massive multi-programming language dataset containing more than 1 million instruction-API call pairs to improve the API call generation capabilities of large language models. By fine-tuning CodeLlama-13B on 20,000 Python instances from API Pack, we achieved around 10% and 5% higher accuracy compared to GPT-3.5 and GPT-4, respectively, in generating unseen API calls. Fine-tuning on API Pack enables cross-programming language generalization by leveraging a large amount of data in one language and small amounts of data from other languages. Scaling the training data to 1 million instances further improves the model's generalization to new APIs not encountered during training. We open-source the API Pack dataset, trained models, and associated source code at https://github.com/zguo0525/API-Pack to facilitate further research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09615">https://arxiv.org/abs/2402.09615</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is pertinent to your interest in Large Language Models (LLMs) since it discusses how to enhance API call generation, which can be beneficial when using LLMs to achieve software automation. Although it doesn't directly discuss controlling web browsers or software, the advancements it discusses can conceivably be applied to those scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09723" target="_blank">Efficient Prompt Optimization Through the Lens of Best Arm Identification</a></h3>
            <a href="https://arxiv.org/html/2402.09723v3/extracted/5633348/figures/procedure.png" target="_blank"><img src="https://arxiv.org/html/2402.09723v3/extracted/5633348/figures/procedure.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengshuai Shi, Kun Yang, Zihan Chen, Jundong Li, Jing Yang, Cong Shen</p>
            <p><strong>Summary:</strong> arXiv:2402.09723v3 Announce Type: replace-cross 
Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically finding good prompts, i.e., prompt optimization. Most existing works follow the scheme of selecting from a pre-generated pool of candidate prompts. However, these designs mainly focus on the generation strategy, while limited attention has been paid to the selection method. Especially, the cost incurred during the selection (e.g., accessing LLM and evaluating the responses) is rarely explicitly considered. To overcome this limitation, this work provides a principled framework, TRIPLE, to efficiently perform prompt selection under an explicit budget constraint. TRIPLE is built on a novel connection established between prompt optimization and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB); thus, it is capable of leveraging the rich toolbox from BAI-FB systematically and also incorporating unique characteristics of prompt optimization. Extensive experiments on multiple well-adopted tasks using various LLMs demonstrate the remarkable performance improvement of TRIPLE over baselines while satisfying the limited budget constraints. As an extension, variants of TRIPLE are proposed to efficiently select examples for few-shot prompts, also achieving superior empirical performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09723">https://arxiv.org/abs/2402.09723</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses optimization of large language models (LLMs), which is critical in controlling software or automation. However, it doesn't specifically cover controlling web browsers or direct applications of controlling software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12146" target="_blank">Enabling Weak LLMs to Judge Response Reliability via Meta Ranking</a></h3>
            <a href="https://arxiv.org/html/2402.12146v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.12146v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2402.12146v3 Announce Type: replace-cross 
Abstract: Despite the strong performance of large language models (LLMs) across a wide range of tasks, they still have reliability issues. Previous studies indicate that strong LLMs like GPT-4-turbo excel in evaluating the reliability of responses from LLMs, but face efficiency and local deployment issues. Thus, to enable weak LLMs to effectively assess the reliability of LLM responses, we propose a novel cross-query-comparison-based method called $\textit{Meta Ranking}$ (MR). Unlike previous few-shot methods that solely based on in-context learning capabilities in LLMs, MR assesses reliability by pairwisely ranking the target query-response pair with multiple reference query-response pairs. We found that MR is highly effective in error detection for LLM responses, where weak LLMs, such as Phi-2, could surpass strong baselines like GPT-3.5-turbo, requiring only five reference samples and significantly improving efficiency. We further demonstrate that MR can enhance strong LLMs' performance in two practical applications: model cascading and instruction tuning. In model cascading, we combine open- and closed-source LLMs to achieve performance comparable to GPT-4-turbo with lower costs. In instruction tuning, we use MR for iterative training data filtering, significantly reducing data processing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with fewer training tokens. These results underscore the high potential of MR in both efficiency and effectiveness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12146">https://arxiv.org/abs/2402.12146</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper addresses the usage of large language models, particularly for judging response reliability. The Meta Ranking method proposed in it could be useful in the context of controlling software or automating tasks as it could assess how reliable an LLM response is.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15938" target="_blank">Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.15938v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.15938v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, Ge Li</p>
            <p><strong>Summary:</strong> arXiv:2402.15938v3 Announce Type: replace-cross 
Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\%-30.2\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9\% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15938">https://arxiv.org/abs/2402.15938</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While this paper does not propose new methods for using large language models to control software or web browsers, it does delve into the important topic of data contamination and evaluation in large language models. This could be quite useful in helping to build reliable and robust agents based on large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.19732" target="_blank">Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization</a></h3>
            <a href="https://arxiv.org/html/2405.19732v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.19732v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo</p>
            <p><strong>Summary:</strong> arXiv:2405.19732v2 Announce Type: replace-cross 
Abstract: Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor. Will this strategy also work well for solving complex non-convex optimization problems? Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step. Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor. In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach. The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner. We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization. Inferred results from LLMs are used as restarting points for the next stage of gradient optimization. By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods. Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs. The code is released at https://github.com/guozix/LLM-catalyst.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.19732">https://arxiv.org/abs/2405.19732</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interest in 'Agents based on large-language models'. It discusses a new method that combines the optimization methods of a conventional gradient-based optimizer with Large Language Models. Moreover, it highlights the use of LLMs to optimize solutions for problems based on natural language instructions, which aligns with your interest in using large language models for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.19787" target="_blank">From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers</a></h3>
            <a href="https://arxiv.org/html/2405.19787v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.19787v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dylan Zhang, Justin Wang, Francois Charton</p>
            <p><strong>Summary:</strong> arXiv:2405.19787v2 Announce Type: replace-cross 
Abstract: Instruction tuning -- tuning large language models on instruction-output pairs -- is a promising technique for making models better adapted to the real world. Yet, the key factors driving the model's capability to understand and follow instructions not seen during training remain under-explored. Our investigation begins with a series of synthetic experiments within the theoretical framework of a Turing-complete algorithm called Markov algorithm, which allows fine-grained control over the instruction-tuning data. Generalization and robustness with respect to the training distribution emerge once a diverse enough set of tasks is provided, even though very few examples are provided for each task. We extend these initial results to a real-world application scenario of code generation and find that a more diverse instruction set, extending beyond code-related tasks, improves the performance of code generation. Our observations suggest that a more diverse semantic space for instruction-tuning sets greatly improves the model's ability to follow instructions and perform tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.19787">https://arxiv.org/abs/2405.19787</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper can be of your interest as it discusses tuning large language models to follow instructions (a form of control) which is relevant to your interest in using large language models to control software or web browsers.</p>
        </div>
        </div><div class='timestamp'>Report generated on June 03, 2024 at 21:43:18</div></body></html>