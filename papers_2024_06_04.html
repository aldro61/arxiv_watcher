
            <html>
            <head>
                <title>Report Generated on June 04, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for June 04, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00079" target="_blank">Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling</a></h3>
            <a href="https://arxiv.org/html/2406.00079v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00079v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo, Hechang Chen, Lichao Sun, Bo Yang</p>
            <p><strong>Summary:</strong> arXiv:2406.00079v1 Announce Type: new 
Abstract: Recent works have shown the remarkable superiority of transformer models in reinforcement learning (RL), where the decision-making problem is formulated as sequential generation. Transformer-based agents could emerge with self-improvement in online environments by providing task contexts, such as multiple trajectories, called in-context RL. However, due to the quadratic computation complexity of attention in transformers, current in-context RL methods suffer from huge computational costs as the task horizon increases. In contrast, the Mamba model is renowned for its efficient ability to process long-term dependencies, which provides an opportunity for in-context RL to solve tasks that require long-term memory. To this end, we first implement Decision Mamba (DM) by replacing the backbone of Decision Transformer (DT). Then, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers and Mamba in high-quality prediction and long-term memory. Specifically, DM-H first generates high-value sub-goals from long-term memory through the Mamba model. Then, we use sub-goals to prompt the transformer, establishing high-quality predictions. Experimental results demonstrate that DM-H achieves state-of-the-art in long and short-term tasks, such as D4RL, Grid World, and Tmaze benchmarks. Regarding efficiency, the online testing of DM-H in the long-term task is 28$\times$ times faster than the transformer-based baselines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00079">https://arxiv.org/abs/2406.00079</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems to be relevant to your research interests in 'Agents based on large-language models', wherein it proposes a new kind of reinforcement learning model, the Decision Mamba, which might apply to automating decision-making tasks. However, it doesn't explicitly mention using large language models for control purposes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00131" target="_blank">How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures</a></h3>
            <a href="https://arxiv.org/html/2406.00131v1/extracted/5635549/icl_fig.png" target="_blank"><img src="https://arxiv.org/html/2406.00131v1/extracted/5635549/icl_fig.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kevin Christian Wibisono, Yixin Wang</p>
            <p><strong>Summary:</strong> arXiv:2406.00131v1 Announce Type: new 
Abstract: Large language models (LLMs) like transformers have impressive in-context learning (ICL) capabilities; they can generate predictions for new queries based on input-output sequences in prompts without parameter updates. While many theories have attempted to explain ICL, they often focus on structured training data similar to ICL tasks, such as regression. In practice, however, these models are trained in an unsupervised manner on unstructured text data, which bears little resemblance to ICL tasks. To this end, we investigate how ICL emerges from unsupervised training on unstructured data. The key observation is that ICL can arise simply by modeling co-occurrence information using classical language models like continuous bag of words (CBOW), which we theoretically prove and empirically validate. Furthermore, we establish the necessity of positional information and noise structure to generalize ICL to unseen data. Finally, we present instances where ICL fails and provide theoretical explanations; they suggest that the ICL ability of LLMs to identify certain tasks can be sensitive to the structure of the training data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00131">https://arxiv.org/abs/2406.00131</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper provides insights into the in-context learning capabilities of large language models, a topic you expressed interest in under your llm-agents tag. Despite not explicitly addressing software or web browser control, it offers valuable knowledge on how LLMs learn and adapt to new tasks without explicit retraining, which could be relevant for automation purposes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00132" target="_blank">QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation</a></h3>
            
            <p><strong>Authors:</strong> Zhuo Chen, Rumen Dangovski, Charlotte Loh, Owen Dugan, Di Luo, Marin Solja\v{c}i\'c</p>
            <p><strong>Summary:</strong> arXiv:2406.00132v1 Announce Type: new 
Abstract: We propose Quantum-informed Tensor Adaptation (QuanTA), a novel, easy-to-implement, fine-tuning method with no inference overhead for large-scale pre-trained language models. By leveraging quantum-inspired methods derived from quantum circuit structures, QuanTA enables efficient high-rank fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank approximation may fail for complicated downstream tasks. Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations. Experiments demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning, and scalability compared to traditional methods. Furthermore, QuanTA shows superior performance with fewer trainable parameters compared to other approaches and can be designed to integrate with existing fine-tuning algorithms for further improvement, providing a scalable and efficient solution for fine-tuning large language models and advancing state-of-the-art in natural language processing.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00132">https://arxiv.org/abs/2406.00132</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper proposes a new fine-tuning method for large-scale pre-trained language models, which could be applied for controlling software or for automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00144" target="_blank">Query2CAD: Generating CAD models using natural language queries</a></h3>
            <a href="https://arxiv.org/html/2406.00144v1/extracted/5635606/architecture.png" target="_blank"><img src="https://arxiv.org/html/2406.00144v1/extracted/5635606/architecture.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Akshay Badagabettu, Sai Sravan Yarlagadda, Amir Barati Farimani</p>
            <p><strong>Summary:</strong> arXiv:2406.00144v1 Announce Type: new 
Abstract: Computer Aided Design (CAD) engineers typically do not achieve their best prototypes in a single attempt. Instead, they iterate and refine their designs to achieve an optimal solution through multiple revisions. This traditional approach, though effective, is time-consuming and relies heavily on the expertise of skilled engineers. To address these challenges, we introduce Query2CAD, a novel framework to generate CAD designs. The framework uses a large language model to generate executable CAD macros. Additionally, Query2CAD refines the generation of the CAD model with the help of its self-refinement loops. Query2CAD operates without supervised data or additional training, using the LLM as both a generator and a refiner. The refiner leverages feedback generated by the BLIP2 model, and to address false negatives, we have incorporated human-in-the-loop feedback into our system. Additionally, we have developed a dataset that encompasses most operations used in CAD model designing and have evaluated our framework using this dataset. Our findings reveal that when we used GPT-4 Turbo as our language model, the architecture achieved a success rate of 53.6\% on the first attempt. With subsequent refinements, the success rate increased by 23.1\%. In particular, the most significant improvement in the success rate was observed with the first iteration of the refinement. With subsequent refinements, the accuracy of the correct designs did not improve significantly. We have open-sourced our data, model, and code (github.com/akshay140601/Query2CAD).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00144">https://arxiv.org/abs/2406.00144</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents Query2CAD, a framework that uses a large language model for generating executable CAD macros, an application of utilizing large language models to control software. However, since it's more focused on a specific application (CAD macros), it might not cover all aspects of your interest in large language models for broader software control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00240" target="_blank">Exploring Vulnerabilities and Protections in Large Language Models: A Survey</a></h3>
            <a href="https://arxiv.org/html/2406.00240v1/extracted/5633349/digram.png" target="_blank"><img src="https://arxiv.org/html/2406.00240v1/extracted/5633349/digram.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Frank Weizhen Liu, Chenhui Hu</p>
            <p><strong>Summary:</strong> arXiv:2406.00240v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly become key components in various AI applications, understanding their security vulnerabilities and the effectiveness of defense mechanisms is crucial. This survey examines the security challenges of LLMs, focusing on two main areas: Prompt Hacking and Adversarial Attacks, each with specific types of threats. Under Prompt Hacking, we explore Prompt Injection and Jailbreaking Attacks, discussing how they work, their potential impacts, and ways to mitigate them. Similarly, we analyze Adversarial Attacks, breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities and the defense strategies that can be implemented. The survey highlights these security challenges and discusses robust defensive frameworks to protect LLMs against these threats. By detailing these security issues, the survey contributes to the broader discussion on creating resilient AI systems that can resist sophisticated attacks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00240">https://arxiv.org/abs/2406.00240</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the security challenges and defense frameworks of Large Language Models (LLMs), which are key components in AI applications, and you are interested in agents based on large-language models. However, it does not directly address the use of LLMs for computer automation or controlling software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00548" target="_blank">LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models</a></h3>
            
            <p><strong>Authors:</strong> Tianci Liu, Haoyu Wang, Shiyang Wang, Yu Cheng, Jing Gao</p>
            <p><strong>Summary:</strong> arXiv:2406.00548v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable \textit{fairness-fluency} trade-offs. However, it is still under-explored to what extent the fluency \textit{has to} be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00548">https://arxiv.org/abs/2406.00548</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, 'LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models', discusses the use and debiasing of large language models, a topic closely related to your interest in agents based on large-language models. However, it doesn't directly touch on computer automation or control of software/web browsers, so it receives a 4 rather than a perfect 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00894" target="_blank">Pretrained Hybrids with MAD Skills</a></h3>
            <a href="https://arxiv.org/html/2406.00894v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00894v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nicholas Roberts, Samuel Guo, Zhiqi Gao, Satya Sai Srinath Namburi GNVV, Sonia Cromp, Chengjun Wu, Chengyu Duan, Frederic Sala</p>
            <p><strong>Summary:</strong> arXiv:2406.00894v1 Announce Type: new 
Abstract: While Transformers underpin modern large language models (LMs), there is a growing list of alternative architectures with new capabilities, promises, and tradeoffs. This makes choosing the right LM architecture challenging. Recently-proposed $\textit{hybrid architectures}$ seek a best-of-all-worlds approach that reaps the benefits of all architectures. Hybrid design is difficult for two reasons: it requires manual expert-driven search, and new hybrids must be trained from scratch. We propose $\textbf{Manticore}$, a framework that addresses these challenges. Manticore $\textit{automates the design of hybrid architectures}$ while reusing pretrained models to create $\textit{pretrained}$ hybrids. Our approach augments ideas from differentiable Neural Architecture Search (NAS) by incorporating simple projectors that translate features between pretrained blocks from different architectures. We then fine-tune hybrids that combine pretrained models from different architecture families -- such as the GPT series and Mamba -- end-to-end. With Manticore, we enable LM selection without training multiple models, the construction of pretrained hybrids from existing pretrained models, and the ability to $\textit{program}$ pretrained hybrids to have certain capabilities. Manticore hybrids outperform existing manually-designed hybrids, achieve strong performance on Long Range Arena (LRA) tasks, and can improve on pretrained transformers and state space models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00894">https://arxiv.org/abs/2406.00894</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses Manticore, a framework for designing hybrid architectures that harnesses large language models. It is relevant to your interest in 'Using large language models to control software' since Manticore is programmed to have certain capabilities, thus potentially applicable for automation and control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01124" target="_blank">Latent Logic Tree Extraction for Event Sequence Explanation from LLMs</a></h3>
            <a href="https://arxiv.org/html/2406.01124v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01124v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li</p>
            <p><strong>Summary:</strong> arXiv:2406.01124v1 Announce Type: new 
Abstract: Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01124">https://arxiv.org/abs/2406.01124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper provides an approach to extract logic trees from Large Language Models (LLMs) which can be used in the control of various systems. Although it doesn't specifically mention 'automation' or 'web browsers', it provides significant insights about harnessing the power of LLMs for sequence based tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01361" target="_blank">Learning to Play Atari in a World of Tokens</a></h3>
            <a href="https://arxiv.org/html/2406.01361v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01361v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pranav Agarwal, Sheldon Andrews, Samira Ebrahimi Kahou</p>
            <p><strong>Summary:</strong> arXiv:2406.01361v1 Announce Type: new 
Abstract: Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01361">https://arxiv.org/abs/2406.01361</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in agents based on large-language models, specifically regarding learning behaviors in complex environments. Although it doesn't specifically discuss controlling software or web browsers, the concept of learning behavior from discrete representations in world modeling could be a valuable insight.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01424" target="_blank">Universal In-Context Approximation By Prompting Fully Recurrent Models</a></h3>
            <a href="https://arxiv.org/html/2406.01424v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01424v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Aleksandar Petrov, Tom A. Lamb, Alasdair Paren, Philip H. S. Torr, Adel Bibi</p>
            <p><strong>Summary:</strong> arXiv:2406.01424v1 Announce Type: new 
Abstract: Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01424">https://arxiv.org/abs/2406.01424</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is based on in-context learning solutions using generative models. The approach could be used to guide agent behaviour in software or web browsing scenarios.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01462" target="_blank">Understanding Preference Fine-Tuning Through the Lens of Coverage</a></h3>
            <a href="https://arxiv.org/html/2406.01462v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01462v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun</p>
            <p><strong>Summary:</strong> arXiv:2406.01462v1 Announce Type: new 
Abstract: Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01462">https://arxiv.org/abs/2406.01462</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research paper is relevant to your preference on 'Agents based on large-language models' as it discusses the utilization of both online and offline methods for preference fine-tuning in large language models. However, it doesn't explicitly handle controlling software or web browsers, or computer automation with large language models, which is why it does not receive a perfect score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00024" target="_blank">Embedding-Aligned Language Models</a></h3>
            
            <p><strong>Authors:</strong> Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Lior Shani, Ethan Liang, Craig Boutilier</p>
            <p><strong>Summary:</strong> arXiv:2406.00024v1 Announce Type: cross 
Abstract: We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M dataset to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00024">https://arxiv.org/abs/2406.00024</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models controlling software. The new method presented, 'embedding-aligned guided language' (EAGLE), uses reinforcement learning to guide LLM's generation. This could have potential applications in computer automation and other software controls.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00045" target="_blank">Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization</a></h3>
            <a href="https://arxiv.org/html/2406.00045v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00045v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen</p>
            <p><strong>Summary:</strong> arXiv:2406.00045v1 Announce Type: cross 
Abstract: Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting "steering vectors" to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. This work proposes an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00045">https://arxiv.org/abs/2406.00045</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper pertains to controlling the behavior of Large Language Models, making it relevant to your interest in using such models for computer automation. It presents a new method of creating steering vectors for better alignment to desired outputs, which could be useful in your application interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00053" target="_blank">Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting</a></h3>
            <a href="https://arxiv.org/html/2406.00053v1/extracted/5627753/figures/task_figures_pure.png" target="_blank"><img src="https://arxiv.org/html/2406.00053v1/extracted/5627753/figures/task_figures_pure.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Suraj Anand, Michael A. Lepori, Jack Merullo, Ellie Pavlick</p>
            <p><strong>Summary:</strong> arXiv:2406.00053v1 Announce Type: cross 
Abstract: Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning, where information is statically encoded in model parameters from iterated observations of the data. Despite this apparent ability to learn in-context, language models are known to struggle when faced with unseen or rarely seen tokens. Hence, we study $\textbf{structural in-context learning}$, which we define as the ability of a model to execute in-context learning on arbitrary tokens -- so called because the model must generalize on the basis of e.g. sentence structure or task structure, rather than semantic content encoded in token embeddings. An ideal model would be able to do both: flexibly deploy in-weights operations (in order to robustly accommodate ambiguous or unknown contexts using encoded semantic information) and structural in-context operations (in order to accommodate novel tokens). We study structural in-context algorithms in a simple part-of-speech setting using both practical and toy models. We find that active forgetting, a technique that was recently introduced to help models generalize to new languages, forces models to adopt structural in-context learning solutions. Finally, we introduce $\textbf{temporary forgetting}$, a straightforward extension of active forgetting that enables one to control how much a model relies on in-weights vs. in-context solutions. Importantly, temporary forgetting allows us to induce a $\textit{dual process strategy}$ where in-context and in-weights solutions coexist within a single model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00053">https://arxiv.org/abs/2406.00053</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper aligns with your interest in large language models. It discusses structural in-context learning, a key aspect of using language models for various tasks. While not directly about controlling software or web browsers, the learnings from this paper could be applied in those areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00059" target="_blank">Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution</a></h3>
            <a href="https://arxiv.org/html/2406.00059v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00059v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo</p>
            <p><strong>Summary:</strong> arXiv:2406.00059v1 Announce Type: cross 
Abstract: The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00059">https://arxiv.org/abs/2406.00059</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> You should read this paper as it discusses how large language models can be used for controlling external tools, which aligns with your interest in using such models for software control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00069" target="_blank">Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating Hallucination in Structured Data Generation with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.00069v1/extracted/5632986/images/cabs_fig_1.png" target="_blank"><img src="https://arxiv.org/html/2406.00069v1/extracted/5632986/images/cabs_fig_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chengwei Wei, Kee Kiat Koo, Amir Tavanaei, Karim Bouyarmane</p>
            <p><strong>Summary:</strong> arXiv:2406.00069v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have facilitated structured data generation, with applications in domains like tabular data, document databases, product catalogs, etc. However, concerns persist about generation veracity due to incorrect references or hallucinations, necessitating the incorporation of some form of model confidence for mitigation. Existing confidence estimation methods on LLM generations primarily focus on the confidence at the individual token level or the entire output sequence level, limiting their applicability to structured data generation, which consists of an intricate mix of both independent and correlated entries at the sub-structure level. In this paper, we first investigate confidence estimation methods for generated sub-structure-level data. We introduce the concept of Confidence Network that applies on the hidden state of the LLM transformer, as a more targeted estimate than the traditional token conditional probability. We further propose Confidence-Aware sub-structure Beam Search (CABS), a novel decoding method operating at the sub-structure level in structured data generation. CABS enhances the faithfulness of structured data generation by considering confidence scores from the Confidence Network for each sub-structure-level data and iteratively refining the prompts. Results show that CABS outperforms traditional token-level beam search for structured data generation by 16.7% Recall at 90% precision averagely on the problem of product attribute generation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00069">https://arxiv.org/abs/2406.00069</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper does not specifically address your stated subtopics, but it is about using Large Language Models for structured data generation which may be generally related to your interests in computer automation using large language models. It also introduces a novel method to mitigate errors in generated data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00083" target="_blank">BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.00083v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00083v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, Qian Lou</p>
            <p><strong>Summary:</strong> arXiv:2406.00083v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as "hallucinations." Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like "The Republican Party, Donald Trump, etc." Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\% to 74.6\% or increase the rate of negative responses from 0.22\% to 72\% for targeted queries.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00083">https://arxiv.org/abs/2406.00083</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in Large Language Models (LLMs). It discusses the vulnerabilities and potential attacks on these models, specifically in the retrieval part, which could lead to indirect attacks on the generative parts of LLMs. This could be crucial if you plan to use LLMs for software or web browser control or for other computer automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00222" target="_blank">Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</a></h3>
            <a href="https://arxiv.org/html/2406.00222v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00222v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Maximillian Chen, Ruoxi Sun, Sercan \"O. Ar{\i}k, Tomas Pfister</p>
            <p><strong>Summary:</strong> arXiv:2406.00222v1 Announce Type: cross 
Abstract: Large language models (LLMs) aligned through reinforcement learning from human feedback (RLHF) have quickly become one of the dominant paradigms for building intelligent conversational assistant agents. However, despite their strong performance across many benchmarks, LLM-based agents still lack conversational skills such as disambiguation: when generalized assistants are faced with ambiguity, they often overhedge or implicitly guess users' ground-truth intents rather than asking clarification questions, and under task-specific settings, high-quality conversation samples are often limited, affecting models' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) which allows for sample-efficient dialogue policy learning in multi-turn conversation. We demonstrate ACT's efficacy under sample-efficient conditions in three difficult conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for text-to-SQL generation. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard approaches to supervised fine-tuning and DPO.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00222">https://arxiv.org/abs/2406.00222</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it revolves around 'Large Language Models (LLMs)' and their use in intelligent conversational assistant agents. More specifically, it focuses on aligning these models through 'Reinforcement Learning from Human Feedback (RLHF)', a concept similar to your interest in 'Using large language models to control software'. The 'Action-Based Contrastive Self-Training (ACT)' introduced through the study could provide a unique insight into how large language models can be used for automating and optimizing dialogue action policies.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00793" target="_blank">Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective</a></h3>
            <a href="https://arxiv.org/html/2406.00793v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00793v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fabian Falck, Ziyu Wang, Chris Holmes</p>
            <p><strong>Summary:</strong> arXiv:2406.00793v1 Announce Type: cross 
Abstract: In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the martingale property, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00793">https://arxiv.org/abs/2406.00793</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is focused on Large Language Models (LLM) and their learning methods. It also discusses the system's trustworthiness and safety, which could be relevant to using LLM for controlling software or other automatic tasks, even though it doesn't directly address these subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00832" target="_blank">BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</a></h3>
            <a href="https://arxiv.org/html/2406.00832v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00832v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lin Gui, Cristina G\^arbacea, Victor Veitch</p>
            <p><strong>Summary:</strong> arXiv:2406.00832v1 Announce Type: cross 
Abstract: This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00832">https://arxiv.org/abs/2406.00832</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLMs) as it discusses the problem of aligning samples from LLMs to human preferences. It also proposes BoNBoN Alignment, a new method to fine-tune a LLM which could potentially be applied for controlling software and automations.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01288" target="_blank">Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</a></h3>
            <a href="https://arxiv.org/html/2406.01288v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01288v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin</p>
            <p><strong>Summary:</strong> arXiv:2406.01288v1 Announce Type: cross 
Abstract: Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01288">https://arxiv.org/abs/2406.01288</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it explores the capabilities of large-language models in escaping constraints (jailbreak) and their robustness against defenses. This potentially suggests methods for controlling software or automating processes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.00255" target="_blank">SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks</a></h3>
            <a href="https://arxiv.org/html/2309.00255v3/extracted/5629426/Drawio/sortednet-figure2.jpg" target="_blank"><img src="https://arxiv.org/html/2309.00255v3/extracted/5629426/Drawio/sortednet-figure2.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Parsa Kavehzadeh, Marzieh Tahaei, Boxing Chen, Ali Ghodsi</p>
            <p><strong>Summary:</strong> arXiv:2309.00255v3 Announce Type: replace 
Abstract: Deep neural networks (DNNs) must cater to a variety of users with different performance needs and budgets, leading to the costly practice of training, storing, and maintaining numerous user/task-specific models. There are solutions in the literature to deal with single dynamic or many-in-one models instead of many individual networks; however, they suffer from significant drops in performance, lack of generalization across different model architectures or different dimensions (e.g. depth, width, attention blocks), heavy model search requirements during training, and training a limited number of sub-models. To address these limitations, we propose SortedNet, a generalized and scalable training solution to harness the inherent modularity of DNNs. Thanks to a generalized nested architecture (which we refer as \textit{sorted} architecture in this paper) with shared parameters and its novel update scheme combining random sub-model sampling and a new gradient accumulation mechanism, SortedNet enables the training of sub-models simultaneously along with the training of the main model (without any significant extra training or inference overhead), simplifies dynamic model selection, customizes deployment during inference, and reduces the model storage requirement significantly. The versatility and scalability of SortedNet are validated through various architectures and tasks, including LLaMA, BERT, RoBERTa (NLP tasks), ResNet and MobileNet (image classification) demonstrating its superiority over existing dynamic training methods. For example, we introduce a novel adaptive self-speculative approach based on sorted-training to accelerate large language models decoding. Moreover, SortedNet is able to train 160 sub-models at once, achieving at least 96\% of the original model's performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.00255">https://arxiv.org/abs/2309.00255</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents an approach that could scale and accelerate large language models which aligns with your interest in agents based on large-language models, specifically regarding using them to control software and automate computer tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.17505" target="_blank">Arrows of Time for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.17505v3/extracted/5640544/figure/all_med_lang_re.png" target="_blank"><img src="https://arxiv.org/html/2401.17505v3/extracted/5640544/figure/all_med_lang_re.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vassilis Papadopoulos, J\'er\'emie Wenger, Cl\'ement Hongler</p>
            <p><strong>Summary:</strong> arXiv:2401.17505v3 Announce Type: replace 
Abstract: We study the probabilistic modeling performed by Autoregressive Large Language Models (LLMs) through the angle of time directionality, addressing a question first raised in (Shannon, 1951). For large enough models, we empirically find a time asymmetry in their ability to learn natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.17505">https://arxiv.org/abs/2401.17505</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper studies how Large Language Models (LLMs) learn natural language and can predict the next token. It will be useful in understanding how these models perform tasks, which could be beneficial for controlling software or browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.18018" target="_blank">On Prompt-Driven Safeguarding for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.18018v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.18018v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng</p>
            <p><strong>Summary:</strong> arXiv:2401.18018v4 Announce Type: replace 
Abstract: Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a "higher-refusal" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.18018">https://arxiv.org/abs/2401.18018</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the safe use of large language models, particularly in deciding whether to comply with or refuse user queries. It may be of interest because it's related to controlling and optimizing large language models' behaviors. While this is not specifically about controlling software or web browsers, it does touch on a pertinent topic in your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01306" target="_blank">KTO: Model Alignment as Prospect Theoretic Optimization</a></h3>
            <a href="https://arxiv.org/html/2402.01306v2/extracted/5636534/figures/utility.png" target="_blank"><img src="https://arxiv.org/html/2402.01306v2/extracted/5636534/figures/utility.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela</p>
            <p><strong>Summary:</strong> arXiv:2402.01306v2 Announce Type: replace 
Abstract: Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01306">https://arxiv.org/abs/2402.01306</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it explores an approach towards aligning Large Language Models (LLMs) with human feedback, and this concept can be applied in controlling software or automating tasks using LLMs. However, it doesn't directly discuss controlling software or web browsers with LLMs, thus making it slightly less relevant.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02456" target="_blank">tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)</a></h3>
            
            <p><strong>Authors:</strong> Junhua Zeng, Chao Li, Zhun Sun, Qibin Zhao, Guoxu Zhou</p>
            <p><strong>Summary:</strong> arXiv:2402.02456v2 Announce Type: replace 
Abstract: Tensor networks are efficient for extremely high-dimensional representation, but their model selection, known as tensor network structure search (TN-SS), is a challenging problem. Although several works have targeted TN-SS, most existing algorithms are manually crafted heuristics with poor performance, suffering from the curse of dimensionality and local convergence. In this work, we jump out of the box, studying how to harness large language models (LLMs) to automatically discover new TN-SS algorithms, replacing the involvement of human experts. By observing how human experts innovate in research, we model their common workflow and propose an automatic algorithm discovery framework called tnGPS. The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement. The experimental results demonstrate that the algorithms discovered by tnGPS exhibit superior performance in benchmarks compared to the current state-of-the-art methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02456">https://arxiv.org/abs/2402.02456</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses harnessing large language models (LLMs) to automatically discover new tensor network structure search (TN-SS) algorithms, which could have implications for controlling software or enabling computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02619" target="_blank">Increasing Trust in Language Models through the Reuse of Verified Circuits</a></h3>
            <a href="https://arxiv.org/html/2402.02619v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02619v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Philip Quirke, Clement Neo, Fazl Barez</p>
            <p><strong>Summary:</strong> arXiv:2402.02619v5 Announce Type: replace 
Abstract: Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of language models built using them. The reuse of verified circuits reduces the effort to verify more complex composite models which we believe to be a significant step towards safety of language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02619">https://arxiv.org/abs/2402.02619</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper directly addresses your interest in using large language models for automation tasks. It discusses how verified task modules like performing arithmetic operations can be inserted into an untrained model to improve its reliability which is crucial for automation tasks. However, it doesn't explicitly mention the control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14688" target="_blank">Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.14688v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14688v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener</p>
            <p><strong>Summary:</strong> arXiv:2402.14688v2 Announce Type: replace 
Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. Code: https://github.com/likenneth/q_probe .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14688">https://arxiv.org/abs/2402.14688</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper is highly relevant to your interest in large language models for agents as it discusses Q-Probing technique to adapt a pre-trained language model to maximize a task-specific reward function. However, its focus on code generation makes it slightly less directly relevant to the control over software and web browsers you mentioned.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15179" target="_blank">Advancing Parameter Efficiency in Fine-tuning via Representation Editing</a></h3>
            <a href="https://arxiv.org/html/2402.15179v3/extracted/5637730/Architecture.png" target="_blank"><img src="https://arxiv.org/html/2402.15179v3/extracted/5637730/Architecture.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang</p>
            <p><strong>Summary:</strong> arXiv:2402.15179v3 Announce Type: replace 
Abstract: Parameter Efficient Fine-Tuning (PEFT) techniques have drawn significant attention due to their ability to yield competitive results while updating only a small portion of the adjustable parameters. However, existing PEFT methods pose challenges in hyperparameter selection, such as choosing the rank for LoRA or Adapter, or specifying the length of soft prompts. To address these challenges, we propose a novel fine-tuning approach for neural models, named Representation EDiting (RED), which modifies the representations generated at some layers through the application of scaling and biasing operations. While existing PEFT methods still demonstrate over-parameterization that could potentially undermine the generalization ability acquired from pre-training, RED can substantially reduce the number of trainable parameters by a factor of 25, 700 compared to full parameter fine-tuning and by a factor of 32 relative to LoRA. Remarkably, RED achieves results comparable or superior to both full parameter fine-tuning and other PEFT methods. Extensive experiments across various model architectures and scales, including RoBERTa, GPT-2, T5, and LLaMA-2, have demonstrated the effectiveness and efficiency of RED1, thereby positioning it as a promising PEFT strategy for large-scale neural models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15179">https://arxiv.org/abs/2402.15179</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the efficiency and effectiveness of using neural models like RoBERTa, GPT-2, T5, and LLaMA-2 in Representation EDiting (RED), which can be relevant to the use of large language models for controlling software or other similar applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.03507" target="_blank">GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></h3>
            <a href="https://arxiv.org/html/2403.03507v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.03507v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian</p>
            <p><strong>Summary:</strong> arXiv:2403.03507v2 Announce Type: replace 
Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.03507">https://arxiv.org/abs/2403.03507</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is about improving the memory efficiency of training large language models, which is vital in developing applications like software control or automation. Although it does not directly tackle the subtopics you outlined under 'llm-agents', efficient training under resource constraints is an important aspect of deploying such models successfully.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.06833" target="_blank">Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</a></h3>
            <a href="https://arxiv.org/html/2403.06833v2/extracted/5639893/figures/chatgpt-london.png" target="_blank"><img src="https://arxiv.org/html/2403.06833v2/extracted/5639893/figures/chatgpt-london.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert</p>
            <p><strong>Summary:</strong> arXiv:2403.06833v2 Announce Type: replace 
Abstract: Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model's outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.06833">https://arxiv.org/abs/2403.06833</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models as it focuses on their safety features and practical applications. However, it doesn't specifically discuss controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.14367" target="_blank">Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data</a></h3>
            <a href="https://arxiv.org/html/2404.14367v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.14367v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar</p>
            <p><strong>Summary:</strong> arXiv:2404.14367v3 Announce Type: replace 
Abstract: Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a "negative gradient") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.14367">https://arxiv.org/abs/2404.14367</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper pertains to your interest in large language models as it deals with the technique of fine-tuning these models, particularly using preference data. It could offer valuable insight for creating effective LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.18400" target="_blank">LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2404.18400v2/extracted/5638465/fig_table_file/LLMSR-v9.jpg" target="_blank"><img src="https://arxiv.org/html/2404.18400v2/extracted/5638465/fig_table_file/LLMSR-v9.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy</p>
            <p><strong>Summary:</strong> arXiv:2404.18400v2 Announce Type: replace 
Abstract: Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery, commonly known as symbolic regression, largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established symbolic regression baselines. Incorporating scientific prior knowledge also enables LLM-SR to search the equation space more efficiently than baselines. Code is available at: https://github.com/deep-symbolic-mathematics/LLM-SR</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.18400">https://arxiv.org/abs/2404.18400</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems relevant to your research interests in the use of large language models for tasks like scientific equation discovery, a type of computer automation. Although it doesn't explicitly mention controlling software or web browsers, the use of LLMs to generate code and propose hypotheses is quite related to your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11930" target="_blank">Data Contamination Calibration for Black-box LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.11930v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11930v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wentao Ye, Jiaqi Hu, Liyao Li, Haobo Wang, Gang Chen, Junbo Zhao</p>
            <p><strong>Summary:</strong> arXiv:2405.11930v2 Announce Type: replace 
Abstract: The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11930">https://arxiv.org/abs/2405.11930</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a new method for handling data contamination in LLMs and the techniques are applicable for varied applications, including computer automation and controlling software/web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16444" target="_blank">CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</a></h3>
            <a href="https://arxiv.org/html/2405.16444v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.16444v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang</p>
            <p><strong>Summary:</strong> arXiv:2405.16444v2 Announce Type: replace 
Abstract: Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized.
  This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16444">https://arxiv.org/abs/2405.16444</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not directly mention the control of software or web browsers, it does discuss optimizing the serving of large language models, which could have implications for automation and control applications. The paper also proposes a new method (CacheBlend) for this purpose.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17233" target="_blank">CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs</a></h3>
            <a href="https://arxiv.org/html/2405.17233v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.17233v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haoyu Wang, Bei Liu, Hang Shao, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian</p>
            <p><strong>Summary:</strong> arXiv:2405.17233v2 Announce Type: replace 
Abstract: Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code is available at https://github.com/fayuge/CLAQ.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17233">https://arxiv.org/abs/2405.17233</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't touch on using LLMs to control software or web browsers, it focuses on improving the efficiency of LLMs. This can be vital in tasks that require controlling applications, as efficiency is a key aspect.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18628" target="_blank">Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference</a></h3>
            <a href="https://arxiv.org/html/2405.18628v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18628v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan</p>
            <p><strong>Summary:</strong> arXiv:2405.18628v2 Announce Type: replace 
Abstract: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18628">https://arxiv.org/abs/2405.18628</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses efficiency improvements in Large Language Models (LLM), which might provide insights for the use of LLMs in controlling software and fulfills your interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.13673" target="_blank">Physics of Language Models: Part 1, Learning Hierarchical Language Structures</a></h3>
            
            <p><strong>Authors:</strong> Zeyuan Allen-Zhu, Yuanzhi Li</p>
            <p><strong>Summary:</strong> arXiv:2305.13673v3 Announce Type: replace-cross 
Abstract: Transformer-based language models are effective but complex, and understanding their inner workings is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models grasp complex, recursive language structures defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn this CFG language and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.
  This paper also presents several corollaries, including showing why positional embedding is inferior to relative attention or rotary embedding; demonstrating that encoder-based models (e.g., BERT, deBERTa) cannot learn very deeply nested CFGs as effectively as generative models (e.g., GPT); and highlighting the necessity of adding structural and syntactic errors to the pretraining data to make the model more robust to corrupted language prefixes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.13673">https://arxiv.org/abs/2305.13673</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper advances the study of large language models, focusing particularly on hierarchical language structures like context-free grammars—a key component of many programming languages. Although it doesn't directly address controlling software or web browsers, the findings could contribute to improved automation using large language models. However, since it does not propose any explicitly new methods for this application, its relevance may not be as high.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.10193" target="_blank">Conformal Language Modeling</a></h3>
            <a href="https://arxiv.org/html/2306.10193v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.10193v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, Regina Barzilay</p>
            <p><strong>Summary:</strong> arXiv:2306.10193v2 Announce Type: replace-cross 
Abstract: We propose a novel approach to conformal prediction for generative language models (LMs). Standard conformal prediction produces prediction sets -- in place of single predictions -- that have rigorous, statistical performance guarantees. LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components -- such as phrases or sentences -- that are each independently correct (e.g., that are not "hallucinations"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.10193">https://arxiv.org/abs/2306.10193</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems to discuss conformal prediction for language models. While it doesn't specifically mention the control of software or web browsers, the application of these techniques in multiple tasks such as open-domain question answering and text summarization, might be pertinent to your interest in agents based on large-language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08540" target="_blank">Do pretrained Transformers Learn In-Context by Gradient Descent?</a></h3>
            <a href="https://arxiv.org/html/2310.08540v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.08540v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lingfeng Shen, Aayush Mishra, Daniel Khashabi</p>
            <p><strong>Summary:</strong> arXiv:2310.08540v5 Announce Type: replace-cross 
Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses \emph{ICL objective} (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that \emph{the equivalence between ICL and GD remains an open hypothesis} and calls for further studies.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08540">https://arxiv.org/abs/2310.08540</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> You should consider this paper as it explores the learning process of large language models (LLMs), a detail which can be crucial for understanding and improving LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.12516" target="_blank">ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks</a></h3>
            <a href="https://arxiv.org/html/2310.12516v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.12516v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao</p>
            <p><strong>Summary:</strong> arXiv:2310.12516v2 Announce Type: replace-cross 
Abstract: Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering.
  We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12516">https://arxiv.org/abs/2310.12516</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the reliability of large language models, particularly with reference to open-domain question-answering tasks. Though it doesn't specifically discuss controlling applications, it can be useful for understanding how to design agents that use LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.12815" target="_blank">Formalizing and Benchmarking Prompt Injection Attacks and Defenses</a></h3>
            <a href="https://arxiv.org/html/2310.12815v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.12815v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</p>
            <p><strong>Summary:</strong> arXiv:2310.12815v3 Announce Type: replace-cross 
Abstract: A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.12815">https://arxiv.org/abs/2310.12815</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores the concept of using Large Language Models (LLMs) and applies it in a security context. Although it does not tackle the control aspect directly, it focuses on how LLMs can interact with applications which can be useful information in developing agents based on large language models for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17807" target="_blank">Clover: Closed-Loop Verifiable Code Generation</a></h3>
            
            <p><strong>Authors:</strong> Chuyue Sun, Ying Sheng, Oded Padon, Clark Barrett</p>
            <p><strong>Summary:</strong> arXiv:2310.17807v3 Announce Type: replace-cross 
Abstract: The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results show that for this dataset, (i) LLMs are reasonably successful at automatically generating formal specifications; and (ii) our consistency checker achieves a promising acceptance rate (up to 87%) for correct instances while maintaining zero tolerance for incorrect ones (no false positives).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17807">https://arxiv.org/abs/2310.17807</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not specifically focus on using large language models to control software or web browsers, it discusses large language models used for code generation which is a part of computer automation. Therefore, insights from this paper may be important for your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.07466" target="_blank">On Measuring Faithfulness or Self-consistency of Natural Language Explanations</a></h3>
            <a href="https://arxiv.org/html/2311.07466v3/extracted/5636385/emojis/person.png" target="_blank"><img src="https://arxiv.org/html/2311.07466v3/extracted/5636385/emojis/person.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Letitia Parcalabescu, Anette Frank</p>
            <p><strong>Summary:</strong> arXiv:2311.07466v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \url{https://github.com/Heidelberg-NLP/CC-SHAP}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.07466">https://arxiv.org/abs/2311.07466</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses large language models and their capabilities for generating coherent and self-consistent explanations. It indirectly relates to your interest in developing agents based on large language models, as it delves into the self-consistent behavior of such models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.08045" target="_blank">Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game</a></h3>
            <a href="https://arxiv.org/html/2311.08045v4/extracted/5638876/figures/dist_shift.png" target="_blank"><img src="https://arxiv.org/html/2311.08045v4/extracted/5638876/figures/dist_shift.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Tianhao Hu, Peixin Cao, Nan Du, Xiaolong Li</p>
            <p><strong>Summary:</strong> arXiv:2311.08045v4 Announce Type: replace-cross 
Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However, continuously updating LLMs for alignment raises a distribution gap between model-generated samples and human-annotated responses, hindering training effectiveness. To mitigate this issue, previous methods require additional preference annotation on newly generated samples to adapt to the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an Adversarial Preference Optimization (APO) framework, in which the LLM and the reward model update alternatively via a min-max game. Through adversarial training, the reward model can adapt to the shifted generation distribution of the LLM without any additional annotation. With comprehensive experiments, we find the proposed adversarial training framework further enhances existing alignment baselines in terms of LLM helpfulness and harmlessness. The code is at https://github.com/Linear95/APO.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.08045">https://arxiv.org/abs/2311.08045</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses the process of improving the interaction quality of large language models, which aligns with your interest in using LLMs to control software or perform computer automation. However, it doesn't directly address controlling software or web browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.01258" target="_blank">Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape</a></h3>
            <a href="https://arxiv.org/html/2402.01258v2/extracted/5637580/degen.png" target="_blank"><img src="https://arxiv.org/html/2402.01258v2/extracted/5637580/degen.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Juno Kim, Taiji Suzuki</p>
            <p><strong>Summary:</strong> arXiv:2402.01258v2 Announce Type: replace-cross 
Abstract: Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.01258">https://arxiv.org/abs/2402.01258</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLMs). It delves into the deeper understanding of the optimization of Transformers, a type of LLM. While it doesn't directly mention controlling software or browsers, the insights from the mean-field dynamics and loss landscape could be beneficial for development of LLM-agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02416" target="_blank">Aligner: Efficient Alignment by Learning to Correct</a></h3>
            <a href="https://arxiv.org/html/2402.02416v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02416v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang</p>
            <p><strong>Summary:</strong> arXiv:2402.02416v3 Announce Type: replace-cross 
Abstract: With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9\% in helpfulness and 23.8\% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0\% to 58.3\%, surpassing GPT-4 Omni's 57.5\% Win Rate (community report).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02416">https://arxiv.org/abs/2402.02416</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, 'Aligner: Efficient Alignment by Learning to Correct,' seems relevant to your interests in large language models, specifically how they can be used in dynamic environments. The paper discusses 'Aligner,' an approach that manipulates the responses of large language models for optimal results. It might not be exactly about controlling software or web browsers, but the idea seems promising for better interaction with software or applications controlled by these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02805" target="_blank">Graph-enhanced Large Language Models in Asynchronous Plan Reasoning</a></h3>
            <a href="https://arxiv.org/html/2402.02805v2/extracted/5640031/img/make-calzone.png" target="_blank"><img src="https://arxiv.org/html/2402.02805v2/extracted/5640031/img/make-calzone.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, Janet B. Pierrehumbert</p>
            <p><strong>Summary:</strong> arXiv:2402.02805v2 Announce Type: replace-cross 
Abstract: Planning is a fundamental property of human intelligence. Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents. Our code and data are available at https://github.com/fangru-lin/graph-llm-asynchow-plan.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02805">https://arxiv.org/abs/2402.02805</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant as it discusses using large language models (LLMs) as autonomous agents. It also focuses on the limitations and efficiencies of these models, including GPT-4 and LLaMA-2, especially in the context of task complexity; which aligns with your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04411" target="_blank">DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton</a></h3>
            <a href="https://arxiv.org/html/2402.04411v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04411v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yiyou Sun, Junjie Hu, Wei Cheng, Haifeng Chen</p>
            <p><strong>Summary:</strong> arXiv:2402.04411v2 Announce Type: replace-cross 
Abstract: This paper introduces the retrieval-augmented large language model with Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach acts as a semantic router which enables the LLM to adhere to a deterministic response pathway. The routing is achieved by the retrieval-augmentation generation (RAG) strategy, which carefully selects dialogue examples aligned with the current conversational context. The advantages of DFA-RAG include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-RAG's effectiveness, indicating its potential as a valuable contribution to the conversational agent.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04411">https://arxiv.org/abs/2402.04411</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your interest in large language models (LLMs) as it presents a new framework to enhance the capabilities of conversational agents using LLMs. Even though it doesn't directly involve controlling software or web browsers, the introduction of a semantic router to guide the LLM responses may extrapolate into these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08017" target="_blank">Lumos : Empowering Multimodal LLMs with Scene Text Recognition</a></h3>
            <a href="https://arxiv.org/html/2402.08017v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08017v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar</p>
            <p><strong>Summary:</strong> arXiv:2402.08017v2 Announce Type: replace-cross 
Abstract: We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08017">https://arxiv.org/abs/2402.08017</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents Lumos, a multimodal question-answering system built around a large language model. This research, though not directly about controlling software or web browsers, involves LLM-based computer automation, which aligns with your interest in 'LLM-agents'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08277" target="_blank">Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</a></h3>
            <a href="https://arxiv.org/html/2402.08277v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08277v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</p>
            <p><strong>Summary:</strong> arXiv:2402.08277v5 Announce Type: replace-cross 
Abstract: Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08277">https://arxiv.org/abs/2402.08277</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on improving the faithfulness and traceability of answers provided by Large Language Models (LLMs). Though it doesn't explicitly focus on controlling software or web browsers, it does discuss LLMs performance, which is relevant to your interest in LLMs for automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08567" target="_blank">Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</a></h3>
            <a href="https://arxiv.org/html/2402.08567v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08567v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin</p>
            <p><strong>Summary:</strong> arXiv:2402.08567v2 Announce Type: replace-cross 
Abstract: A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08567">https://arxiv.org/abs/2402.08567</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper appears to be relevant to your third topic area as it discusses multimodal large language model (MLLM) agents, which pertains to agents based on large language models. However, it primarily focuses on the safety issue in multi-agent environments rather than proposing new methods for using LLMs to control software or automate computers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09025" target="_blank">SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks</a></h3>
            <a href="https://arxiv.org/html/2402.09025v3/extracted/5636695/images/transformer_v7.png" target="_blank"><img src="https://arxiv.org/html/2402.09025v3/extracted/5636695/images/transformer_v7.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim</p>
            <p><strong>Summary:</strong> arXiv:2402.09025v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: https://github.com/jiwonsong-dev/SLEB.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09025">https://arxiv.org/abs/2402.09025</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Though this paper does not focus on using LLMs to control specific applications like software or web browsers, it presents a novel method (SLEB) for streamlining LLMs, which is a relevant aspect considering your interest in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14809" target="_blank">CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</a></h3>
            <a href="https://arxiv.org/html/2402.14809v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14809v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang</p>
            <p><strong>Summary:</strong> arXiv:2402.14809v4 Announce Type: replace-cross 
Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14809">https://arxiv.org/abs/2402.14809</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper investigates the performance of Large Language Models (LLMs) in different reasoning domains, which is critical for automating tasks and controlling software and web browsers. The insights could potentially help in designing and improving LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14859" target="_blank">The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative</a></h3>
            <a href="https://arxiv.org/html/2402.14859v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.14859v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu</p>
            <p><strong>Summary:</strong> arXiv:2402.14859v2 Announce Type: replace-cross 
Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. Our findings reveal that, an MLLM agent, when manipulated to produce specific prompts or instructions, can effectively ``infect'' other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated prompts, highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14859">https://arxiv.org/abs/2402.14859</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper claims to present novel discoveries about the influence and manipulation within a society of Multimodal Large Language Models (MLLMs). It's aligned with your interest in agents based on large-language models, particularly in terms of interaction and the influence of large language models. However, the paper is more towards security aspects, not about controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15043" target="_blank">KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.15043v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.15043v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2402.15043v2 Announce Type: replace-cross 
Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15043">https://arxiv.org/abs/2402.15043</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper speaks directly to your interest in large language models and their evaluation, particularly their role in the comprehension and application of knowledge in more complex conversations. Although it doesn't specifically address control of web browsers or software, the evaluation framework could potentially be relevant to these topics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.06395" target="_blank">MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</a></h3>
            
            <p><strong>Authors:</strong> Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun</p>
            <p><strong>Summary:</strong> arXiv:2404.06395v3 Announce Type: replace-cross 
Abstract: The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.06395">https://arxiv.org/abs/2404.06395</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper does not directly address controlling software or web browswers, it does propose scalable training strategies for large and small language models. These strategies might help in developing new kinds of agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11143" target="_blank">OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</a></h3>
            <a href="https://arxiv.org/html/2405.11143v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.11143v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jian Hu, Xibin Wu, Weixun Wang,  Xianyu, Dehao Zhang, Yu Cao</p>
            <p><strong>Summary:</strong> arXiv:2405.11143v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to grow by scaling laws, reinforcement learning from human feedback (RLHF) has gained significant attention due to its outstanding performance. However, unlike pretraining or fine-tuning a single model, scaling reinforcement learning from human feedback (RLHF) for training large language models poses coordination challenges across four models. We present OpenRLHF, an open-source framework enabling efficient RLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the same GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters using Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and diverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF provides an out-of-the-box solution with optimized algorithms and launch scripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO, rejection sampling, and other alignment techniques. Empowering state-of-the-art LLM development, OpenRLHF's code is available at https://github.com/OpenLLMAI/OpenRLHF.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11143">https://arxiv.org/abs/2405.11143</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces 'OpenRLHF', a framework for reinforcement learning from human feedback at scale. It directly relates to using large language models for control tasks like software control and automation. However, the specifics of browser control are not discussed.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15223" target="_blank">iVideoGPT: Interactive VideoGPTs are Scalable World Models</a></h3>
            <a href="https://arxiv.org/html/2405.15223v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15223v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long</p>
            <p><strong>Summary:</strong> arXiv:2405.15223v2 Announce Type: replace-cross 
Abstract: World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15223">https://arxiv.org/abs/2405.15223</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents iVideoGPT, a model-based agent built on a large language model that interacts with its environment, fulfills your interest in computational automation using LLMs. However, it is not specifically about controlling software or web-browsers, hence the rating is not a full 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.16282" target="_blank">Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.16282v3/extracted/5640762/Figures/Flowchart.png" target="_blank"><img src="https://arxiv.org/html/2405.16282v3/extracted/5640762/Figures/Flowchart.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, Ali Emami</p>
            <p><strong>Summary:</strong> arXiv:2405.16282v3 Announce Type: replace-cross 
Abstract: As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model's confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.16282">https://arxiv.org/abs/2405.16282</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems highly relevant to your interest in large language models and how they evaluate confidence in generated responses. The insight from this research might be crucial for the design and evaluation of agents based on large language models. However, it does not specifically mention controlling software or web browsers with LLMs, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18952" target="_blank">Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets</a></h3>
            <a href="https://arxiv.org/html/2405.18952v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18952v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peter Devine</p>
            <p><strong>Summary:</strong> arXiv:2405.18952v2 Announce Type: replace-cross 
Abstract: Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences. This involves an evaluator model ranking multiple candidate responses to user prompts. However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked. Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each. Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts. Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18952">https://arxiv.org/abs/2405.18952</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper investigates Large Language Models (LLMs) and their application in Reinforcement Learning from AI Feedback (RLAIF). Even though the paper doesn't specifically discuss controlling software or web browsers, it provides fundamental guidance on how to effectively align the outputs of LLMs with human preferences.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.19327" target="_blank">MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</a></h3>
            <a href="https://arxiv.org/html/2405.19327v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.19327v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen</p>
            <p><strong>Summary:</strong> arXiv:2405.19327v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.19327">https://arxiv.org/abs/2405.19327</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not directly align with your subtopics of language models controlling software or browsers, or computer automation, it presents valuable information on the creation and development of bilingual large language models (LLM). Further, since you're interested in large language models generally, the openness and detailed description of the MAP-Neo model could be highly relevant and interested, even though it's not exactly on point with your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.19616" target="_blank">Easy Problems That LLMs Get Wrong</a></h3>
            <a href="https://arxiv.org/html/2405.19616v2/extracted/5636080/confidence-interval-white-paper.png" target="_blank"><img src="https://arxiv.org/html/2405.19616v2/extracted/5636080/confidence-interval-white-paper.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sean Williams, James Huckle</p>
            <p><strong>Summary:</strong> arXiv:2405.19616v2 Announce Type: replace-cross 
Abstract: We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others. Through a series of straightforward questions, it uncovers the significant limitations of well-regarded models to perform tasks that humans manage with ease. It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies. Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications. We hope this work paves the way for future research to enhance the usefulness and reliability of new models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.19616">https://arxiv.org/abs/2405.19616</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on the study of large language models, their limitations and potential improvements which aligns with your interest in large language models to control software and automation. However, it does not specifically mention control of software or web browsers, hence the score is 4 instead of 5.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00566" target="_blank">An Unsupervised Approach for Periodic Source Detection in Time Series</a></h3>
            <a href="https://arxiv.org/html/2406.00566v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00566v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Berken Utku Demirel, Christian Holz</p>
            <p><strong>Summary:</strong> arXiv:2406.00566v1 Announce Type: new 
Abstract: Detection of periodic patterns of interest within noisy time series data plays a critical role in various tasks, spanning from health monitoring to behavior analysis. Existing learning techniques often rely on labels or clean versions of signals for detecting the periodicity, and those employing self-supervised learning methods are required to apply proper augmentations, which is already challenging for time series and can result in collapse -- all representations collapse to a single point due to strong augmentations. In this work, we propose a novel method to detect the periodicity in time series without the need for any labels or requiring tailored positive or negative data generation mechanisms with specific augmentations. We mitigate the collapse issue by ensuring the learned representations retain information from the original samples without imposing any random variance constraints on the batch. Our experiments in three time series tasks against state-of-the-art learning methods show that the proposed approach consistently outperforms prior works, achieving performance improvements of more than 45--50\%, showing its effectiveness. Code: https://github.com/eth-siplab/Unsupervised_Periodicity_Detection</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00566">https://arxiv.org/abs/2406.00566</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interests as it focuses on a novel approach for detecting periodic patterns in time series. This aligns with your interest in new deep learning methods and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10198" target="_blank">SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</a></h3>
            <a href="https://arxiv.org/html/2402.10198v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10198v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko</p>
            <p><strong>Summary:</strong> arXiv:2402.10198v3 Announce Type: replace 
Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10198">https://arxiv.org/abs/2402.10198</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it directly discusses a new transformer-like model for time series forecasting, proposing SAMformer. It aligns well with your interest in new transformer-like models for time series and new deep learning methods for the same.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.11463" target="_blank">Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective</a></h3>
            <a href="https://arxiv.org/html/2402.11463v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.11463v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang</p>
            <p><strong>Summary:</strong> arXiv:2402.11463v3 Announce Type: replace 
Abstract: In long-term time series forecasting (LTSF) tasks, an increasing number of models have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their dynamical structures. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding and the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.11463">https://arxiv.org/abs/2402.11463</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is very relevant to your interests as it presents a new deep learning model, Attraos, for time series forecasting that incorporates chaos theory. This aligns with your desire for cutting-edge methods in deep learning for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00133" target="_blank">Streamflow Prediction with Uncertainty Quantification for Water Management: A Constrained Reasoning and Learning Approach</a></h3>
            <a href="https://arxiv.org/html/2406.00133v1/extracted/5633324/Figures/Streamflow_flowchart.png" target="_blank"><img src="https://arxiv.org/html/2406.00133v1/extracted/5633324/Figures/Streamflow_flowchart.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mohammed Amine Gharsallaoui, Bhupinderjeet Singh, Supriya Savalkar, Aryan Deshwal, Yan Yan, Ananth Kalyanaraman, Kirti Rajagopalan, Janardhan Rao Doppa</p>
            <p><strong>Summary:</strong> arXiv:2406.00133v1 Announce Type: new 
Abstract: Predicting the spatiotemporal variation in streamflow along with uncertainty quantification enables decision-making for sustainable management of scarce water resources. Process-based hydrological models (aka physics-based models) are based on physical laws, but using simplifying assumptions which can lead to poor accuracy. Data-driven approaches offer a powerful alternative, but they require large amount of training data and tend to produce predictions that are inconsistent with physical laws. This paper studies a constrained reasoning and learning (CRL) approach where physical laws represented as logical constraints are integrated as a layer in the deep neural network. To address small data setting, we develop a theoretically-grounded training approach to improve the generalization accuracy of deep models. For uncertainty quantification, we combine the synergistic strengths of Gaussian processes (GPs) and deep temporal models (i.e., deep models for time-series forecasting) by passing the learned latent representation as input to a standard distance-based kernel. Experiments on multiple real-world datasets demonstrate the effectiveness of both CRL and GP with deep kernel approaches over strong baseline methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00133">https://arxiv.org/abs/2406.00133</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper could be interesting for you as it offers a data-driven approach, integrating logical constraints in a deep neural network for time-series forecasting, thus presenting a new method for time series. However, it does not focus specifically on deep learning or foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00561" target="_blank">Learning to Approximate Particle Smoothing Trajectories via Diffusion Generative Models</a></h3>
            
            <p><strong>Authors:</strong> Ella Tamir, Arno Solin</p>
            <p><strong>Summary:</strong> arXiv:2406.00561v1 Announce Type: new 
Abstract: Learning dynamical systems from sparse observations is critical in numerous fields, including biology, finance, and physics. Even if tackling such problems is standard in general information fusion, it remains challenging for contemporary machine learning models, such as diffusion models. We introduce a method that integrates conditional particle filtering with ancestral sampling and diffusion models, enabling the generation of realistic trajectories that align with observed data. Our approach uses a smoother based on iterating a conditional particle filter with ancestral sampling to first generate plausible trajectories matching observed marginals, and learns the corresponding diffusion model. This approach provides both a generative method for high-quality, smoothed trajectories under complex constraints, and an efficient approximation of the particle smoothing distribution for classical tracking problems. We demonstrate the approach in time-series generation and interpolation tasks, including vehicle tracking and single-cell RNA sequencing data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00561">https://arxiv.org/abs/2406.00561</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper explores a method for learning dynamical systems from sparse observations, which is typically both a time series and a deep learning problem. However, it does not specifically focus on forecasting but rather on generating and interpolating time-series data. While it does seem to present a novel procedure for generating realistic trajectories that match observed data, it might not precisely align with your interests in forecasting and transformer-like models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00596" target="_blank">Multi-variable Adversarial Time-Series Forecast Model</a></h3>
            <a href="https://arxiv.org/html/2406.00596v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00596v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaoqiao Chen</p>
            <p><strong>Summary:</strong> arXiv:2406.00596v1 Announce Type: new 
Abstract: Short-term industrial enterprises power system forecasting is an important issue for both load control and machine protection. Scientists focus on load forecasting but ignore other valuable electric-meters which should provide guidance of power system protection. We propose a new framework, multi-variable adversarial time-series forecasting model, which regularizes Long Short-term Memory (LSTM) models via an adversarial process. The novel model forecasts all variables (may in different type, such as continue variables, category variables, etc.) in power system at the same time and helps trade-off process between forecasting accuracy of single variable and variable-variable relations. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. The predict results of electricity consumption of industrial enterprises by multi-variable adversarial time-series forecasting model show that the proposed approach is able to achieve better prediction accuracy. We also applied this model to real industrial enterprises power system data we gathered from several large industrial enterprises via advanced power monitors, and got impressed forecasting results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00596">https://arxiv.org/abs/2406.00596</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new framework for time series forecasting, applying LSTM models through an adversarial process. However, it primarily focuses on the application to power systems rather than proposing a broader foundation model.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00630" target="_blank">On Non-asymptotic Theory of Recurrent Neural Networks in Temporal Point Processes</a></h3>
            
            <p><strong>Authors:</strong> Zhiheng Chen, Guanhua Fang, Wen Yu</p>
            <p><strong>Summary:</strong> arXiv:2406.00630v1 Announce Type: cross 
Abstract: Temporal point process (TPP) is an important tool for modeling and predicting irregularly timed events across various domains. Recently, the recurrent neural network (RNN)-based TPPs have shown practical advantages over traditional parametric TPP models. However, in the current literature, it remains nascent in understanding neural TPPs from theoretical viewpoints. In this paper, we establish the excess risk bounds of RNN-TPPs under many well-known TPP settings. We especially show that an RNN-TPP with no more than four layers can achieve vanishing generalization errors. Our technical contributions include the characterization of the complexity of the multi-layer RNN class, the construction of $\tanh$ neural networks for approximating dynamic event intensity functions, and the truncation technique for alleviating the issue of unbounded event sequences. Our results bridge the gap between TPP's application and neural network theory.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00630">https://arxiv.org/abs/2406.00630</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper discusses theoretical aspects of Recurrent Neural Networks (RNNs) in Temporal Point Processes (TPP), which are tools for predicting irregularly timed events. This is relevant to your interest in innovative deep learning methods for time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2203.13563" target="_blank">An Intelligent End-to-End Neural Architecture Search Framework for Electricity Forecasting Model Development</a></h3>
            <a href="https://arxiv.org/html/2203.13563v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2203.13563v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jin Yang, Guangxin Jiang, Yinan Wang, Ying Chen</p>
            <p><strong>Summary:</strong> arXiv:2203.13563v2 Announce Type: replace 
Abstract: Recent years have witnessed exponential growth in developing deep learning (DL) models for time-series electricity forecasting in power systems. However, most of the proposed models are designed based on the designers' inherent knowledge and experience without elaborating on the suitability of the proposed neural architectures. Moreover, these models cannot be self-adjusted to dynamically changed data patterns due to the inflexible design of their structures. Although several recent studies have considered the application of the neural architecture search (NAS) technique for obtaining a network with an optimized structure in the electricity forecasting sector, their training process is computationally expensive and their search strategies are not flexible, indicating that the NAS application in this area is still at an infancy stage. In this study, we propose an intelligent automated architecture search (IAAS) framework for the development of time-series electricity forecasting models. The proposed framework contains three primary components, i.e., network function-preserving transformation operation, reinforcement learning (RL)-based network transformation control, and heuristic network screening, which aim to improve the search quality of a network structure. After conducting comprehensive experiments on two publicly-available electricity load datasets and two wind power datasets, we demonstrate that the proposed IAAS framework significantly outperforms the ten existing models or methods in terms of forecasting accuracy and stability. Finally, we perform an ablation experiment to showcase the importance of critical components in the proposed IAAS framework in improving forecasting accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2203.13563">https://arxiv.org/abs/2203.13563</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in deep learning methods for time series, and specifically forecasting. It discusses the development of an intelligent automated architecture search (IAAS) framework for time series electricity forecasting. However, it doesn't specifically mention foundation models, multimodal models, or transformer-like models which are your specific sub-interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02713" target="_blank">Position: What Can Large Language Models Tell Us about Time Series Analysis</a></h3>
            <a href="https://arxiv.org/html/2402.02713v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02713v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen</p>
            <p><strong>Summary:</strong> arXiv:2402.02713v2 Announce Type: replace 
Abstract: Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02713">https://arxiv.org/abs/2402.02713</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses the potential of LLMs in time series analysis and their integration with existing technologies. It doesn't seem to propose a new model, but it highlights the impact of LLMs on time series analysis, which is a topic of interest for you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10634" target="_blank">Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling</a></h3>
            <a href="https://arxiv.org/html/2402.10634v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10634v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ivan Marisca, Cesare Alippi, Filippo Maria Bianchi</p>
            <p><strong>Summary:</strong> arXiv:2402.10634v2 Announce Type: replace 
Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts. Our approach outperforms state-of-the-art methods on synthetic and real-world benchmarks under different missing data distributions, particularly in the presence of contiguous blocks of missing values.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10634">https://arxiv.org/abs/2402.10634</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'time-series' as it discusses a new method in spatiotemporal forecasting that leverages graph neural networks. It especially handles missing data within a time series, which might be interesting for you considering it's about a new deep learning method for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15290" target="_blank">Appendix for Linear Dynamics-embedded Neural Network for Long-Sequence Modeling</a></h3>
            <a href="https://arxiv.org/html/2402.15290v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.15290v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tongyi Liang, Han-Xiong Li</p>
            <p><strong>Summary:</strong> arXiv:2402.15290v2 Announce Type: replace 
Abstract: This appendix provides all necessary materials for the paper 'Linear Dynamics-embedded Neural Network for Long-Sequence Modeling', including model details, experimental configurations, and PyTorch implementation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15290">https://arxiv.org/abs/2402.15290</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it speaks about a technique for long-sequence modeling which could be applied to time series forecasting. However, it does not directly address transformative or multi-modal aspects of time-series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00946" target="_blank">SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters</a></h3>
            <a href="https://arxiv.org/html/2405.00946v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.00946v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shengsheng Lin, Weiwei Lin, Wentai Wu, Haojun Chen, Junjie Yang</p>
            <p><strong>Summary:</strong> arXiv:2405.00946v2 Announce Type: replace 
Abstract: This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than *1k* parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is publicly available at this repository: https://github.com/lss-1138/SparseTSF.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00946">https://arxiv.org/abs/2405.00946</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new lightweight model for long-term time series forecasting named SparseTSF. The model deploys a unique technique called Cross-Period Sparse Forecasting to handle complex temporal dependencies efficiently, leading to improved performance with limited computational resources. This paper doesn't discuss multimodal or transformer-like models for time series but contributes to better forecasting with novel methods and could be useful in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15273" target="_blank">Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders</a></h3>
            <a href="https://arxiv.org/html/2405.15273v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15273v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Qichao Shentu, Beibu Li, Kai Zhao, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo</p>
            <p><strong>Summary:</strong> arXiv:2405.15273v2 Announce Type: replace 
Abstract: Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios with scarce training data. Aiming at this problem, we propose constructing a general time series anomaly detection model, which is pre-trained on extensive multi-domain datasets and can subsequently apply to a multitude of downstream scenarios. The significant divergence of time series data across different domains presents two primary challenges in building such a general model: (1) meeting the diverse requirements of appropriate information bottlenecks tailored to different datasets in one unified model, and (2) enabling distinguishment between multiple normal and abnormal patterns, both are crucial for effective anomaly detection in various target scenarios. To tackle these two challenges, we propose a General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible selection of bottlenecks based on different data and explicitly enhances clear differentiation between normal and abnormal series. We conduct extensive experiments on nine target datasets from different domains. After pre-training on multi-domain data, DADA, serving as a zero-shot anomaly detector for these datasets, still achieves competitive or even superior results compared to those models tailored to each specific dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15273">https://arxiv.org/abs/2405.15273</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it proposes a new general approach to time series anomaly detection, a critical aspect of forecasting. However, it might not delve into specifics like multimodal models or transformers.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00394" target="_blank">Learning Causal Abstractions of Linear Structural Causal Models</a></h3>
            
            <p><strong>Authors:</strong> Riccardo Massidda, Sara Magliacane, Davide Bacciu</p>
            <p><strong>Summary:</strong> arXiv:2406.00394v1 Announce Type: new 
Abstract: The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00394">https://arxiv.org/abs/2406.00394</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your interests as it discusses aspects of causal discovery and causal representation learning. It tackles the unresolved issue of modeling causal knowledge at different granularity levels, which is a subtopic under your causality theme. The paper proposes a new method to learn high-level and low-level causal models, which matches your interest in research that proposes new methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00519" target="_blank">Learning Discrete Concepts in Latent Hierarchical Models</a></h3>
            <a href="https://arxiv.org/html/2406.00519v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00519v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2406.00519v1 Announce Type: new 
Abstract: Learning concepts from natural high-dimensional data (e.g., images) holds potential in building human-aligned and interpretable machine learning models. Despite its encouraging prospect, formalization and theoretical insights into this crucial task are still lacking. In this work, we formalize concepts as discrete latent causal variables that are related via a hierarchical causal model that encodes different abstraction levels of concepts embedded in high-dimensional data (e.g., a dog breed and its eye shapes in natural images). We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible. Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images. We substantiate our theoretical claims with synthetic data experiments. Further, we discuss our theory's implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00519">https://arxiv.org/abs/2406.00519</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper's main focus is on causal representation learning. It introduces the concept of discrete latent causal variables and formulates conditions for the identification of the causal model.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00535" target="_blank">Causal Contrastive Learning for Counterfactual Regression Over Time</a></h3>
            <a href="https://arxiv.org/html/2406.00535v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00535v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Courn\`ede</p>
            <p><strong>Summary:</strong> arXiv:2406.00535v1 Announce Type: new 
Abstract: Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies in the presence of time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00535">https://arxiv.org/abs/2406.00535</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'Causality and Machine Learning' topic. It explores causal discovery through a unique approach to counterfactual regression over time, focusing on long-term predictions with RNNs and other methods. However, it doesn't directly tackle the use of large language models in causal discovery, thus the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01065" target="_blank">Causal prompting model-based offline reinforcement learning</a></h3>
            <a href="https://arxiv.org/html/2406.01065v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01065v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xuehui Yu, Yi Guan, Rujia Shen, Xin Li, Chen Tang, Jingchi Jiang</p>
            <p><strong>Summary:</strong> arXiv:2406.01065v1 Announce Type: new 
Abstract: Model-based offline Reinforcement Learning (RL) allows agents to fully utilise pre-collected datasets without requiring additional or unethical explorations. However, applying model-based offline RL to online systems presents challenges, primarily due to the highly suboptimal (noise-filled) and diverse nature of datasets generated by online systems. To tackle these issues, we introduce the Causal Prompting Reinforcement Learning (CPRL) framework, designed for highly suboptimal and resource-constrained online scenarios. The initial phase of CPRL involves the introduction of the Hidden-Parameter Block Causal Prompting Dynamic (Hip-BCPD) to model environmental dynamics. This approach utilises invariant causal prompts and aligns hidden parameters to generalise to new and diverse online users. In the subsequent phase, a single policy is trained to address multiple tasks through the amalgamation of reusable skills, circumventing the need for training from scratch. Experiments conducted across datasets with varying levels of noise, including simulation-based and real-world offline datasets from the Dnurse APP, demonstrate that our proposed method can make robust decisions in out-of-distribution and noisy environments, outperforming contemporary algorithms. Additionally, we separately verify the contributions of Hip-BCPDs and the skill-reuse strategy to the robustness of performance. We further analyse the visualised structure of Hip-BCPD and the interpretability of sub-skills. We released our source code and the first ever real-world medical dataset for precise medical decision-making tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01065">https://arxiv.org/abs/2406.01065</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper explores causal reinforcement learning and demonstrates a method for addressing different tasks through the reuse of skills. It is relevant to your interest in causal discovery and causal representation learning. While it does not directly mention usage of large language models, the methods described are relevant to the broader field of machine learning and causality.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00317" target="_blank">Combining Experimental and Historical Data for Policy Evaluation</a></h3>
            <a href="https://arxiv.org/html/2406.00317v1/extracted/5636101/figs/pessimistic_illustration.png" target="_blank"><img src="https://arxiv.org/html/2406.00317v1/extracted/5636101/figs/pessimistic_illustration.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ting Li, Chengchun Shi, Qianglin Wen, Yang Sui, Yongli Qin, Chunbo Lai, Hongtu Zhu</p>
            <p><strong>Summary:</strong> arXiv:2406.00317v1 Announce Type: cross 
Abstract: This paper studies policy evaluation with multiple data sources, especially in scenarios that involve one experimental dataset with two arms, complemented by a historical dataset generated under a single control arm. We propose novel data integration methods that linearly integrate base policy value estimators constructed based on the experimental and historical data, with weights optimized to minimize the mean square error (MSE) of the resulting combined estimator. We further apply the pessimistic principle to obtain more robust estimators, and extend these developments to sequential decision making. Theoretically, we establish non-asymptotic error bounds for the MSEs of our proposed estimators, and derive their oracle, efficiency and robustness properties across a broad spectrum of reward shift scenarios. Numerical experiments and real-data-based analyses from a ridesharing company demonstrate the superior performance of the proposed estimators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00317">https://arxiv.org/abs/2406.00317</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning as it discusses policy evaluation with multiple data sources and sequential decision making. It also proposes new data integration methods which could be related to causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00853" target="_blank">A Tutorial on Doubly Robust Learning for Causal Inference</a></h3>
            <a href="https://arxiv.org/html/2406.00853v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00853v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hlynur Dav\'i{\dh} Hlynsson</p>
            <p><strong>Summary:</strong> arXiv:2406.00853v1 Announce Type: cross 
Abstract: Doubly robust learning offers a robust framework for causal inference from observational data by integrating propensity score and outcome modeling. Despite its theoretical appeal, practical adoption remains limited due to perceived complexity and inaccessible software. This tutorial aims to demystify doubly robust methods and demonstrate their application using the EconML package. We provide an introduction to causal inference, discuss the principles of outcome modeling and propensity scores, and illustrate the doubly robust approach through simulated case studies. By simplifying the methodology and offering practical coding examples, we intend to make doubly robust learning accessible to researchers and practitioners in data science and statistics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00853">https://arxiv.org/abs/2406.00853</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses doubly robust learning for causal inference, which aligns with your interest in causal discovery and causal representation learning. Though it doesn't specifically mention the use of large language models, the discussed methods may still be relevant and useful to your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2205.09622" target="_blank">What Is Fairness? On the Role of Protected Attributes and Fictitious Worlds</a></h3>
            <a href="https://arxiv.org/html/2205.09622v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2205.09622v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ludwig Bothmann, Kristina Peters, Bernd Bischl</p>
            <p><strong>Summary:</strong> arXiv:2205.09622v5 Announce Type: replace 
Abstract: A growing body of literature in fairness-aware machine learning (fairML) aims to mitigate machine learning (ML)-related unfairness in automated decision-making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods to ensure that trained ML models achieve low scores on these metrics. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a significant gap between centuries of philosophical discussion and the recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We argue that fairness problems can arise even without the presence of protected attributes (PAs), and point out that fairness and predictive performance are not irreconcilable opposites, but that the latter is necessary to achieve the former. Furthermore, we argue why and how causal considerations are necessary when assessing fairness in the presence of PAs by proposing a fictitious, normatively desired (FiND) world in which PAs have no causal effects. In practice, this FiND world must be approximated by a warped world in which the causal effects of the PAs are removed from the real-world data. Finally, we achieve greater linguistic clarity in the discussion of fairML. We outline algorithms for practical applications and present illustrative experiments on COMPAS data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2205.09622">https://arxiv.org/abs/2205.09622</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests in the intersection of causality and machine learning. It focuses on the concept of fairness in machine learning and dives into the causal effects of protected attributes, bridging philosophical considerations and formal machine learning frameworks. Although it does not specifically address causal discovery or representation learning, it provides significant insights that could be useful in understanding and approaching fairness from a causal perspective.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2211.02763" target="_blank">Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes</a></h3>
            <a href="https://arxiv.org/html/2211.02763v3/extracted/5640499/results_aistats_camera/n5/edge_mse.png" target="_blank"><img src="https://arxiv.org/html/2211.02763v3/extracted/5640499/results_aistats_camera/n5/edge_mse.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mizu Nishikawa-Toomey, Tristan Deleu, Jithendaraa Subramanian, Yoshua Bengio, Laurent Charlin</p>
            <p><strong>Summary:</strong> arXiv:2211.02763v3 Announce Type: replace 
Abstract: Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs and mechanisms, while offering several advantages over existing methods, including the guarantee to sample acyclic graphs, and the flexibility to generalize to non-linear causal mechanisms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2211.02763">https://arxiv.org/abs/2211.02763</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly pertinent to your interests as it explores causal discovery, specifically introducing a novel method for Bayesian causal structure learning using Variational Bayes, which aligns with your interest in 'Causal representation learning' and 'Causal Discovery'. However, the paper does not explicitly mention the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.15927" target="_blank">Parameter Estimation in DAGs from Incomplete Data via Optimal Transport</a></h3>
            <a href="https://arxiv.org/html/2305.15927v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.15927v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vy Vo, Trung Le, Tung-Long Vuong, He Zhao, Edwin Bonilla, Dinh Phung</p>
            <p><strong>Summary:</strong> arXiv:2305.15927v4 Announce Type: replace 
Abstract: Estimating the parameters of a probabilistic directed graphical model from incomplete data is a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a general framework that operates on any directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the versatility and robustness of our approach. Across experiments, we show that not only can our method effectively recover the ground-truth parameters but it also performs comparably or better than competing baselines on downstream applications.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.15927">https://arxiv.org/abs/2305.15927</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causality and machine learning. It specifically discusses a new approach for parameter learning in directed acyclic graphs (DAGs), a common structure used for causal discovery. However, it does not specifically mention application with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.02747" target="_blank">Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation</a></h3>
            <a href="https://arxiv.org/html/2306.02747v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.02747v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wanpeng Zhang, Yilin Li, Boyu Yang, Zongqing Lu</p>
            <p><strong>Summary:</strong> arXiv:2306.02747v3 Announce Type: replace 
Abstract: In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge of environments. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by implicitly tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for the state, termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the causal interpretation for non-stationary reinforcement learning, advocating for the validity of the causal-origin representation. Experimental results further demonstrate the superior performance of COREP over existing methods in tackling non-stationarity problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.02747">https://arxiv.org/abs/2306.02747</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper proposes a new algorithm (COREP) in the intersection of causality and reinforcement learning. While it doesn't directly address causal representation learning or causal discovery, it does contain an element of both. COREP uses a causal graph to model the environment and then leverages this representation to learn a policy. Thus, it is an interesting take on introducing causality into machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.15479" target="_blank">Predictive Coding beyond Correlations</a></h3>
            <a href="https://arxiv.org/html/2306.15479v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.15479v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tommaso Salvatori, Luca Pinchetti, Amine M'Charrak, Beren Millidge, Thomas Lukasiewicz</p>
            <p><strong>Summary:</strong> arXiv:2306.15479v2 Announce Type: replace 
Abstract: Recently, there has been extensive research on the capabilities of biologically plausible algorithms. In this work, we show how one of such algorithms, called predictive coding, is able to perform causal inference tasks. First, we show how a simple change in the inference process of predictive coding enables to compute interventions without the need to mutilate or redefine a causal graph. Then, we explore applications in cases where the graph is unknown, and has to be inferred from observational data. Empirically, we show how such findings can be used to improve the performance of predictive coding in image classification tasks, and conclude that such models are able to perform simple end-to-end causal inference tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.15479">https://arxiv.org/abs/2306.15479</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it proposes a novel approach for causal inference tasks and shows how the predictive coding algorithm can be used for causal inference applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08845" target="_blank">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</a></h3>
            <a href="https://arxiv.org/html/2402.08845v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08845v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</p>
            <p><strong>Summary:</strong> arXiv:2402.08845v3 Announce Type: replace 
Abstract: We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs' discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08845">https://arxiv.org/abs/2402.08845</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This research paper is related to your interests as it focuses on causality. The paper introduces a new strategy, Feature Attribution with Necessity and Sufficiency (FANS), which helps in the improvement of discriminative power for feature importance evaluation in machine learning models. It can be beneficial for understanding causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15255" target="_blank">Optimal Transport for Structure Learning Under Missing Data</a></h3>
            <a href="https://arxiv.org/html/2402.15255v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.15255v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vy Vo, He Zhao, Trung Le, Edwin V. Bonilla, Dinh Phung</p>
            <p><strong>Summary:</strong> arXiv:2402.15255v2 Announce Type: replace 
Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or, preferably, causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirically shown to be sub-optimal. To address this problem, we propose a score-based algorithm for learning causal structures from missing data based on optimal transport. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on expectation maximization. We formulate structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the observed data distribution. Our framework is shown to recover the true causal graphs more effectively than competing methods in most simulations and real-data settings. Empirical evidence also shows the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15255">https://arxiv.org/abs/2402.15255</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is aligned with your interest in 'Causal discovery' subtopic. It proposes a new approach for learning causal structures from missing data based on optimal transport, which could be an interesting perspective in the domain of Causality and ML.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.11332" target="_blank">Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects</a></h3>
            
            <p><strong>Authors:</strong> Seyedeh Baharan Khatami, Harsh Parikh, Haowei Chen, Sudeepa Roy, Babak Salimi</p>
            <p><strong>Summary:</strong> arXiv:2403.11332v2 Announce Type: replace 
Abstract: We address the challenge of inferring causal effects in social network data. This results in challenges due to interference -- where a unit's outcome is affected by neighbors' treatments -- and network-induced confounding factors. While there is extensive literature focusing on estimating causal effects in social network setups, a majority of them make prior assumptions about the form of network-induced confounding mechanisms. Such strong assumptions are rarely likely to hold especially in high-dimensional networks. We propose a novel methodology that combines graph machine learning approaches with the double machine learning framework to enable accurate and efficient estimation of direct and peer effects using a single observational social network. We demonstrate the semiparametric efficiency of our proposed estimator under mild regularity conditions, allowing for consistent uncertainty quantification. We demonstrate that our method is accurate, robust, and scalable via an extensive simulation study. We use our method to investigate the impact of Self-Help Group participation on financial risk tolerance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.11332">https://arxiv.org/abs/2403.11332</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses a novel methodology for inferring causal effects in social network data via graph machine learning. Although it does not directly involve language models in causal discovery, it is a significant contribution to the field of causal discovery using machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17653" target="_blank">InversionView: A General-Purpose Method for Reading Information from Neural Activations</a></h3>
            <a href="https://arxiv.org/html/2405.17653v2/extracted/5635137/images/concept-illustration3.png" target="_blank"><img src="https://arxiv.org/html/2405.17653v2/extracted/5635137/images/concept-illustration3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinting Huang, Madhur Panwar, Navin Goyal, Michael Hahn</p>
            <p><strong>Summary:</strong> arXiv:2405.17653v2 Announce Type: replace 
Abstract: The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present three case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17653">https://arxiv.org/abs/2405.17653</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interests in causal representation learning. It aims to better understand the information encoded in neural activations, which is key in revealing the algorithms used by transformer models and points to a direction in causal discovery. It doesn't explicitly propose a new causality method with regards to machine learning, but does offer insights that may be valuable for further exploration of the causality topic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.18626" target="_blank">Causal Contextual Bandits with Adaptive Context</a></h3>
            <a href="https://arxiv.org/html/2405.18626v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.18626v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rahul Madhavan, Aurghya Maiti, Gaurav Sinha, Siddharth Barman</p>
            <p><strong>Summary:</strong> arXiv:2405.18626v2 Announce Type: replace 
Abstract: We study a variant of causal contextual bandits where the context is chosen based on an initial intervention chosen by the learner. At the beginning of each round, the learner selects an initial action, depending on which a stochastic context is revealed by the environment. Following this, the learner then selects a final action and receives a reward. Given $T$ rounds of interactions with the environment, the objective of the learner is to learn a policy (of selecting the initial and the final action) with maximum expected reward. In this paper we study the specific situation where every action corresponds to intervening on a node in some known causal graph. We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees. This is achieved through an instance-dependent causal parameter, $\lambda$, which characterizes our upper bound. Furthermore, we prove that our simple regret is essentially tight for a large class of instances. A key feature of our work is that we use convex optimization to address the bandit exploration problem. We also conduct experiments to validate our theoretical results, and release our code at our project GitHub repository: https://github.com/adaptiveContextualCausalBandits/aCCB.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.18626">https://arxiv.org/abs/2405.18626</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in causality and machine learning, particularly within causal discovery. It explores a variant of causal contextual bandits and proposes a new approach to handle the bandit exploration problem. This might offer some useful insight into causal discovery using machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2002.10790" target="_blank">Biased Stochastic First-Order Methods for Conditional Stochastic Optimization and Applications in Meta Learning</a></h3>
            <a href="https://arxiv.org/html/2002.10790v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2002.10790v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yifan Hu, Siqi Zhang, Xin Chen, Niao He</p>
            <p><strong>Summary:</strong> arXiv:2002.10790v2 Announce Type: replace-cross 
Abstract: Conditional stochastic optimization covers a variety of applications ranging from invariant learning and causal inference to meta-learning. However, constructing unbiased gradient estimators for such problems is challenging due to the composition structure. As an alternative, we propose a biased stochastic gradient descent (BSGD) algorithm and study the bias-variance tradeoff under different structural assumptions. We establish the sample complexities of BSGD for strongly convex, convex, and weakly convex objectives under smooth and non-smooth conditions. Our lower bound analysis shows that the sample complexities of BSGD cannot be improved for general convex objectives and nonconvex objectives except for smooth nonconvex objectives with Lipschitz continuous gradient estimator. For this special setting, we propose an accelerated algorithm called biased SpiderBoost (BSpiderBoost) that matches the lower bound complexity. We further conduct numerical experiments on invariant logistic regression and model-agnostic meta-learning to illustrate the performance of BSGD and BSpiderBoost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2002.10790">https://arxiv.org/abs/2002.10790</a></p>
            <p><strong>Category:</strong> math.OC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper explores conditional stochastic optimization, which may be related to causal inference and is relevant to the topic of causality and machine learning. It also includes meta-learning, which can be seen as a form of utilizing large language models, implying some degree of overlap with your interest in large-language model agents. However, the direct relevance to specific subtopics you provided is not fully clear, hence the score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2102.11076" target="_blank">Kernel Ridge Riesz Representers: Generalization Error and Mis-specification</a></h3>
            <a href="https://arxiv.org/html/2102.11076v3/extracted/5635867/img/CATE_fn.png" target="_blank"><img src="https://arxiv.org/html/2102.11076v3/extracted/5635867/img/CATE_fn.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rahul Singh</p>
            <p><strong>Summary:</strong> arXiv:2102.11076v3 Announce Type: replace-cross 
Abstract: Kernel balancing weights provide confidence intervals for average treatment effects, based on the idea of balancing covariates for the treated group and untreated group in feature space, often with ridge regularization. Previous works on the classical kernel ridge balancing weights have certain limitations: (i) not articulating generalization error for the balancing weights, (ii) typically requiring correct specification of features, and (iii) providing inference for only average effects.
  I interpret kernel balancing weights as kernel ridge Riesz representers (KRRR) and address these limitations via a new characterization of the counterfactual effective dimension. KRRR is an exact generalization of kernel ridge regression and kernel ridge balancing weights. I prove strong properties similar to kernel ridge regression: population $L_2$ rates controlling generalization error, and a standalone closed form solution that can interpolate. The framework relaxes the stringent assumption that the underlying regression model is correctly specified by the features. It extends inference beyond average effects to heterogeneous effects, i.e. causal functions. I use KRRR to infer heterogeneous treatment effects, by age, of 401(k) eligibility on assets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2102.11076">https://arxiv.org/abs/2102.11076</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This research paper is relevant as it discusses the usage of Kernel balancing weights for average treatment effects in causal functions, aligning with your interest in Causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2210.05955" target="_blank">Identifiability and Asymptotics in Learning Homogeneous Linear ODE Systems from Discrete Observations</a></h3>
            <a href="https://arxiv.org/html/2210.05955v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2210.05955v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuanyuan Wang, Wei Huang, Mingming Gong, Xi Geng, Tongliang Liu, Kun Zhang, Dacheng Tao</p>
            <p><strong>Summary:</strong> arXiv:2210.05955v2 Announce Type: replace-cross 
Abstract: Ordinary Differential Equations (ODEs) have recently gained a lot of attention in machine learning. However, the theoretical aspects, e.g., identifiability and asymptotic properties of statistical estimation are still obscure. This paper derives a sufficient condition for the identifiability of homogeneous linear ODE systems from a sequence of equally-spaced error-free observations sampled from a single trajectory. When observations are disturbed by measurement noise, we prove that under mild conditions, the parameter estimator based on the Nonlinear Least Squares (NLS) method is consistent and asymptotic normal with $n^{-1/2}$ convergence rate. Based on the asymptotic normality property, we construct confidence sets for the unknown system parameters and propose a new method to infer the causal structure of the ODE system, i.e., inferring whether there is a causal link between system variables. Furthermore, we extend the results to degraded observations, including aggregated and time-scaled ones. To the best of our knowledge, our work is the first systematic study of the identifiability and asymptotic properties in learning linear ODE systems. We also construct simulations with various system dimensions to illustrate the established theoretical results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2210.05955">https://arxiv.org/abs/2210.05955</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant under the 'causality' tag. It involves understanding the causal structure of ODE systems and proposes a new method for causal inference which aligns with your interest in 'causal discovery'. However, it does not involve large language models, so it does not fully cover all the subtopics you're interested in within 'causality'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.08672" target="_blank">Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal</a></h3>
            <a href="https://arxiv.org/html/2310.08672v2/extracted/5635956/figures/fafsa_text_messages.png" target="_blank"><img src="https://arxiv.org/html/2310.08672v2/extracted/5635956/figures/fafsa_text_messages.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Susan Athey, Niall Keleher, Jann Spiess</p>
            <p><strong>Summary:</strong> arXiv:2310.08672v2 Announce Type: replace-cross 
Abstract: In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective in our specific application, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.08672">https://arxiv.org/abs/2310.08672</a></p>
            <p><strong>Category:</strong> econ.EM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses a practical application of causal machine learning forecasting in the context of student financial aid renewal, which ties in with your interest in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17816" target="_blank">Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs</a></h3>
            
            <p><strong>Authors:</strong> Jacqueline Maasch, Weishen Pan, Shantanu Gupta, Volodymyr Kuleshov, Kyra Gan, Fei Wang</p>
            <p><strong>Summary:</strong> arXiv:2310.17816v3 Announce Type: replace-cross 
Abstract: Causal discovery is crucial for causal inference in observational studies, as it can enable the identification of valid adjustment sets (VAS) for unbiased effect estimation. However, global causal discovery is notoriously hard in the nonparametric setting, with exponential time and sample complexity in the worst case. To address this, we propose local discovery by partitioning (LDP): a local causal discovery method that is tailored for downstream inference tasks without requiring parametric and pretreatment assumptions. LDP is a constraint-based procedure that returns a VAS for an exposure-outcome pair under latent confounding, given sufficient conditions. The total number of independence tests performed is worst-case quadratic with respect to the cardinality of the variable set. Asymptotic theoretical guarantees are numerically validated on synthetic graphs. Adjustment sets from LDP yield less biased and more precise average treatment effect estimates than baseline discovery algorithms, with LDP outperforming on confounder recall, runtime, and test count for VAS discovery. Notably, LDP ran at least 1300x faster than baselines on a benchmark.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17816">https://arxiv.org/abs/2310.17816</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interest in 'Causal discovery' under 'Causality and machine learning'. It introduces a local causal discovery method that could be beneficial for inference tasks. However, it doesn't directly address large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.18639" target="_blank">Targeted Reduction of Causal Models</a></h3>
            <a href="https://arxiv.org/html/2311.18639v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.18639v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Armin Keki\'c, Bernhard Sch\"olkopf, Michel Besserve</p>
            <p><strong>Summary:</strong> arXiv:2311.18639v2 Announce Type: replace-cross 
Abstract: Why does a phenomenon occur? Addressing this question is central to most scientific inquiries and often relies on simulations of scientific models. As models become more intricate, deciphering the causes behind phenomena in high-dimensional spaces of interconnected variables becomes increasingly challenging. Causal Representation Learning (CRL) offers a promising avenue to uncover interpretable causal patterns within these simulations through an interventional lens. However, developing general CRL frameworks suitable for practical applications remains an open challenge. We introduce Targeted Causal Reduction (TCR), a method for condensing complex intervenable models into a concise set of causal factors that explain a specific target phenomenon. We propose an information theoretic objective to learn TCR from interventional data of simulations, establish identifiability for continuous variables under shift interventions and present a practical algorithm for learning TCRs. Its ability to generate interpretable high-level explanations from complex models is demonstrated on toy and mechanical systems, illustrating its potential to assist scientists in the study of complex phenomena in a broad range of disciplines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.18639">https://arxiv.org/abs/2311.18639</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper introduces a new method, Targeted Causal Reduction (TCR), related to causal representation learning, one of your subtopics in causality. While the paper does not specifically mention machine learning, the new method addresses discovering causes behind phenomena in high-dimensional spaces, which aligns with your interest in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.00376" target="_blank">Spurious Feature Eraser: Stabilizing Test-Time Adaptation for Vision-Language Foundation Model</a></h3>
            
            <p><strong>Authors:</strong> Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu</p>
            <p><strong>Summary:</strong> arXiv:2403.00376v2 Announce Type: replace-cross 
Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired data. However, these models also display significant limitations when applied to downstream tasks, such as fine-grained image classification, as a result of ``decision shortcuts'' that hinder their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, we propose a simple yet effective method, Spurious Feature Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious features. Specifically, we introduce a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit invariant features while disregarding decision shortcuts during the inference phase. The proposed method effectively alleviates excessive dependence on potentially misleading spurious information. We conduct comparative analysis of the proposed method against various approaches which validates the significant superiority.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.00376">https://arxiv.org/abs/2403.00376</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses causal features in a vision-language foundation model, which is a topic of interest to you. It also addresses methods for utilizing these pre-trained features effectively, which could be of relevance for your interest in causal representation learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.05963" target="_blank">Robust Emotion Recognition in Context Debiasing</a></h3>
            <a href="https://arxiv.org/html/2403.05963v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.05963v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang</p>
            <p><strong>Summary:</strong> arXiv:2403.05963v3 Announce Type: replace-cross 
Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes, resulting in bias mitigation and robust prediction. As a model-agnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.05963">https://arxiv.org/abs/2403.05963</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses new methods using a causal graph and a proposed counterfactual emotion inference (CLEF) framework discussing causal relationships for emotion recognition, which aligns with your interest in causal discovery and representation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08839" target="_blank">Multiply-Robust Causal Change Attribution</a></h3>
            <a href="https://arxiv.org/html/2404.08839v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08839v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</p>
            <p><strong>Summary:</strong> arXiv:2404.08839v2 Announce Type: replace-cross 
Abstract: Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application. Our method is implemented as part of the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08839">https://arxiv.org/abs/2404.08839</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest in 'causality and machine learning', more specifically in the subtopic of 'causal discovery'. The paper proposes a new estimation strategy for quantifying the contribution of each causal mechanism, which can be useful in building better models for causal representation learning. However, it does not directly address the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.17483" target="_blank">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</a></h3>
            <a href="https://arxiv.org/html/2404.17483v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.17483v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yoichi Chikahara, Kansei Ushiyama</p>
            <p><strong>Summary:</strong> arXiv:2404.17483v5 Announce Type: replace-cross 
Abstract: There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at https://github.com/ychika/DPSW.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.17483">https://arxiv.org/abs/2404.17483</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper pertains to your interest in 'Causality and Machine Learning', particularly in terms of 'Causal representation learning'. It discusses a new framework for improving the estimation of heterogeneous treatment effects, which is essentially a problem related to causal inference.</p>
        </div>
        </div><div class='timestamp'>Report generated on June 04, 2024 at 22:05:38</div></body></html>