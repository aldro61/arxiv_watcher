
            <html>
            <head>
                <title>Report Generated on June 05, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for June 05, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01638" target="_blank">TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment</a></h3>
            <a href="https://arxiv.org/html/2406.01638v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01638v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chenxi Liu, Qianxiong Xu, Hao Miao, Sun Yang, Lingzheng Zhang, Cheng Long, Ziyue Li, Rui Zhao</p>
            <p><strong>Summary:</strong> arXiv:2406.01638v1 Announce Type: new 
Abstract: The widespread adoption of scalable mobile sensing has led to large amounts of time series data for real-world applications. A fundamental application is multivariate time series forecasting (MTSF), which aims to predict future time series values based on historical observations. Existing MTSF methods suffer from limited parameterization and small-scale training data. Recently, Large language models (LLMs) have been introduced in time series, which achieve promising forecasting performance but incur heavy computational costs. To solve these challenges, we propose TimeCMA, an LLM-empowered framework for time series forecasting with cross-modality alignment. We design a dual-modality encoding module with two branches, where the time series encoding branch extracts relatively low-quality yet pure embeddings of time series through an inverted Transformer. In addition, the LLM-empowered encoding branch wraps the same time series as prompts to obtain high-quality yet entangled prompt embeddings via a Pre-trained LLM. Then, we design a cross-modality alignment module to retrieve high-quality and pure time series embeddings from the prompt embeddings. Moreover, we develop a time series forecasting module to decode the aligned embeddings while capturing dependencies among multiple variables for forecasting. Notably, we tailor the prompt to encode sufficient temporal information into a last token and design the last token embedding storage to reduce computational costs. Extensive experiments on real data offer insight into the accuracy and efficiency of the proposed framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01638">https://arxiv.org/abs/2406.01638</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests as it discusses a new method, TimeCMA, a framework for time series forecasting using Large Language Models. It corresponds to your interest in 'New deep learning methods for time series' and 'New transformer-like models for time series'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02486" target="_blank">A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02486v1/extracted/5643462/figures/TKAT.drawio.png" target="_blank"><img src="https://arxiv.org/html/2406.02486v1/extracted/5643462/figures/TKAT.drawio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Remi Genet, Hugo Inzirillo</p>
            <p><strong>Summary:</strong> arXiv:2406.02486v1 Announce Type: new 
Abstract: Capturing complex temporal patterns and relationships within multivariate data streams is a difficult task. We propose the Temporal Kolmogorov-Arnold Transformer (TKAT), a novel attention-based architecture designed to address this task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by the Temporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder model tailored to handle tasks in which the observed part of the features is more important than the a priori known part. This new architecture combined the theoretical foundation of the Kolmogorov-Arnold representation with the power of transformers. TKAT aims to simplify the complex dependencies inherent in time series, making them more "interpretable". The use of transformer architecture in this framework allows us to capture long-range dependencies through self-attention mechanisms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02486">https://arxiv.org/abs/2406.02486</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant as it introduces a new transformer-like model, namely the Temporal Kolmogorov-Arnold Transformer, specifically designed for time series forecasting. Therefore, it fits well within your interest in new deep learning methods as well as transformer models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02131" target="_blank">CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02131v1/extracted/5641994/fig/performance.png" target="_blank"><img src="https://arxiv.org/html/2406.02131v1/extracted/5641994/fig/performance.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianrong Ding, Zhanyu Liu, Guanjie Zheng, Haiming Jin, Linghe Kong</p>
            <p><strong>Summary:</strong> arXiv:2406.02131v1 Announce Type: new 
Abstract: Dataset condensation is a newborn technique that generates a small dataset that can be used in training deep neural networks to lower training costs. The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets. However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting). This challenge arises from disparities in the evaluation of synthetic data. In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution. Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models. The synthetic data is deemed well-distilled only when all data points within the predictions are similar. Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification. To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation designated as Dataset Condensation for Time Series Forecasting (CondTSF) based on our analysis. Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance. We conduct extensive experiments on eight commonly used time series datasets. CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02131">https://arxiv.org/abs/2406.02131</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes a new method, CondTSF, to optimize dataset condensation specifically for Time Series Forecasting. Though it does not introduce a new deep learning model, the approach could impact how other models handle large time series datasets.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02146" target="_blank">Activation Bottleneck: Sigmoidal Neural Networks Cannot Forecast a Straight Line</a></h3>
            <a href="https://arxiv.org/html/2406.02146v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02146v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Maximilian Toller, Hussain Hussain, Bernhard C Geiger</p>
            <p><strong>Summary:</strong> arXiv:2406.02146v1 Announce Type: new 
Abstract: A neural network has an activation bottleneck if one of its hidden layers has a bounded image. We show that networks with an activation bottleneck cannot forecast unbounded sequences such as straight lines, random walks, or any sequence with a trend: The difference between prediction and ground truth becomes arbitrary large, regardless of the training procedure. Widely-used neural network architectures such as LSTM and GRU suffer from this limitation. In our analysis, we characterize activation bottlenecks and explain why they prevent sigmoidal networks from learning unbounded sequences. We experimentally validate our findings and discuss modifications to network architectures which mitigate the effects of activation bottlenecks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02146">https://arxiv.org/abs/2406.02146</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper discusses a limitation in LSTM and GRU networks - specific types of deep learning methods - for time series forecasting, and suggests modifications to network architectures to address this issue, thus being partially related with 'New deep learning methods for time series'</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02322" target="_blank">A Survey of Transformer Enabled Time Series Synthesis</a></h3>
            <a href="https://arxiv.org/html/2406.02322v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02322v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, Thomas Arnold</p>
            <p><strong>Summary:</strong> arXiv:2406.02322v1 Announce Type: new 
Abstract: Generative AI has received much attention in the image and language domains, with the transformer neural network continuing to dominate the state of the art. Application of these models to time series generation is less explored, however, and is of great utility to machine learning, privacy preservation, and explainability research. The present survey identifies this gap at the intersection of the transformer, generative AI, and time series data, and reviews works in this sparsely populated subdomain. The reviewed works show great variety in approach, and have not yet converged on a conclusive answer to the problems the domain poses. GANs, diffusion models, state space models, and autoencoders were all encountered alongside or surrounding the transformers which originally motivated the survey. While too open a domain to offer conclusive insights, the works surveyed are quite suggestive, and several recommendations for best practice, and suggestions of valuable future work, are provided.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02322">https://arxiv.org/abs/2406.02322</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it caters to the 'New transformer-like models for time series' subtopic. Furthermore, it offers insight into the application of transformer networks in the context of time series data, covering different models like GANs, state space models, and autoencoders. Despite the lack of any new method proposed, it provides a comprehensive survey of transformer applications in time series, which might help you in understanding the state of the art, and spotting future research directions.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02395" target="_blank">GrootVL: Tree Topology is All You Need in State Space Model</a></h3>
            <a href="https://arxiv.org/html/2406.02395v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02395v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yicheng Xiao, Lin Song, Shaoli Huang, Jiangshan Wang, Siyu Song, Yixiao Ge, Xiu Li, Ying Shan</p>
            <p><strong>Summary:</strong> arXiv:2406.02395v1 Announce Type: new 
Abstract: The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02395">https://arxiv.org/abs/2406.02395</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it focuses on modeling long-range dependencies in time series data using the GrootVL network. However, it's focused more on image classification and object detection tasks rather than explicit time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02496" target="_blank">Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability</a></h3>
            <a href="https://arxiv.org/html/2406.02496v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02496v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kunpeng Xu, Lifei Chen, Shengrui Wang</p>
            <p><strong>Summary:</strong> arXiv:2406.02496v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field. This innovative concept has rapidly garnered worldwide interest within the AI community. Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability. In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments. MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series. Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability. This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02496">https://arxiv.org/abs/2406.02496</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper appears to propose new methods (T-KAN and MT-KAN) for time series forecasting, which is a direct interest of yours. It also discusses a new foundation model (KAN), enhancing model interpretability and adaptability in time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02204" target="_blank">The Deep Latent Space Particle Filter for Real-Time Data Assimilation with Uncertainty Quantification</a></h3>
            <a href="https://arxiv.org/html/2406.02204v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02204v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nikolaj T. M\"ucke, Sander M. Boht\'e, Cornelis W. Oosterlee</p>
            <p><strong>Summary:</strong> arXiv:2406.02204v1 Announce Type: cross 
Abstract: In Data Assimilation, observations are fused with simulations to obtain an accurate estimate of the state and parameters for a given physical system. Combining data with a model, however, while accurately estimating uncertainty, is computationally expensive and infeasible to run in real-time for complex systems. Here, we present a novel particle filter methodology, the Deep Latent Space Particle filter or D-LSPF, that uses neural network-based surrogate models to overcome this computational challenge. The D-LSPF enables filtering in the low-dimensional latent space obtained using Wasserstein AEs with modified vision transformer layers for dimensionality reduction and transformers for parameterized latent space time stepping. As we demonstrate on three test cases, including leak localization in multi-phase pipe flow and seabed identification for fully nonlinear water waves, the D-LSPF runs orders of magnitude faster than a high-fidelity particle filter and 3-5 times faster than alternative methods while being up to an order of magnitude more accurate. The D-LSPF thus enables real-time data assimilation with uncertainty quantification for physical systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02204">https://arxiv.org/abs/2406.02204</a></p>
            <p><strong>Category:</strong> cs.CE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces a new methodology (D-LSPF) using neural network-based surrogate models. It utilizes transformer layers, which aligns with your interest in transformer-like models for time series. However, it may slightly lean toward application side as it tackles real-time data assimilation with uncertainty quantification for physical systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02368" target="_blank">Timer: Generative Pre-trained Transformers Are Large Time Series Models</a></h3>
            <a href="https://arxiv.org/html/2402.02368v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.02368v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long</p>
            <p><strong>Summary:</strong> arXiv:2402.02368v2 Announce Type: replace 
Abstract: Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world data-scarce scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progress has been achieved with the emergence of large language models, exhibiting unprecedented abilities such as few-shot generalization, scalability, and task generality, which are however absent in small deep models. To change the status quo of training scenario-specific small models from scratch, this paper aims at the early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), which is generative pre-trained by next token prediction and adapted to various downstream tasks with promising capabilities as an LTSM. Code and datasets are available at: https://github.com/thuml/Large-Time-Series-Model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02368">https://arxiv.org/abs/2402.02368</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses new large time series models and provides the development of a Time Series Transformer, which falls under your interest in transformer-like models for time series. It also provides datasets for training these models and discusses new deep learning methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12038" target="_blank">Adaptive Convolutional Forecasting Network Based on Time Series Feature-Driven</a></h3>
            <a href="https://arxiv.org/html/2405.12038v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.12038v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dandan Zhang, Zhiqiang Zhang, Nanguang Chen, Yun Wang</p>
            <p><strong>Summary:</strong> arXiv:2405.12038v2 Announce Type: replace 
Abstract: Time series data in real-world scenarios contain a substantial amount of nonlinear information, which significantly interferes with the training process of models, leading to decreased prediction performance. Therefore, during the time series forecasting process, extracting the local and global time series patterns and understanding the potential nonlinear features among different time observations are highly significant. To address this challenge, we introduce multi-resolution convolution and deformable convolution operations. By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information at different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture potential nonlinear features among time observations. Building upon this, we propose ACNet, an adaptive convolutional network designed to effectively model the local and global temporal dependencies and the nonlinear features between observations in multivariate time series. Specifically, by extracting and fusing time series features at different resolutions, we capture both local contextual information and global patterns in the time series. The designed nonlinear feature adaptive extraction module captures the nonlinear features among different time observations in the time series. We evaluated the performance of ACNet across twelve real-world datasets. The results indicate that ACNet consistently achieves state-of-the-art performance in both short-term and long-term forecasting tasks with favorable runtime efficiency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12038">https://arxiv.org/abs/2405.12038</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper presents ACNet, a novel adaptive convolutional network for time series forecasting, capturing both local and global temporal dependencies. It suggests new methods for handling time series data, fitting well within your first area of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.15731" target="_blank">Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks</a></h3>
            <a href="https://arxiv.org/html/2405.15731v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.15731v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jerome Sieber, Carmen Amo Alonso, Alexandre Didier, Melanie N. Zeilinger, Antonio Orvieto</p>
            <p><strong>Summary:</strong> arXiv:2405.15731v2 Announce Type: replace 
Abstract: Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.15731">https://arxiv.org/abs/2405.15731</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper explores different types of models including State Space Models and RNNs, which are often used in time series, and discusses their application in foundation models. Although it does not specifically mention forecasting or deep learning methods for time series, it could potentially give insights relevant to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.15925" target="_blank">On the Identifiability of Switching Dynamical Systems</a></h3>
            <a href="https://arxiv.org/html/2305.15925v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.15925v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Carles Balsells-Rodas, Yixin Wang, Yingzhen Li</p>
            <p><strong>Summary:</strong> arXiv:2305.15925v4 Announce Type: replace-cross 
Abstract: The identifiability of latent variable models has received increasing attention due to its relevance in interpretability and out-of-distribution generalisation. In this work, we study the identifiability of Switching Dynamical Systems, taking an initial step toward extending identifiability analysis to sequential latent variable models. We first prove the identifiability of Markov Switching Models, which commonly serve as the prior distribution for the continuous latent variables in Switching Dynamical Systems. We present identification conditions for first-order Markov dependency structures, whose transition distribution is parametrised via non-linear Gaussians. We then establish the identifiability of the latent variables and non-linear mappings in Switching Dynamical Systems up to affine transformations, by leveraging identifiability analysis techniques from identifiable deep latent variable models. We finally develop estimation algorithms for identifiable Switching Dynamical Systems. Throughout empirical studies, we demonstrate the practicality of identifiable Switching Dynamical Systems for segmenting high-dimensional time series such as videos, and showcase the use of identifiable Markov Switching Models for regime-dependent causal discovery in climate data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.15925">https://arxiv.org/abs/2305.15925</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper could be of interest as it discusses the identifiability of Switching Dynamical Systems, a relevant topic for time series analysis. Furthermore, it suggests an algorithm for identifiable Switching Dynamical Systems and demonstrates its application on segmenting high-dimensional time series data, which might present new methods for time series analysis.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.09267" target="_blank">Deep Limit Order Book Forecasting</a></h3>
            <a href="https://arxiv.org/html/2403.09267v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.09267v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Antonio Briola, Silvia Bartolucci, Tomaso Aste</p>
            <p><strong>Summary:</strong> arXiv:2403.09267v4 Announce Type: replace-cross 
Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that evaluates predictions' practicality by focusing on the probability of accurately forecasting complete transactions. This work offers academics and practitioners an avenue to make informed and robust decisions on the application of deep learning techniques, their scope and limitations, effectively exploiting emergent statistical properties of the Limit Order Book.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.09267">https://arxiv.org/abs/2403.09267</a></p>
            <p><strong>Category:</strong> q-fin.TR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper explores new deep learning methodologies for forecasting high-frequency Limit Order Book mid-price changes, which falls under time-series forecasting, one of your interests. However, it does not explicitly discuss foundation models, multimodal methods or transformer models, hence it is not a perfect fit.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.18063" target="_blank">Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis</a></h3>
            <a href="https://arxiv.org/html/2403.18063v2/extracted/5640893/Heracles_Teaser.png" target="_blank"><img src="https://arxiv.org/html/2403.18063v2/extracted/5640893/Heracles_Teaser.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Badri N. Patro, Suhas Ranganath, Vinay P. Namboodiri, Vijay S. Agneeswaran</p>
            <p><strong>Summary:</strong> arXiv:2403.18063v2 Announce Type: replace-cross 
Abstract: Transformers have revolutionized image modeling tasks with adaptations like DeIT, Swin, SVT, Biformer, STVit, and FDVIT. However, these models often face challenges with inductive bias and high quadratic complexity, making them less efficient for high-resolution images. State space models (SSMs) such as Mamba, V-Mamba, ViM, and SiMBA offer an alternative to handle high resolution images in computer vision tasks. These SSMs encounter two major issues. First, they become unstable when scaled to large network sizes. Second, although they efficiently capture global information in images, they inherently struggle with handling local information. To address these challenges, we introduce Heracles, a novel SSM that integrates a local SSM, a global SSM, and an attention-based token interaction module. Heracles leverages a Hartely kernel-based state space model for global image information, a localized convolutional network for local details, and attention mechanisms in deeper layers for token interactions. Our extensive experiments demonstrate that Heracles-C-small achieves state-of-the-art performance on the ImageNet dataset with 84.5\% top-1 accuracy. Heracles-C-Large and Heracles-C-Huge further improve accuracy to 85.9\% and 86.4\%, respectively. Additionally, Heracles excels in transfer learning tasks on datasets such as CIFAR-10, CIFAR-100, Oxford Flowers, and Stanford Cars, and in instance segmentation on the MSCOCO dataset. Heracles also proves its versatility by achieving state-of-the-art results on seven time-series datasets, showcasing its ability to generalize across domains with spectral data, capturing both local and global information. The project page is available at this link.\url{https://github.com/badripatro/heracles}</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.18063">https://arxiv.org/abs/2403.18063</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper introduces Heracles, a novel State Space Model (SSM) with attention mechanisms, which has proven successful in time-series analysis on seven datasets, hence potentially of interest regarding 'new deep learning methods for time series', 'new foundation models for time series', and 'datasets to train foundation models for time series'. However, it is not a transformer-like model, nor specifically aimed at multimodal deep learning for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2211.15856" target="_blank">Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting</a></h3>
            <a href="https://arxiv.org/html/2211.15856v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2211.15856v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin A. Cash, Rebecca Willett</p>
            <p><strong>Summary:</strong> arXiv:2211.15856v4 Announce Type: replace 
Abstract: Producing high-quality forecasts of key climate variables, such as temperature and precipitation, on subseasonal time scales has long been a gap in operational forecasting. This study explores an application of machine learning (ML) models as post-processing tools for subseasonal forecasting. Lagged numerical ensemble forecasts (i.e., an ensemble where the members have different initialization dates) and observational data, including relative humidity, pressure at sea level, and geopotential height, are incorporated into various ML methods to predict monthly average precipitation and two-meter temperature two weeks in advance for the continental United States. For regression, quantile regression, and tercile classification tasks, we consider using linear models, random forests, convolutional neural networks, and stacked models (a multi-model approach based on the prediction of the individual ML models). Unlike previous ML approaches that often use ensemble mean alone, we leverage information embedded in the ensemble forecasts to enhance prediction accuracy. Additionally, we investigate extreme event predictions that are crucial for planning and mitigation efforts. Considering ensemble members as a collection of spatial forecasts, we explore different approaches to using spatial information. Trade-offs between different approaches may be mitigated with model stacking. Our proposed models outperform standard baselines such as climatological forecasts and ensemble means. In addition, we investigate feature importance, trade-offs between using the full ensemble or only the ensemble mean, and different modes of accounting for spatial variability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2211.15856">https://arxiv.org/abs/2211.15856</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 3.5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> While this paper does use machine learning models for time-series forecasting, it doesn't seem to propose new deep learning methods or any transformer-like models for time series as per your interests. However, it may be beneficial in terms of understanding how machine learning elements are being integrated into climate forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01823" target="_blank">Causal Discovery with Fewer Conditional Independence Tests</a></h3>
            <a href="https://arxiv.org/html/2406.01823v1/extracted/5641205/plots/FIG1.png" target="_blank"><img src="https://arxiv.org/html/2406.01823v1/extracted/5641205/plots/FIG1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</p>
            <p><strong>Summary:</strong> arXiv:2406.01823v1 Announce Type: new 
Abstract: Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01823">https://arxiv.org/abs/2406.01823</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper's focus is primarily on causal discovery through an algorithm which requires a lesser number of conditional independence tests. It addresses your interest in causal discovery and the development of new methods in causality and machine learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01933" target="_blank">Orthogonal Causal Calibration</a></h3>
            
            <p><strong>Authors:</strong> Justin Whitehouse, Christopher Jung, Vasilis Syrgkanis, Bryan Wilder, Zhiwei Steven Wu</p>
            <p><strong>Summary:</strong> arXiv:2406.01933v1 Announce Type: cross 
Abstract: Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making. Given this importance, one should ensure these estimators are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters.
  In this work, we provide a general framework for calibrating predictors involving nuisance estimation. We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\ell$, under which we say an estimator $\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. We prove generic upper bounds on the calibration error of any causal parameter estimate $\theta$ with respect to any loss $\ell$ using a concept called Neyman Orthogonality. Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true. We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration. One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure. The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01933">https://arxiv.org/abs/2406.01933</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> You should read this paper because it discusses calibrating estimators of causal parameters, such as conditional average treatment effects and conditional quantile treatment effects, which falls under your interest in causal representation learning and causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02310" target="_blank">Disentangled Representation via Variational AutoEncoder for Continuous Treatment Effect Estimation</a></h3>
            <a href="https://arxiv.org/html/2406.02310v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02310v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ruijing Cui, Jianbin Sun, Bingyu He, Kewei Yang, Bingfeng Ge</p>
            <p><strong>Summary:</strong> arXiv:2406.02310v1 Announce Type: new 
Abstract: Continuous treatment effect estimation holds significant practical importance across various decision-making and assessment domains, such as healthcare and the military. However, current methods for estimating dose-response curves hinge on balancing the entire representation by treating all covariates as confounding variables. Although various approaches disentangle covariates into different factors for treatment effect estimation, they are confined to binary treatment settings. Moreover, observational data are often tainted with non-causal noise information that is imperceptible to the human. Hence, in this paper, we propose a novel Dose-Response curve estimator via Variational AutoEncoder (DRVAE) disentangled covariates representation. Our model is dedicated to disentangling covariates into instrumental factors, confounding factors, adjustment factors, and external noise factors, thereby facilitating the estimation of treatment effects under continuous treatment settings by balancing the disentangled confounding factors. Extensive results on synthetic and semi-synthetic datasets demonstrate that our model outperforms the current state-of-the-art methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02310">https://arxiv.org/abs/2406.02310</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper proposes a new method for dealing with causality in continuous treatment settings and aligns with your interest in causal representation learning. However, it is not directly related to the use of large language models in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02464" target="_blank">Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments</a></h3>
            <a href="https://arxiv.org/html/2406.02464v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02464v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jonas Schweisthal, Dennis Frauen, Mihaela van der Schaar, Stefan Feuerriegel</p>
            <p><strong>Summary:</strong> arXiv:2406.02464v1 Announce Type: new 
Abstract: Estimating the conditional average treatment effect (CATE) from observational data is relevant for many applications such as personalized medicine. Here, we focus on the widespread setting where the observational data come from multiple environments, such as different hospitals, physicians, or countries. Furthermore, we allow for violations of standard causal assumptions, namely, overlap within the environments and unconfoundedness. To this end, we move away from point identification and focus on partial identification. Specifically, we show that current assumptions from the literature on multiple environments allow us to interpret the environment as an instrumental variable (IV). This allows us to adapt bounds from the IV literature for partial identification of CATE by leveraging treatment assignment mechanisms across environments. Then, we propose different model-agnostic learners (so-called meta-learners) to estimate the bounds that can be used in combination with arbitrary machine learning models. We further demonstrate the effectiveness of our meta-learners across various experiments using both simulated and real-world data. Finally, we discuss the applicability of our meta-learners to partial identification in instrumental variable settings, such as randomized controlled trials with non-compliance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02464">https://arxiv.org/abs/2406.02464</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper discusses about causal effects and provides methods to estimate bounds in multivariate settings. The context of estimating causal effects from observational data is relevant to your interests in casual discovery. However, it does not explicitly mention the use of large language models in this process.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02049" target="_blank">Causal Effect Identification in LiNGAM Models with Latent Confounders</a></h3>
            <a href="https://arxiv.org/html/2406.02049v1/extracted/5642326/figures/perc_ident_p_20_p_100_random_edge.png" target="_blank"><img src="https://arxiv.org/html/2406.02049v1/extracted/5642326/figures/perc_ident_p_20_p_100_random_edge.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar, Mathias Drton, Negar Kiyavash</p>
            <p><strong>Summary:</strong> arXiv:2406.02049v1 Announce Type: cross 
Abstract: We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02049">https://arxiv.org/abs/2406.02049</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in 'Causality and Machine Learning', specifically to the subtopic of 'Causal discovery'. It discusses the identification of causal effects in models which can be utilized for the purpose of causal discovery. However, it does not mention the use of large language models in the process, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02191" target="_blank">On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data</a></h3>
            <a href="https://arxiv.org/html/2406.02191v1/extracted/5642803/graph/chainlike-var.png" target="_blank"><img src="https://arxiv.org/html/2406.02191v1/extracted/5642803/graph/chainlike-var.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shunxing Fan, Mingming Gong, Kun Zhang</p>
            <p><strong>Summary:</strong> arXiv:2406.02191v1 Announce Type: cross 
Abstract: We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02191">https://arxiv.org/abs/2406.02191</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'Causality and Machine Learning' as it discusses the concept of causal discovery, although  it doesn't specifically mention usage of large language models in this context.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02534" target="_blank">Enhancing predictive imaging biomarker discovery through treatment effect analysis</a></h3>
            
            <p><strong>Authors:</strong> Shuhan Xiao, Lukas Klein, Jens Petersen, Philipp Vollmuth, Paul F. Jaeger, Klaus H. Maier-Hein</p>
            <p><strong>Summary:</strong> arXiv:2406.02534v1 Announce Type: cross 
Abstract: Identifying predictive biomarkers, which forecast individual treatment effectiveness, is crucial for personalized medicine and informs decision-making across diverse disciplines. These biomarkers are extracted from pre-treatment data, often within randomized controlled trials, and have to be distinguished from prognostic biomarkers, which are independent of treatment assignment. Our study focuses on the discovery of predictive imaging biomarkers, aiming to leverage pre-treatment images to unveil new causal relationships. Previous approaches relied on labor-intensive handcrafted or manually derived features, which may introduce biases. In response, we present a new task of discovering predictive imaging biomarkers directly from the pre-treatment images to learn relevant image features. We propose an evaluation protocol for this task to assess a model's ability to identify predictive imaging biomarkers and differentiate them from prognostic ones. It employs statistical testing and a comprehensive analysis of image feature attribution. We explore the suitability of deep learning models originally designed for estimating the conditional average treatment effect (CATE) for this task, which previously have been primarily assessed for the precision of CATE estimation, overlooking the evaluation of imaging biomarker discovery. Our proof-of-concept analysis demonstrates promising results in discovering and validating predictive imaging biomarkers from synthetic outcomes and real-world image datasets.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02534">https://arxiv.org/abs/2406.02534</a></p>
            <p><strong>Category:</strong> eess.IV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper proposes a new task of discovering predictive imaging biomarkers, which aligns with your interest in causal discovery. However, the research does not seem to explore the use of large language models in causal discovery, which lowers its relevance.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.13453" target="_blank">Learning to Intervene on Concept Bottlenecks</a></h3>
            <a href="https://arxiv.org/html/2308.13453v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.13453v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> David Steinmann, Wolfgang Stammer, Felix Friedrich, Kristian Kersting</p>
            <p><strong>Summary:</strong> arXiv:2308.13453v3 Announce Type: replace 
Abstract: While deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Moreover, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Up to this point, these interventions were typically applied to the model just once and then discarded. To rectify this, we present concept bottleneck memory models (CB2Ms), which keep a memory of past interventions. Specifically, CB2Ms leverage a two-fold memory to generalize interventions to appropriate novel situations, enabling the model to identify errors and reapply previous interventions. This way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. Our experimental evaluations on challenging scenarios like handling distribution shifts and confounded data demonstrate that CB2Ms are able to successfully generalize interventions to unseen data and can indeed identify wrongly inferred concepts. Hence, CB2Ms are a valuable tool for users to provide interactive feedback on CBMs, by guiding a user's interaction and requiring fewer interventions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.13453">https://arxiv.org/abs/2308.13453</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in 'causality and machine learning' because it introduces a new model (Concept Bottleneck Memory Models) that allows for causal discovery and causal representation learning. These models create inherent explanations via their concept representations and allow for interventional interactions, aligning with your interests in causal representation learning and causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.00809" target="_blank">Towards Causal Foundation Model: on Duality between Causal Inference and Attention</a></h3>
            <a href="https://arxiv.org/html/2310.00809v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.00809v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma</p>
            <p><strong>Summary:</strong> arXiv:2310.00809v3 Announce Type: replace 
Abstract: Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for treatment effect estimations. We propose a novel, theoretically justified method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset methodologies. These results provide compelling evidence that our method has the potential to serve as a stepping stone for the development of causal foundation models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.00809">https://arxiv.org/abs/2310.00809</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your interests in causality and machine learning. It specifically addresses causal inference within foundation models and proposes a new method 'Causal Inference with Attention' which is based on transformer-type architecture. The paper discusses the use of large unlabeled datasets for self-supervised causal learning, which is directly related to your interest in causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08845" target="_blank">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</a></h3>
            <a href="https://arxiv.org/html/2402.08845v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.08845v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</p>
            <p><strong>Summary:</strong> arXiv:2402.08845v4 Announce Type: replace 
Abstract: We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs' discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08845">https://arxiv.org/abs/2402.08845</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it delves into the area of causal discovery and representation learning, specifically focusing on Feature Attribution Methods (FAMs). The new method proposed, FANS, might contribute to understanding causal relationships in machine learning models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.11548" target="_blank">Adaptive Online Experimental Design for Causal Discovery</a></h3>
            <a href="https://arxiv.org/html/2405.11548v2/extracted/5642095/synthresL1.png" target="_blank"><img src="https://arxiv.org/html/2405.11548v2/extracted/5642095/synthresL1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</p>
            <p><strong>Summary:</strong> arXiv:2405.11548v2 Announce Type: replace 
Abstract: Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.11548">https://arxiv.org/abs/2405.11548</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant since it discusses causal discovery, a subtopic you are interested in. It also presents a new method for adaptive online experimental design for causal discovery, thus aligning with your interest in new method proposals.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.14928" target="_blank">Graph Machine Learning in the Era of Large Language Models (LLMs)</a></h3>
            <a href="https://arxiv.org/html/2404.14928v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.14928v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li</p>
            <p><strong>Summary:</strong> arXiv:2404.14928v2 Announce Type: replace 
Abstract: Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.14928">https://arxiv.org/abs/2404.14928</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is very relevant as it applies large language models in the domain of Graph Machine Learning. It specifically discusses enhancing the reasoning capabilities of these models, a concept closely related to application in controlling software or automating tasks as you're interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02061" target="_blank">Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.02061v1/extracted/5641230/figures/Humpty_Dumpty_Tenniel.jpg" target="_blank"><img src="https://arxiv.org/html/2406.02061v1/extracted/5641230/figures/Humpty_Dumpty_Tenniel.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev</p>
            <p><strong>Summary:</strong> arXiv:2406.02061v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02061">https://arxiv.org/abs/2406.02061</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper deals with the functionality and reasoning capabilities of Large Language Models, which fit into your 'Agents based on large-language models' interest. Moreover, it discusses both their successful and failed responses, which provide insights into their control capabilities.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02128" target="_blank">Iteration Head: A Mechanistic Study of Chain-of-Thought</a></h3>
            <a href="https://arxiv.org/html/2406.02128v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02128v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Yang, Francois Charton, Julia Kempe</p>
            <p><strong>Summary:</strong> arXiv:2406.02128v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power. However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited. This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting. In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined "iteration heads". We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02128">https://arxiv.org/abs/2406.02128</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your 'Agents based on large-language models' interest. It discusses the mechanism of 'Chain-of-Thought' reasoning in large language models and transformers, which seems relevant to your interest in understanding how large language models can be used in automation and complex tasks. However, it doesn't directly touch on control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02214" target="_blank">SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining</a></h3>
            <a href="https://arxiv.org/html/2406.02214v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02214v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andi Han, Jiaxiang Li, Wei Huang, Mingyi Hong, Akiko Takeda, Pratik Jawanpuria, Bamdev Mishra</p>
            <p><strong>Summary:</strong> arXiv:2406.02214v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02214">https://arxiv.org/abs/2406.02214</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it discusses a method for efficient training of Large Language Models, which is directly related to your interest in utilizing such models for controlling software and computer automation. However, it doesn't specifically focus on controlling software or web browsers, hence the score isn't the maximum.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02290" target="_blank">A Study of Optimizations for Fine-tuning Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.02290v1/extracted/5634503/model_states_memory_diagram.png" target="_blank"><img src="https://arxiv.org/html/2406.02290v1/extracted/5634503/model_states_memory_diagram.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Arjun Singh, Nikhil Pandey, Anup Shirgaonkar, Pavan Manoj, Vijay Aski</p>
            <p><strong>Summary:</strong> arXiv:2406.02290v1 Announce Type: new 
Abstract: Fine-tuning large language models is a popular choice among users trying to adapt them for specific applications. However, fine-tuning these models is a demanding task because the user has to examine several factors, such as resource budget, runtime, model size and context length among others. A specific challenge is that fine-tuning is memory intensive, imposing constraints on the required hardware memory and context length of training data that can be handled. In this work, we share a detailed study on a variety of fine-tuning optimizations across different fine-tuning scenarios. In particular, we assess Gradient Checkpointing, Low Rank Adaptation, DeepSpeed's ZeRO Redundancy Optimizer and Flash Attention. With a focus on memory and runtime, we examine the impact of different optimization combinations on GPU memory usage and execution runtime during fine-tuning phase. We provide recommendation on best default optimization for balancing memory and runtime across diverse model sizes. We share effective strategies for fine-tuning very large models with tens or hundreds of billions of parameters and enabling large context lengths during fine-tuning. Furthermore, we propose the appropriate optimization mixtures for fine-tuning under GPU resource limitations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02290">https://arxiv.org/abs/2406.02290</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could be relevant to your interest in 'Agents based on large-language models'. It discusses the process of fine-tuning large language models, which is an integral part of controlling software or automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02356" target="_blank">Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks</a></h3>
            <a href="https://arxiv.org/html/2406.02356v1/extracted/5643500/1_592_392_first_digit_unconditional/Llama-2-7b-hf_592392_token_dist_position_1.png" target="_blank"><img src="https://arxiv.org/html/2406.02356v1/extracted/5643500/1_592_392_first_digit_unconditional/Llama-2-7b-hf_592392_token_dist_position_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Andrew Gambardella, Yusuke Iwasawa, Yutaka Matsuo</p>
            <p><strong>Summary:</strong> arXiv:2406.02356v1 Announce Type: new 
Abstract: The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate. We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by m-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve. Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an n-digit by m-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized. We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and Mistral-7B by 150% (0.22 to 0.55).</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02356">https://arxiv.org/abs/2406.02356</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper analyzes the capabilities of large language models, specifically their ability to perform arithmetic tasks. It's relevant to your interest in LLM Agents, though it doesn't directly touch on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02479" target="_blank">Applying Fine-Tuned LLMs for Reducing Data Needs in Load Profile Analysis</a></h3>
            
            <p><strong>Authors:</strong> Yi Hu, Hyeonjin Kim, Kai Ye, Ning Lu</p>
            <p><strong>Summary:</strong> arXiv:2406.02479v1 Announce Type: new 
Abstract: This paper presents a novel method for utilizing fine-tuned Large Language Models (LLMs) to minimize data requirements in load profile analysis, demonstrated through the restoration of missing data in power system load profiles. A two-stage fine-tuning strategy is proposed to adapt a pre-trained LLMs, i.e., GPT-3.5, for missing data restoration tasks. Through empirical evaluation, we demonstrate the effectiveness of the fine-tuned model in accurately restoring missing data, achieving comparable performance to state-of-the-art specifically designed models such as BERT-PIN. Key findings include the importance of prompt engineering and the optimal utilization of fine-tuning samples, highlighting the efficiency of few-shot learning in transferring knowledge from general user cases to specific target users. Furthermore, the proposed approach demonstrates notable cost-effectiveness and time efficiency compared to training models from scratch, making it a practical solution for scenarios with limited data availability and computing resources. This research has significant potential for application to other power system load profile analysis tasks. Consequently, it advances the use of LLMs in power system analytics, offering promising implications for enhancing the resilience and efficiency of power distribution systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02479">https://arxiv.org/abs/2406.02479</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper uses a fine-tuned Large Language Model (LLM) for data restoration tasks in power system load profiles. However, it relates to the aspect of your interest in computer automation using large language models, rather than controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02542" target="_blank">Loki: Low-Rank Keys for Efficient Sparse Attention</a></h3>
            <a href="https://arxiv.org/html/2406.02542v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02542v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele</p>
            <p><strong>Summary:</strong> arXiv:2406.02542v1 Announce Type: new 
Abstract: Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02542">https://arxiv.org/abs/2406.02542</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Even though the paper does not directly fall into controlling software or web browsers with LLMs, it talks about improving performance of large language models via a novel sparse attention method. This could enhance the efficiency of LLMs in controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02543" target="_blank">To Believe or Not to Believe Your LLM</a></h3>
            <a href="https://arxiv.org/html/2406.02543v1/extracted/5640955/prob_London.png" target="_blank"><img src="https://arxiv.org/html/2406.02543v1/extracted/5640955/prob_London.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yasin Abbasi Yadkori, Ilja Kuzborskij, Andr\'as Gy\"orgy, Csaba Szepesv\'ari</p>
            <p><strong>Summary:</strong> arXiv:2406.02543v1 Announce Type: new 
Abstract: We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02543">https://arxiv.org/abs/2406.02543</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interests in large language models (LLMs). It specifically discusses uncertainty in responses given a query by LLMs which is fundamental to controlling software or automation tasks. Although it does not mention controlling web browsers or specific automation use-cases, the studied concept potentially translates to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02550" target="_blank">Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</a></h3>
            <a href="https://arxiv.org/html/2406.02550v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02550v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov</p>
            <p><strong>Summary:</strong> arXiv:2406.02550v1 Announce Type: new 
Abstract: Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a \, x + b \, y \;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing the highly structured representations in both phases; and discuss the learnt algorithm.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02550">https://arxiv.org/abs/2406.02550</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper discusses the application of large language models (here, a GPT-style transformer) for out-of-distribution generalisation and in-context learning, which could have implications for using similar models in software control or automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01631" target="_blank">An LLM-based Recommender System Environment</a></h3>
            <a href="https://arxiv.org/html/2406.01631v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01631v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nathan Corecco, Giorgio Piatti, Luca A. Lanzend\"orfer, Flint Xiaofeng Fan, Roger Wattenhofer</p>
            <p><strong>Summary:</strong> arXiv:2406.01631v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has gained popularity in the realm of recommender systems due to its ability to optimize long-term rewards and guide users in discovering relevant content. However, the successful implementation of RL in recommender systems is challenging because of several factors, including the limited availability of online data for training on-policy methods. This scarcity requires expensive human interaction for online model training. Furthermore, the development of effective evaluation frameworks that accurately reflect the quality of models remains a fundamental challenge in recommender systems. To address these challenges, we propose a comprehensive framework for synthetic environments that simulate human behavior by harnessing the capabilities of large language models (LLMs). We complement our framework with in-depth ablation studies and demonstrate its effectiveness with experiments on movie and book recommendations. By utilizing LLMs as synthetic users, this work introduces a modular and novel framework for training RL-based recommender systems. The software, including the RL environment, is publicly available.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01631">https://arxiv.org/abs/2406.01631</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents an application of large language models as synthetic users in a recommender system environment, which falls under the interest of using large language models for software control and automation. While it doesn't focus specifically on new control methods, its perspective on using LLMs could be highly relevant.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01633" target="_blank">On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots</a></h3>
            
            <p><strong>Authors:</strong> Christine Herlihy, Jennifer Neville, Tobias Schnabel, Adith Swaminathan</p>
            <p><strong>Summary:</strong> arXiv:2406.01633v1 Announce Type: cross 
Abstract: We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators -- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system.
  We first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01633">https://arxiv.org/abs/2406.01633</a></p>
            <p><strong>Category:</strong> cs.IR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is useful as it discusses LLM-based chatbots which aligns with your interest in agents based on large-language models. However, it primarily focuses on recommender systems and not specifically on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01698" target="_blank">Demystifying Platform Requirements for Diverse LLM Inference Use Cases</a></h3>
            <a href="https://arxiv.org/html/2406.01698v1/extracted/5640861/Figures/Sources/2_sypder.png" target="_blank"><img src="https://arxiv.org/html/2406.01698v1/extracted/5640861/Figures/Sources/2_sypder.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna</p>
            <p><strong>Summary:</strong> arXiv:2406.01698v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question. In this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters. Our analysis provides insights into configuring platforms for different LLM workloads and use cases. We quantify the platform requirements to support SOTA LLMs models like LLaMA and GPT-4 under diverse serving settings. Furthermore, we project the hardware capabilities needed to enable future LLMs potentially exceeding hundreds of trillions of parameters. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01698">https://arxiv.org/abs/2406.01698</a></p>
            <p><strong>Category:</strong> cs.AR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the deployment and hardware requirements of large language models (LLMs), which is certainly relevant to your interest in agents based on large-language models. However, it doesn't explicitly deal with controlling software or web browsers with LLMs, nor does it focus on computer automation using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01940" target="_blank">Process-Driven Autoformalization in Lean 4</a></h3>
            <a href="https://arxiv.org/html/2406.01940v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01940v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianqiao Lu, Zhengying Liu, Yingjia Wan, Yinya Huang, Haiming Wang, Zhicheng Yang, Jing Tang, Zhijiang Guo</p>
            <p><strong>Summary:</strong> arXiv:2406.01940v1 Announce Type: cross 
Abstract: Autoformalization, the conversion of natural language mathematics into formal languages, offers significant potential for advancing mathematical reasoning. However, existing efforts are limited to formal languages with substantial online corpora and struggle to keep pace with rapidly evolving languages like Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the autoformalization capabilities of large language models (LLMs). This benchmark encompasses a comprehensive assessment of questions, answers, formal statements, and proofs. Additionally, we introduce a \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model that leverages the precise feedback from Lean 4 compilers to enhance autoformalization. Our experiments demonstrate that the PSV method improves autoformalization, enabling higher accuracy using less filtered training data. Furthermore, when fine-tuned with data containing detailed process information, PSV can leverage the data more effectively, leading to more significant improvements in autoformalization for Lean 4. Our dataset and code are available at \url{https://github.com/rookie-joe/PDA}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01940">https://arxiv.org/abs/2406.01940</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the potential for Large Language Models (LLMs) in the process of autoformalization, which aligns closely with controlling software or partaking in computer automation tasks, as they relate to the complex task of mathematical reasoning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01967" target="_blank">DrEureka: Language Model Guided Sim-To-Real Transfer</a></h3>
            <a href="https://arxiv.org/html/2406.01967v1/extracted/5641960/figures/concept/concept.png" target="_blank"><img src="https://arxiv.org/html/2406.01967v1/extracted/5641960/figures/concept/concept.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam Wang, Yuke Zhu, Linxi Fan, Osbert Bastani, Dinesh Jayaraman</p>
            <p><strong>Summary:</strong> arXiv:2406.01967v1 Announce Type: cross 
Abstract: Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01967">https://arxiv.org/abs/2406.01967</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant as it investigates the use of large language models to aid in the automation of sim-to-real design. This falls under the broader topic and interest of using large language models for agent control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02536" target="_blank">Mitigate Position Bias in Large Language Models via Scaling a Single Dimension</a></h3>
            <a href="https://arxiv.org/html/2406.02536v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02536v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu</p>
            <p><strong>Summary:</strong> arXiv:2406.02536v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02536">https://arxiv.org/abs/2406.02536</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses Large Language Models (LLM) and ways to improve their performance through mitigation of position bias, which can be relevant to your interest in large language models, particularly its application in controlling software or automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.16617" target="_blank">Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model</a></h3>
            <a href="https://arxiv.org/html/2305.16617v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.16617v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yibo Miao, Hongcheng Gao, Hao Zhang, Zhijie Deng</p>
            <p><strong>Summary:</strong> arXiv:2305.16617v3 Announce Type: replace 
Abstract: The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.16617">https://arxiv.org/abs/2305.16617</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper is not directly tied to control or automation using large language models, it closely related to the functions and utilization of large language model. Relevant primarily due to its topic of efficient detection of machine-generated texts from large language models, it could provide you valuable insights for the safe deployment of LLMs as agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.07579" target="_blank">In-Context Unlearning: Language Models as Few Shot Unlearners</a></h3>
            <a href="https://arxiv.org/html/2310.07579v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.07579v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</p>
            <p><strong>Summary:</strong> arXiv:2310.07579v3 Announce Type: replace 
Abstract: Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.07579">https://arxiv.org/abs/2310.07579</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper interestingly talks about 'In-Context Unlearning' in Large Language Models, which isn't directly related to their application as agents, but it could have some useful implications on their use for controlling software or for automation since unlearning is about modifying the behavior of these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.17022" target="_blank">Controlled Decoding from Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.17022v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2310.17022v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, Ahmad Beirami</p>
            <p><strong>Summary:</strong> arXiv:2310.17022v3 Announce Type: replace 
Abstract: KL-regularized reinforcement learning (RL) is a popular alignment framework to control the language model responses towards high reward outcomes. We pose a tokenwise RL objective and propose a modular solver for it, called controlled decoding (CD). CD exerts control through a separate prefix scorer module, which is trained to learn a value function for the reward. The prefix scorer is used at inference time to control the generation from a frozen base model, provably sampling from a solution to the RL objective. We empirically demonstrate that CD is effective as a control mechanism on popular benchmarks. We also show that prefix scorers for multiple rewards may be combined at inference time, effectively solving a multi-objective RL problem with no additional training. We show that the benefits of applying CD transfer to an unseen base model with no further tuning as well. Finally, we show that CD can be applied in a blockwise decoding fashion at inference-time, essentially bridging the gap between the popular best-of-K strategy and tokenwise control through reinforcement learning. This makes CD a promising approach for alignment of language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.17022">https://arxiv.org/abs/2310.17022</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper pertains to your interest in the application of large language models for controlling outcomes. It details a method called controlled decoding, which uses reinforcement learning to guide language model responses towards high reward outcomes. Although it does not directly discuss controlling software or web browsers with large language models, it lays foundational concepts that could be applied in those fields.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.16291" target="_blank">Observable Propagation: Uncovering Feature Vectors in Transformers</a></h3>
            <a href="https://arxiv.org/html/2312.16291v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.16291v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jacob Dunefsky, Arman Cohan</p>
            <p><strong>Summary:</strong> arXiv:2312.16291v2 Announce Type: replace 
Abstract: A key goal of current mechanistic interpretability research in NLP is to find linear features (also called "feature vectors") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called "observable propagation" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- using almost no data. Our paradigm centers on the concept of "observables", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the coupling coefficient which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.16291">https://arxiv.org/abs/2312.16291</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because while it does not directly discuss controlling software or browsers using large language models, it investigates interpretability aspects of transformer language models, which act as foundation models for large language models. The techniques and findings could be applicable in creating more efficient or transparent LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.06627" target="_blank">Feedback Loops With Language Models Drive In-Context Reward Hacking</a></h3>
            <a href="https://arxiv.org/html/2402.06627v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.06627v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Pan, Erik Jones, Meena Jagadeesan, Jacob Steinhardt</p>
            <p><strong>Summary:</strong> arXiv:2402.06627v2 Announce Type: replace 
Abstract: Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.06627">https://arxiv.org/abs/2402.06627</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the impacts of large language models influencing the behaviour of the external world as autonomous agents which includes control over APIs that read and write to web pages. Though it doesn't propose new methods, it critically discusses current ones.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01124" target="_blank">Latent Logic Tree Extraction for Event Sequence Explanation from LLMs</a></h3>
            <a href="https://arxiv.org/html/2406.01124v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01124v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li</p>
            <p><strong>Summary:</strong> arXiv:2406.01124v2 Announce Type: replace 
Abstract: Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01124">https://arxiv.org/abs/2406.01124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of Large Language Models (LLMs) to derive explanations for event sequences, which is related to your interest in using LLMs for complex tasks, though it does not specifically tackle control of software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2303.00438" target="_blank">A Framework for Neurosymbolic Robot Action Planning using Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2303.00438v3/extracted/5642991/robot.png" target="_blank"><img src="https://arxiv.org/html/2303.00438v3/extracted/5642991/robot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alessio Capitanelli, Fulvio Mastrogiovanni</p>
            <p><strong>Summary:</strong> arXiv:2303.00438v3 Announce Type: replace-cross 
Abstract: Symbolic task planning is a widely used approach to enforce robot autonomy due to its ease of understanding and deployment in robot architectures. However, techniques for symbolic task planning are difficult to scale in real-world, human-robot collaboration scenarios because of the poor performance in complex planning domains or when frequent re-planning is needed. We present a framework, Teriyaki, specifically aimed at bridging the gap between symbolic task planning and machine learning approaches. The rationale is training Large Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner compatible with the Planning Domain Definition Language (PDDL), and then leveraging its generative capabilities to overcome a number of limitations inherent to symbolic task planners. Potential benefits include (i) a better scalability in so far as the planning domain complexity increases, since LLMs' response time linearly scales with the combined length of the input and the output, and (ii) the ability to synthesize a plan action-by-action instead of end-to-end, making each action available for execution as soon as it is generated instead of waiting for the whole plan to be available, which in turn enables concurrent planning and execution. Recently, significant efforts have been devoted by the research community to evaluate the cognitive capabilities of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an overall planning performance comparable to traditional planners in specific planning domains, while leveraging LLMs capabilities to build a look-ahead predictive planning model. Preliminary results in selected domains show that our method can: (i) solve 95.5% of problems in a test data set of 1,000 samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic planner; (iii) reduce average overall waiting times for a plan availability by up to 61.4%</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2303.00438">https://arxiv.org/abs/2303.00438</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it goes in depth on how Large Language Models (LLMs) like GPT-3 can be leveraged for symbolic task planning in robotics. Although it does not explicitly mention the control of software or web browsers, it fits your interest in machine learning and computer automation using LLMs. However, it is not strictly focused on proposing a completely new method, but more on the application of an existing model, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2308.10248" target="_blank">Activation Addition: Steering Language Models Without Optimization</a></h3>
            <a href="https://arxiv.org/html/2308.10248v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2308.10248v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid</p>
            <p><strong>Summary:</strong> arXiv:2308.10248v4 Announce Type: replace-cross 
Abstract: Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2308.10248">https://arxiv.org/abs/2308.10248</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it presents a technique for steering the behaviour of large language models (LLMs) known as 'Activation Addition'. This method manipulates model behavior at inference time, allowing for control of high-level properties of the output and might be useful in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.08516" target="_blank">LLMs cannot find reasoning errors, but can correct them given the error location</a></h3>
            <a href="https://arxiv.org/html/2311.08516v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.08516v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Gladys Tyen, Hassan Mansoor, Victor C\u{a}rbune, Peter Chen, Tony Mak</p>
            <p><strong>Summary:</strong> arXiv:2311.08516v3 Announce Type: replace-cross 
Abstract: While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs on their mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs -- separately from mistake finding -- using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs' correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.08516">https://arxiv.org/abs/2311.08516</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the performance of large language models, especially in regard to their self-correction abilities, which is relevant to your interest in large language model-based agents. However, the paper does not directly discuss their use for computer automation or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.04455" target="_blank">Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use</a></h3>
            <a href="https://arxiv.org/html/2312.04455v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.04455v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, Rui Yan</p>
            <p><strong>Summary:</strong> arXiv:2312.04455v4 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, Attention Buckets also exhibited notable enhancements in performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.04455">https://arxiv.org/abs/2312.04455</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper will be relevant for you because it discusses enhancing the context awareness of Large Language Models (LLMs) for effective tool use. While it doesn't directly address controlling software or web browsers, it focuses on improving tool-use by LLMs which can be directly relevant to improve computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.12200" target="_blank">APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</a></h3>
            <a href="https://arxiv.org/html/2401.12200v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.12200v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao</p>
            <p><strong>Summary:</strong> arXiv:2401.12200v2 Announce Type: replace-cross 
Abstract: Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.12200">https://arxiv.org/abs/2401.12200</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in agents based on large-language models. The paper discusses strategies for fine-tuning and inference with large Language Models, which are applied to software controls.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04247" target="_blank">Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</a></h3>
            <a href="https://arxiv.org/html/2402.04247v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04247v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein</p>
            <p><strong>Summary:</strong> arXiv:2402.04247v3 Announce Type: replace-cross 
Abstract: Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents, called scientific LLM agents, also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This perspective paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04247">https://arxiv.org/abs/2402.04247</a></p>
            <p><strong>Category:</strong> cs.CY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models used as agents. It discusses the potential vulnerabilities of these agents, particularly in scientific domains, and the need for safety measures. While it doesn't propose new ways of using LLMs for software or web browser control directly, the paper could provide valuable insights about the understanding of LLM-based agents, which can indirectly benefit the development of new methods in these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.06126" target="_blank">Learn To be Efficient: Build Structured Sparsity in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.06126v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.06126v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Haizhong Zheng, Xiaoyan Bai, Xueshen Liu, Z. Morley Mao, Beidi Chen, Fan Lai, Atul Prakash</p>
            <p><strong>Summary:</strong> arXiv:2402.06126v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. However, existing methods only focus on utilizing this naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity. To achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations. Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms SOTA baselines. Along with our hardware-aware custom kernel implementation, LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.06126">https://arxiv.org/abs/2402.06126</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper goes in detail about Large Language Models (LLMs) and introduces the Learn-To-be-Efficient (LTE) training algorithm, which might influence the efficiency of using LLMs to control web browsers or software. It doesn't specifically address their application for controlling software or web browsers, hence the 4 score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.06529" target="_blank">Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity</a></h3>
            <a href="https://arxiv.org/html/2402.06529v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.06529v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kaiqu Liang, Zixu Zhang, Jaime Fern\'andez Fisac</p>
            <p><strong>Summary:</strong> arXiv:2402.06529v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries. Code is available at https://github.com/kevinliang888/IntroPlan.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.06529">https://arxiv.org/abs/2402.06529</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it details the use of large language models in guiding robot behavior, which is closely tied to your interest in large language model-controlled agents. However, it does not specifically address your subtopics of controlling software or web browsers, so it does not earn a maximum score.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.09615" target="_blank">API Pack: A Massive Multi-Programming Language Dataset for API Call Generation</a></h3>
            <a href="https://arxiv.org/html/2402.09615v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.09615v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda</p>
            <p><strong>Summary:</strong> arXiv:2402.09615v4 Announce Type: replace-cross 
Abstract: We introduce API Pack, a massive multi-programming language dataset containing more than 1 million instruction-API call pairs to improve the API call generation capabilities of large language models. By fine-tuning CodeLlama-13B on 20,000 Python instances from API Pack, we enable it to outperform GPT-3.5 and GPT-4 in generating unseen API calls. Fine-tuning on API Pack also facilitates cross-programming language generalization by leveraging a large amount of data in one language and small amounts of data from other languages. Scaling the training data to 1 million instances further improves the model's ability to generalize to new APIs not used in training. To facilitate further research, we open-source the API Pack dataset, trained model, and associated source code at https://github.com/zguo0525/API-Pack.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.09615">https://arxiv.org/abs/2402.09615</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper might be interesting because it details how API calls in different programming languages can be improved using a large language model. Although it doesn't specifically cover web browsing or computer automation, the techniques could potentially be applied to those areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10693" target="_blank">Exploring Precision and Recall to assess the quality and diversity of LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.10693v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10693v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Florian Le Bronnec, Alexandre Verine, Benjamin Negrevergne, Yann Chevaleyre, Alexandre Allauzen</p>
            <p><strong>Summary:</strong> arXiv:2402.10693v3 Announce Type: replace-cross 
Abstract: We introduce a novel evaluation framework for Large Language Models (LLMs) such as \textsc{Llama-2} and \textsc{Mistral}, focusing on importing Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals new insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned on instruction dataset or with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges that current LLMs face in generating diverse and high-quality text. We release our code and data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10693">https://arxiv.org/abs/2402.10693</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper primarily focuses on evaluation framework for large language models (LLMs), revealing their capabilities and challenges related to text generation tasks. These insights could be useful for developing LLMs to control software or perform automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.13542" target="_blank">ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling</a></h3>
            <a href="https://arxiv.org/html/2402.13542v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.13542v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang</p>
            <p><strong>Summary:</strong> arXiv:2402.13542v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.13542">https://arxiv.org/abs/2402.13542</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents a new technique for training retriever models, which are a core component of large language models used for controlling software. The paper also discusses about how to adapt large language models to specific domains and mitigate hallucinations in knowledge-intensive tasks. Therefore, it might interest you.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.13516" target="_blank">LIRE: listwise reward enhancement for preference alignment</a></h3>
            <a href="https://arxiv.org/html/2405.13516v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.13516v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao</p>
            <p><strong>Summary:</strong> arXiv:2405.13516v2 Announce Type: replace-cross 
Abstract: Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.13516">https://arxiv.org/abs/2405.13516</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is somewhat relevant to your interests. It discusses Large Language Models (LLMs) and addresses a new approach for aligning the generation of LLMs with human values. While it may not directly discuss controlling software or web browsers (your specific subtopics of interest), it presents new ways of manipulating and refining LLM performance that may be applicable to your interests in 'computer automation using large language models'.</p>
        </div>
        </div><div class='timestamp'>Report generated on June 05, 2024 at 21:49:41</div></body></html>