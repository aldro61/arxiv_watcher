
            <html>
            <head>
                <title>Report Generated on June 06, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for June 06, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.07344" target="_blank">TKAN: Temporal Kolmogorov-Arnold Networks</a></h3>
            <a href="https://arxiv.org/html/2405.07344v2/extracted/5646652/figures/TKAN.drawio.png" target="_blank"><img src="https://arxiv.org/html/2405.07344v2/extracted/5646652/figures/TKAN.drawio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Remi Genet, Hugo Inzirillo</p>
            <p><strong>Summary:</strong> arXiv:2405.07344v2 Announce Type: replace 
Abstract: Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management. This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency. By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.07344">https://arxiv.org/abs/2405.07344</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it proposes a new neural network architecture, Temporal Kolmogorov-Arnold Networks (TKANs), for multi-step time series forecasting. The newly introduced method combines features of Kolmogorov-Arnold Networks (KANs) and Long Short-Term Memory (LSTM) models. Therefore, it matches with your interest in new deep learning strategies for time series data.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02131" target="_blank">CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02131v2/extracted/5644737/fig/performance.png" target="_blank"><img src="https://arxiv.org/html/2406.02131v2/extracted/5644737/fig/performance.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jianrong Ding, Zhanyu Liu, Guanjie Zheng, Haiming Jin, Linghe Kong</p>
            <p><strong>Summary:</strong> arXiv:2406.02131v2 Announce Type: replace 
Abstract: Dataset condensation is a newborn technique that generates a small dataset that can be used in training deep neural networks to lower training costs. The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets. However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting). This challenge arises from disparities in the evaluation of synthetic data. In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution. Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models. The synthetic data is deemed well-distilled only when all data points within the predictions are similar. Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification. To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation designated as Dataset Condensation for Time Series Forecasting (CondTSF) based on our analysis. Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance. We conduct extensive experiments on eight commonly used time series datasets. CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02131">https://arxiv.org/abs/2406.02131</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper proposes CondTSF, a new method for time series forecasting by using a dataset condensation technique, which is one of your interests in the 'time-series' category. The paper helps in understanding ways to lower training costs and enhances the performance of time-series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02486" target="_blank">A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02486v2/extracted/5646631/figures/TKAT.drawio.png" target="_blank"><img src="https://arxiv.org/html/2406.02486v2/extracted/5646631/figures/TKAT.drawio.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Remi Genet, Hugo Inzirillo</p>
            <p><strong>Summary:</strong> arXiv:2406.02486v2 Announce Type: replace 
Abstract: Capturing complex temporal patterns and relationships within multivariate data streams is a difficult task. We propose the Temporal Kolmogorov-Arnold Transformer (TKAT), a novel attention-based architecture designed to address this task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by the Temporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder model tailored to handle tasks in which the observed part of the features is more important than the a priori known part. This new architecture combined the theoretical foundation of the Kolmogorov-Arnold representation with the power of transformers. TKAT aims to simplify the complex dependencies inherent in time series, making them more "interpretable". The use of transformer architecture in this framework allows us to capture long-range dependencies through self-attention mechanisms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02486">https://arxiv.org/abs/2406.02486</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 5</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant as it proposes a new transformer-like model, the Temporal Kolmogorov-Arnold Transformer (TKAT), for time-series forecasting. It employs attention mechanisms to capture complex temporal patterns and relationships within multivariate data streams. This aligns well with your interest in new deep learning methods and transformer-like models for time series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02578" target="_blank">Pretrained Mobility Transformer: A Foundation Model for Human Mobility</a></h3>
            <a href="https://arxiv.org/html/2406.02578v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02578v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinhua Wu, Haoyu He, Yanchao Wang, Qi Wang</p>
            <p><strong>Summary:</strong> arXiv:2406.02578v1 Announce Type: new 
Abstract: Ubiquitous mobile devices are generating vast amounts of location-based service data that reveal how individuals navigate and utilize urban spaces in detail. In this study, we utilize these extensive, unlabeled sequences of user trajectories to develop a foundation model for understanding urban space and human mobility. We introduce the \textbf{P}retrained \textbf{M}obility \textbf{T}ransformer (PMT), which leverages the transformer architecture to process user trajectories in an autoregressive manner, converting geographical areas into tokens and embedding spatial and temporal information within these representations. Experiments conducted in three U.S. metropolitan areas over a two-month period demonstrate PMT's ability to capture underlying geographic and socio-demographic characteristics of regions. The proposed PMT excels across various downstream tasks, including next-location prediction, trajectory imputation, and trajectory generation. These results support PMT's capability and effectiveness in decoding complex patterns of human mobility, offering new insights into urban spatial functionality and individual mobility preferences.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02578">https://arxiv.org/abs/2406.02578</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it introduces a new transformer-based foundation model (PMT) for processing time-series data, specifically user trajectories in urban spaces, which aligns with your interest in new transformer-like models and foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02587" target="_blank">Capturing Climatic Variability: Using Deep Learning for Stochastic Downscaling</a></h3>
            <a href="https://arxiv.org/html/2406.02587v1/extracted/5633834/figures/Gan_Architecture.png" target="_blank"><img src="https://arxiv.org/html/2406.02587v1/extracted/5633834/figures/Gan_Architecture.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kiri Daust, Adam Monahan</p>
            <p><strong>Summary:</strong> arXiv:2406.02587v1 Announce Type: new 
Abstract: Adapting to the changing climate requires accurate local climate information, a computationally challenging problem. Recent studies have used Generative Adversarial Networks (GANs), a type of deep learning, to learn complex distributions and downscale climate variables efficiently. Capturing variability while downscaling is crucial for estimating uncertainty and characterising extreme events - critical information for climate adaptation. Since downscaling is an undetermined problem, many fine-scale states are physically consistent with the coarse-resolution state. To quantify this ill-posed problem, downscaling techniques should be stochastic, able to sample realisations from a high-resolution distribution conditioned on low-resolution input. Previous stochastic downscaling attempts have found substantial underdispersion, with models failing to represent the full distribution. We propose approaches to improve the stochastic calibration of GANs in three ways: a) injecting noise inside the network, b) adjusting the training process to explicitly account for the stochasticity, and c) using a probabilistic loss metric. We tested our models first on a synthetic dataset with known distributional properties, and then on a realistic downscaling scenario, predicting high-resolution wind components from low-resolution climate covariates. Injecting noise, on its own, substantially improved the quality of conditional and full distributions in tests with synthetic data, but performed less well for wind field downscaling, where models remained underdispersed. For wind downscaling, we found that adjusting the training method and including the probabilistic loss improved calibration. The best model, with all three changes, showed much improved skill at capturing the full variability of the high-resolution distribution and thus at characterising extremes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02587">https://arxiv.org/abs/2406.02587</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper might be relevant to your interests as it involves the use of deep learning (specifically, Generative Adversarial Networks) for forecasting climate variables - a type of time-series. However, it doesn't specifically address the development of new DL methods or foundations for time series, but rather, application of existing methodologies, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02614" target="_blank">Frequency Enhanced Pre-training for Cross-city Few-shot Traffic Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02614v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02614v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhanyu Liu, Jianrong Ding, Guanjie Zheng</p>
            <p><strong>Summary:</strong> arXiv:2406.02614v1 Announce Type: new 
Abstract: The field of Intelligent Transportation Systems (ITS) relies on accurate traffic forecasting to enable various downstream applications. However, developing cities often face challenges in collecting sufficient training traffic data due to limited resources and outdated infrastructure. Recognizing this obstacle, the concept of cross-city few-shot forecasting has emerged as a viable approach. While previous cross-city few-shot forecasting methods ignore the frequency similarity between cities, we have made an observation that the traffic data is more similar in the frequency domain between cities. Based on this fact, we propose a \textbf{F}requency \textbf{E}nhanced \textbf{P}re-training Framework for \textbf{Cross}-city Few-shot Forecasting (\textbf{FEPCross}). FEPCross has a pre-training stage and a fine-tuning stage. In the pre-training stage, we propose a novel Cross-Domain Spatial-Temporal Encoder that incorporates the information of the time and frequency domain and trains it with self-supervised tasks encompassing reconstruction and contrastive objectives. In the fine-tuning stage, we design modules to enrich training samples and maintain a momentum-updated graph structure, thereby mitigating the risk of overfitting to the few-shot training data. Empirical evaluations performed on real-world traffic datasets validate the exceptional efficacy of FEPCross, outperforming existing approaches of diverse categories and demonstrating characteristics that foster the progress of cross-city few-shot forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02614">https://arxiv.org/abs/2406.02614</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> Even though it is not explicitly related to deep learning, this paper is relevant to your interests because it proposes a new methodology (FEPCross) for time series forecasting. It also addresses the problem of limited resources for data collection, which relates to your interest in new datasets for training foundation models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02635" target="_blank">Evidentially Calibrated Source-Free Time-Series Domain Adaptation with Temporal Imputation</a></h3>
            <a href="https://arxiv.org/html/2406.02635v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02635v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peiliang Gong, Mohamed Ragab, Emadeldeen Eldele, Wenyu Zhang, Min Wu, Chuan-Sheng Foo, Daoqiang Zhang, Xiaoli Li, Zhenghua Chen</p>
            <p><strong>Summary:</strong> arXiv:2406.02635v1 Announce Type: new 
Abstract: Source-free domain adaptation (SFDA) aims to adapt a model pre-trained on a labeled source domain to an unlabeled target domain without access to source data, preserving the source domain's privacy. While SFDA is prevalent in computer vision, it remains largely unexplored in time series analysis. Existing SFDA methods, designed for visual data, struggle to capture the inherent temporal dynamics of time series, hindering adaptation performance. This paper proposes MAsk And imPUte (MAPU), a novel and effective approach for time series SFDA. MAPU addresses the critical challenge of temporal consistency by introducing a novel temporal imputation task. This task involves randomly masking time series signals and leveraging a dedicated temporal imputer to recover the original signal within the learned embedding space, bypassing the complexities of noisy raw data. Notably, MAPU is the first method to explicitly address temporal consistency in the context of time series SFDA. Additionally, it offers seamless integration with existing SFDA methods, providing greater flexibility. We further introduce E-MAPU, which incorporates evidential uncertainty estimation to address the overconfidence issue inherent in softmax predictions. To achieve that, we leverage evidential deep learning to obtain a better-calibrated pre-trained model and adapt the target encoder to map out-of-support target samples to a new feature representation closer to the source domain's support. This fosters better alignment, ultimately enhancing adaptation performance. Extensive experiments on five real-world time series datasets demonstrate that both MAPU and E-MAPU achieve significant performance gains compared to existing methods. These results highlight the effectiveness of our proposed approaches for tackling various time series domain adaptation problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02635">https://arxiv.org/abs/2406.02635</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in the 'Time Series and Deep Learning' category because it presents a new approach (MAPU and E-MAPU) for time series Source-Free Domain Adaptation (SFDA). However, it does not explicitly discuss foundation models, multimodal models, or transformer-like models for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02726" target="_blank">Temporal Graph Learning Recurrent Neural Network for Traffic Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02726v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02726v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sanghyun Lee, Chanyoung Park</p>
            <p><strong>Summary:</strong> arXiv:2406.02726v1 Announce Type: new 
Abstract: Accurate traffic flow forecasting is a crucial research topic in transportation management. However, it is a challenging problem due to rapidly changing traffic conditions, high nonlinearity of traffic flow, and complex spatial and temporal correlations of road networks. Most existing studies either try to capture the spatial dependencies between roads using the same semantic graph over different time steps, or assume all sensors on the roads are equally likely to be connected regardless of the distance between them. However, we observe that the spatial dependencies between roads indeed change over time, and two distant roads are not likely to be helpful to each other when predicting the traffic flow, both of which limit the performance of existing studies. In this paper, we propose Temporal Graph Learning Recurrent Neural Network (TGLRN) to address these problems. More precisely, to effectively model the nature of time series, we leverage Recurrent Neural Networks (RNNs) to dynamically construct a graph at each time step, thereby capturing the time-evolving spatial dependencies between roads (i.e., microscopic view). Simultaneously, we provide the Adaptive Structure Information to the model, ensuring that close and consecutive sensors are considered to be more important for predicting the traffic flow (i.e., macroscopic view). Furthermore, to endow TGLRN with robustness, we introduce an edge sampling strategy when constructing the graph at each time step, which eventually leads to further improvements on the model performance. Experimental results on four commonly used real-world benchmark datasets show the effectiveness of TGLRN.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02726">https://arxiv.org/abs/2406.02726</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new model, Temporal Graph Learning Recurrent Neural Network, to enhance traffic flow forecasting. It might be interesting for your research on new deep learning methods for time series and forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02770" target="_blank">Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models</a></h3>
            <a href="https://arxiv.org/html/2406.02770v1/extracted/5643099/EncDecArchitecture.jpg" target="_blank"><img src="https://arxiv.org/html/2406.02770v1/extracted/5643099/EncDecArchitecture.jpg" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kathrin Donandt, Karim B\"ottger, Dirk S\"offker</p>
            <p><strong>Summary:</strong> arXiv:2406.02770v1 Announce Type: new 
Abstract: Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02770">https://arxiv.org/abs/2406.02770</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper uses deep learning models, specifically encoder-decoder architectures (including a transformer encoder-decoder), to forecast inland vessel trajectories. While it is more of an application rather than a proposal of a new method, it does partially address your interest in new transformer-like models for time series forecasting by applying a transformer-based encoder-decoder model to a specific problem. However, it isn't focused on multimodal methods or foundation models for time series analysis, so it may not fully align with all your subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02827" target="_blank">Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting</a></h3>
            <a href="https://arxiv.org/html/2406.02827v1/extracted/5644480/stochdiff.png" target="_blank"><img src="https://arxiv.org/html/2406.02827v1/extracted/5644480/stochdiff.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuansan Liu, Sudanthi Wijewickrema, Dongting Hu, Christofer Bester, Stephen O'Leary, James Bailey</p>
            <p><strong>Summary:</strong> arXiv:2406.02827v1 Announce Type: new 
Abstract: Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02827">https://arxiv.org/abs/2406.02827</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it proposes a new Stochastic Diffusion model for time series forecasting, which aligns with your interest in new deep learning methods for time series. However, since it does not discuss foundation models or transformer-like models for time series, I've given it a score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02867" target="_blank">Oscillations enhance time-series prediction in reservoir computing with feedback</a></h3>
            <a href="https://arxiv.org/html/2406.02867v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02867v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuji Kawai, Takashi Morita, Jihoon Park, Minoru Asada</p>
            <p><strong>Summary:</strong> arXiv:2406.02867v1 Announce Type: new 
Abstract: Reservoir computing, a machine learning framework used for modeling the brain, can predict temporal data with little observations and minimal computational resources. However, it is difficult to accurately reproduce the long-term target time series because the reservoir system becomes unstable. This predictive capability is required for a wide variety of time-series processing, including predictions of motor timing and chaotic dynamical systems. This study proposes oscillation-driven reservoir computing (ODRC) with feedback, where oscillatory signals are fed into a reservoir network to stabilize the network activity and induce complex reservoir dynamics. The ODRC can reproduce long-term target time series more accurately than conventional reservoir computing methods in a motor timing and chaotic time-series prediction tasks. Furthermore, it generates a time series similar to the target in the unexperienced period, that is, it can learn the abstract generative rules from limited observations. Given these significant improvements made by the simple and computationally inexpensive implementation, the ODRC would serve as a practical model of various time series data. Moreover, we will discuss biological implications of the ODRC, considering it as a model of neural oscillations and their cerebellar processors.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02867">https://arxiv.org/abs/2406.02867</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it proposes a new technique (oscillation-driven reservoir computing) for long-term time series prediction, which is an aspect of deep learning and time series you're interested in.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03140" target="_blank">Continual Traffic Forecasting via Mixture of Experts</a></h3>
            <a href="https://arxiv.org/html/2406.03140v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.03140v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sanghyun Lee, Chanyoung Park</p>
            <p><strong>Summary:</strong> arXiv:2406.03140v1 Announce Type: new 
Abstract: The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time. Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient. To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (TFMoE) for traffic forecasting under evolving networks. The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group. This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting. Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of TFMoE. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03140">https://arxiv.org/abs/2406.03140</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This research paper presents a novel method for traffic forecasting that seems to fall under 'New deep learning methods for time series'. Though it does not explicitly mention multimodal learning or transformers, it still presents a new perspective on time-series forecasting.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03157" target="_blank">A Combination Model Based on Sequential General Variational Mode Decomposition Method for Time Series Prediction</a></h3>
            <a href="https://arxiv.org/html/2406.03157v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.03157v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wei Chen, Yuanyuan Yang, Jianyu Liu</p>
            <p><strong>Summary:</strong> arXiv:2406.03157v1 Announce Type: cross 
Abstract: Accurate prediction of financial time series is a key concern for market economy makers and investors. The article selects online store sales and Australian beer sales as representatives of non-stationary, trending, and seasonal financial time series, and constructs a new SGVMD-ARIMA combination model in a non-linear combination way to predict financial time series. The ARIMA model, LSTM model, and other classic decomposition prediction models are used as control models to compare the accuracy of different models. The empirical results indicate that the constructed combination prediction model has universal advantages over the single prediction model and linear combination prediction model of the control group. Within the prediction interval, our proposed combination model has improved advantages over traditional decomposition prediction control group models.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03157">https://arxiv.org/abs/2406.03157</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is quite relevant to your interests as it details a new method for time series prediction. It doesn't explicitly mention deep learning or transformer-like models, but it does present a novel combination model for forecasting financial time series, which aligns closely with your interest in new methods for time series.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.11888" target="_blank">Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues</a></h3>
            <a href="https://arxiv.org/html/2307.11888v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2307.11888v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</p>
            <p><strong>Summary:</strong> arXiv:2307.11888v3 Announce Type: replace 
Abstract: Deep neural networks based on linear RNNs interleaved with position-wise MLPs are gaining traction as competitive approaches for sequence modeling. Examples of such architectures include state-space models (SSMs) like S4, LRU, and Mamba: recently proposed models that achieve promising performance on text, genetics, and other data that require long-range reasoning. Despite experimental evidence highlighting these architectures' effectiveness and computational efficiency, their expressive power remains relatively unexplored, especially in connection to specific choices crucial in practice - e.g., carefully designed initialization distribution and potential use of complex numbers. In this paper, we show that combining MLPs with both real or complex linear diagonal recurrences leads to arbitrarily precise approximation of regular causal sequence-to-sequence maps. At the heart of our proof, we rely on a separation of concerns: the linear RNN provides a lossless encoding of the input sequence, and the MLP performs non-linear processing on this encoding. While we show that real diagonal linear recurrences are enough to achieve universality in this architecture, we prove that employing complex eigenvalues near unit disk - i.e., empirically the most successful strategy in S4 - greatly helps the RNN in storing information. We connect this finding with the vanishing gradient issue and provide experiments supporting our claims.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.11888">https://arxiv.org/abs/2307.11888</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is in line with your interest in time series and deep learning. It discusses deep neural networks based on linear RNNs interleaved with position-wise MLPs, which can be considered as new deep learning methods for time series. Even though it does not explicitly mention time series forecasting, the methodologies can potentially be applied in that domain.</p>
        </div>
        </div><h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02584" target="_blank">Planetary Causal Inference: Implications for the Geography of Poverty</a></h3>
            <a href="https://arxiv.org/html/2406.02584v1/extracted/5632803/PubYearProjected.png" target="_blank"><img src="https://arxiv.org/html/2406.02584v1/extracted/5632803/PubYearProjected.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</p>
            <p><strong>Summary:</strong> arXiv:2406.02584v1 Announce Type: new 
Abstract: Earth observation data such as satellite imagery can, when combined with machine learning, have profound impacts on our understanding of the geography of poverty through the prediction of living conditions, especially where government-derived economic indicators are either unavailable or potentially untrustworthy. Recent work has progressed in using EO data not only to predict spatial economic outcomes, but also to explore cause and effect, an understanding which is critical for downstream policy analysis. In this review, we first document the growth of interest in EO-ML analyses in the causal space. We then trace the relationship between spatial statistics and EO-ML methods before discussing the four ways in which EO data has been used in causal ML pipelines -- (1.) poverty outcome imputation for downstream causal analysis, (2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and (4.) EO-based transportability analysis. We conclude by providing a workflow for how researchers can incorporate EO data in causal ML analysis going forward.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02584">https://arxiv.org/abs/2406.02584</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it involves the concept of causal ML pipelines and exploring cause and effect, aligned with your interests on Causal representation learning and Causal discovery. Also, it covers the use of machine learning which fits the overall scope of your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03212" target="_blank">Inferring the time-varying coupling of dynamical systems with temporal convolutional autoencoders</a></h3>
            <a href="https://arxiv.org/html/2406.03212v1/extracted/5644235/TCNI_schematic.png" target="_blank"><img src="https://arxiv.org/html/2406.03212v1/extracted/5644235/TCNI_schematic.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Josuan Calderon, Gordon J. Berman</p>
            <p><strong>Summary:</strong> arXiv:2406.03212v1 Announce Type: new 
Abstract: Most approaches for assessing causality in complex dynamical systems fail when the interactions between variables are inherently non-linear and non-stationary. Here we introduce Temporal Autoencoders for Causal Inference (TACI), a methodology that combines a new surrogate data metric for assessing causal interactions with a novel two-headed machine learning architecture to identify and measure the direction and strength of time-varying causal interactions. Through tests on both synthetic and real-world datasets, we demonstrate TACI's ability to accurately quantify dynamic causal interactions across a variety of systems. Our findings display the method's effectiveness compared to existing approaches and also highlight our approach's potential to build a deeper understanding of the mechanisms that underlie time-varying interactions in physical and biological systems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03212">https://arxiv.org/abs/2406.03212</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in causal discovery as it discusses a methodology for assessing causal interactions in dynamical systems through the use of Temporal Autoencoders (TACI). While it does involve time series data and an autoencoder, which is a type of deep learning model, its main contribution is in the realm of causality, specifically causal discovery.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03234" target="_blank">Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning</a></h3>
            
            <p><strong>Authors:</strong> Inwoo Hwang, Yunhyeok Kwak, Suhyung Choi, Byoung-Tak Zhang, Sanghack Lee</p>
            <p><strong>Summary:</strong> arXiv:2406.03234v1 Announce Type: new 
Abstract: Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03234">https://arxiv.org/abs/2406.03234</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests as it deals with 'Causal dynamics learning', one of the subtopics under your causality interest. It proposes a novel method for inferencing fine-grained causal structures in the context of reinforcement learning, upon which some of your interests are based.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02873" target="_blank">Prediction-powered Generalization of Causal Inferences</a></h3>
            <a href="https://arxiv.org/html/2406.02873v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02873v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ilker Demirel, Ahmed Alaa, Anthony Philippakis, David Sontag</p>
            <p><strong>Summary:</strong> arXiv:2406.02873v1 Announce Type: cross 
Abstract: Causal inferences from a randomized controlled trial (RCT) may not pertain to a target population where some effect modifiers have a different distribution. Prior work studies generalizing the results of a trial to a target population with no outcome but covariate data available. We show how the limited size of trials makes generalization a statistically infeasible task, as it requires estimating complex nuisance functions. We develop generalization algorithms that supplement the trial data with a prediction model learned from an additional observational study (OS), without making any assumptions on the OS. We theoretically and empirically show that our methods facilitate better generalization when the OS is high-quality, and remain robust when it is not, and e.g., have unmeasured confounding.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02873">https://arxiv.org/abs/2406.02873</a></p>
            <p><strong>Category:</strong> stat.ML</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is a good fit because it revolves around causal inferences and their generalization, which aligns with your interest in causality and machine learning. While it may not be exclusively about methods, it offers relevant insights for your research topic.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.00382" target="_blank">Calibrated and Conformal Propensity Scores for Causal Effect Estimation</a></h3>
            <a href="https://arxiv.org/html/2306.00382v2/extracted/5644349/images/compare_hist_clipped1.png" target="_blank"><img src="https://arxiv.org/html/2306.00382v2/extracted/5644349/images/compare_hist_clipped1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shachi Deshpande, Volodymyr Kuleshov</p>
            <p><strong>Summary:</strong> arXiv:2306.00382v2 Announce Type: replace-cross 
Abstract: Propensity scores are commonly used to estimate treatment effects from observational data. We argue that the probabilistic output of a learned propensity score model should be calibrated -- i.e., a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group -- and we propose simple recalibration techniques to ensure this property. We prove that calibration is a necessary condition for unbiased treatment effect estimation when using popular inverse propensity weighted and doubly robust estimators. We derive error bounds on causal effect estimates that directly relate to the quality of uncertainties provided by the probabilistic propensity score model and show that calibration strictly improves this error bound while also avoiding extreme propensity weights. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional image covariates and genome-wide association studies (GWASs). Calibrated propensity scores improve the speed of GWAS analysis by more than two-fold by enabling the use of simpler models that are faster to train.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.00382">https://arxiv.org/abs/2306.00382</a></p>
            <p><strong>Category:</strong> stat.ME</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses a new approach to propensity score estimation, which is an important technique in observational-data-driven causal effect analysis. The paper discusses the new calibration techniques which ensure better estimation of treatment effects and speeds up the analysis process in high dimensional tasks which is relevant to your interest in 'Causal discovery'.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.00139" target="_blank">Is Knowledge All Large Language Models Needed for Causal Reasoning?</a></h3>
            <a href="https://arxiv.org/html/2401.00139v2/extracted/5645175/fig/attribution2-icml.png" target="_blank"><img src="https://arxiv.org/html/2401.00139v2/extracted/5645175/fig/attribution2-icml.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hengrui Cai, Shengjie Liu, Rui Song</p>
            <p><strong>Summary:</strong> arXiv:2401.00139v2 Announce Type: replace-cross 
Abstract: This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.00139">https://arxiv.org/abs/2401.00139</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests in causality and machine learning because it proposes a novel causal attribution model for large language models and discusses their ability in causal discovery. The missing point for a full score is its focus on using numerical data, while you indicated a preference for 'using large language models in causal discovery'.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02592" target="_blank">LOLAMEME: Logic, Language, Memory, Mechanistic Framework</a></h3>
            <a href="https://arxiv.org/html/2406.02592v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02592v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jay Desai, Xiaobo Guo, Srinivasan H. Sengamedu</p>
            <p><strong>Summary:</strong> arXiv:2406.02592v1 Announce Type: new 
Abstract: The performance of Large Language Models has achieved superhuman breadth with unprecedented depth. At the same time, the language models are mostly black box models and the underlying mechanisms for performance have been evaluated using synthetic or mechanistic schemes. We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure. The proposed framework is called LOLAMEME and we provide two instantiations of LOLAMEME: LoLa and MeMe languages. We then consider two generative language model architectures: transformer-based GPT-2 and convolution-based Hyena. We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02592">https://arxiv.org/abs/2406.02592</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it delves into the performance and mechanisms of Large Language Models which aligns with your interest in using large language models for various purposes. While it does not explicitly mention controlling software or web browsers, the proposed Logic, Language, Memory, Mechanistic Framework (LOLAMEME) could potentially be useful towards these applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02611" target="_blank">LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments</a></h3>
            <a href="https://arxiv.org/html/2406.02611v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02611v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zikun Ye, Hema Yoganarasimhan, Yufeng Zheng</p>
            <p><strong>Summary:</strong> arXiv:2406.02611v1 Announce Type: new 
Abstract: In the rapidly evolving digital content landscape, media firms and news publishers require automated and efficient methods to enhance user engagement. This paper introduces the LLM-Assisted Online Learning Algorithm (LOLA), a novel framework that integrates Large Language Models (LLMs) with adaptive experimentation to optimize content delivery. Leveraging a large-scale dataset from Upworthy, which includes 17,681 headline A/B tests aimed at evaluating the performance of various headlines associated with the same article content, we first investigate three broad pure-LLM approaches: prompt-based methods, embedding-based classification models, and fine-tuned open-source LLMs. Our findings indicate that prompt-based approaches perform poorly, achieving no more than 65% accuracy in identifying the catchier headline among two options. In contrast, OpenAI-embedding-based classification models and fine-tuned Llama-3-8b models achieve comparable accuracy, around 82-84%, though still falling short of the performance of experimentation with sufficient traffic. We then introduce LOLA, which combines the best pure-LLM approach with the Upper Confidence Bound algorithm to adaptively allocate traffic and maximize clicks. Our numerical experiments on Upworthy data show that LOLA outperforms the standard A/B testing method (the current status quo at Upworthy), pure bandit algorithms, and pure-LLM approaches, particularly in scenarios with limited experimental traffic or numerous arms. Our approach is both scalable and broadly applicable to content experiments across a variety of digital settings where firms seek to optimize user engagement, including digital advertising and social media recommendations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02611">https://arxiv.org/abs/2406.02611</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces the LLM-Assisted Online Learning Algorithm (LOLA), which facilitates automation in delivering online content. Even though it does not directly correspond to software or web browser control, the methodology could be adapted for such purposes with Large Language Models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02613" target="_blank">ACCO: Accumulate while you Communicate, Hiding Communications in Distributed LLM Training</a></h3>
            
            <p><strong>Authors:</strong> Adel Nabli (MLIA, Mila), Louis Fournier (MLIA), Pierre Erbacher (MLIA), Louis Serrano (MLIA), Eugene Belilovsky (Mila), Edouard Oyallon</p>
            <p><strong>Summary:</strong> arXiv:2406.02613v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) relies heavily on distributed implementations, employing multiple GPUs to compute stochastic gradients on model replicas in parallel. However, synchronizing gradients in data parallel settings induces a communication overhead increasing with the number of distributed workers, which can impede the efficiency gains of parallelization. To address this challenge, optimization algorithms reducing inter-worker communication have emerged, such as local optimization methods used in Federated Learning. While effective in minimizing communication overhead, these methods incur significant memory costs, hindering scalability: in addition to extra momentum variables, if communications are only allowed between multiple local optimization steps, then the optimizer's states cannot be sharded among workers. In response, we propose $\textbf{AC}$cumulate while $\textbf{CO}$mmunicate ($\texttt{ACCO}$), a memory-efficient optimization algorithm tailored for distributed training of LLMs. $\texttt{ACCO}$ allows to shard optimizer states across workers, overlaps gradient computations and communications to conceal communication costs, and accommodates heterogeneous hardware. Our method relies on a novel technique to mitigate the one-step delay inherent in parallel execution of gradient computations and communications, eliminating the need for warmup steps and aligning with the training dynamics of standard distributed optimization while converging faster in terms of wall-clock time. We demonstrate the effectiveness of $\texttt{ACCO}$ on several LLMs training and fine-tuning tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02613">https://arxiv.org/abs/2406.02613</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents ACCO, a new method for optimizing the distributed training of large language models which directly falls under your interest in agents based on large-language models. Even though it doesn't focus on controlling software or web browsers, it discusses the technical aspects of large language model training which could be valuable in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02616" target="_blank">Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach</a></h3>
            <a href="https://arxiv.org/html/2406.02616v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02616v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, Honggang Zhang</p>
            <p><strong>Summary:</strong> arXiv:2406.02616v1 Announce Type: new 
Abstract: Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency. Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02616">https://arxiv.org/abs/2406.02616</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your research interest in large language models and their use in controlling systems. It specifically discusses optimizing deployment of large language models in edge computing environments. Although it doesn't directly address control of software or web browsers, its insights into more effective model deployment could be beneficial to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02764" target="_blank">Adaptive Preference Scaling for Reinforcement Learning with Human Feedback</a></h3>
            <a href="https://arxiv.org/html/2406.02764v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02764v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ilgee Hong, Zichong Li, Alexander Bukharin, Yixiao Li, Haoming Jiang, Tianbao Yang, Tuo Zhao</p>
            <p><strong>Summary:</strong> arXiv:2406.02764v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02764">https://arxiv.org/abs/2406.02764</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is about using preference data to improve policy performance in tasks like natural language generation with large language models. While it doesn't specifically mention control in software or web browsers, the principles discussed could potentially be adapted for those purposes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02847" target="_blank">Exact Conversion of In-Context Learning to Model Weights</a></h3>
            <a href="https://arxiv.org/html/2406.02847v1/extracted/5644623/figures/training.png" target="_blank"><img src="https://arxiv.org/html/2406.02847v1/extracted/5644623/figures/training.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi</p>
            <p><strong>Summary:</strong> arXiv:2406.02847v1 Announce Type: new 
Abstract: In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02847">https://arxiv.org/abs/2406.02847</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper could fall under your interest in agents based on large-language models because it discusses In-Context Learning of large language models and a new approach to implementing this learning.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02900" target="_blank">Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</a></h3>
            <a href="https://arxiv.org/html/2406.02900v1/extracted/5644857/figs/dpo_scaling_law_curve_fits_final_kl.png" target="_blank"><img src="https://arxiv.org/html/2406.02900v1/extracted/5644857/figs/dpo_scaling_law_curve_fits_final_kl.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum</p>
            <p><strong>Summary:</strong> arXiv:2406.02900v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is \emph{reward over-optimization} or \emph{reward hacking}, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02900">https://arxiv.org/abs/2406.02900</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This research paper falls under your interest in large-language models controlling agents as it discusses the use of Large Language Models (LLMs) in Reinforcement Learning from Human Feedback (RLHF) systems. However, it doesn't directly deal with control of specific applications like software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02913" target="_blank">Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity</a></h3>
            <a href="https://arxiv.org/html/2406.02913v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02913v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu</p>
            <p><strong>Summary:</strong> arXiv:2406.02913v1 Announce Type: new 
Abstract: Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language Models using only forward passes. However, the application of ZO fine-tuning in memory-constrained settings such as mobile phones and laptops is still challenging since full precision forward passes are infeasible. In this study, we address this limitation by integrating sparsity and quantization into ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of fine-tuning an extremely small subset of LLM parameters using ZO. This approach allows the majority of un-tuned parameters to be quantized to accommodate the constraint of limited device memory. Our findings reveal that the pre-training process can identify a set of "sensitive parameters" that can guide the ZO fine-tuning of LLMs on downstream tasks. Our results demonstrate that fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the full ZO fine-tuning performance, while offering wall-clock time speedup. Additionally, we show that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8 GiB of memory and notably reduced latency.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02913">https://arxiv.org/abs/2406.02913</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models. It presents a technique for fine-tuning large language models in resource-constrained settings, which aligns with your interest in using large language models for tasks like software control and automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02958" target="_blank">PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs</a></h3>
            <a href="https://arxiv.org/html/2406.02958v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02958v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar</p>
            <p><strong>Summary:</strong> arXiv:2406.02958v1 Announce Type: new 
Abstract: On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\epsilon=1.29$, $\epsilon=7.58$). We achieve these results while using 9$\times$ fewer rounds, 6$\times$ less client computation per round, and 100$\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02958">https://arxiv.org/abs/2406.02958</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper, 'PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs',  proposes a new method for training large language models, which could be useful for controlling software or computer automation as indicated in your interests. However, it does not specifically focus on control applications, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02969" target="_blank">Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.02969v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02969v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Jacob-Junqi Tian, John Willes, Blanka Horvath, Frank Rudzicz</p>
            <p><strong>Summary:</strong> arXiv:2406.02969v1 Announce Type: new 
Abstract: We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step. Our mechanism leverages the conditional information in each expert's running performance to forecast the best combination of LLMs for predicting the time series in its next step. Diverging from static (learned) Mixture of Experts (MoE) methods, MoE-F employs time-adaptive stochastic filtering techniques to combine experts. By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters corresponding to each of the $N$ individual LLMs. Each filter proposes its best combination of LLMs, given the information that they have access to. Subsequently, the $N$ filter outputs are aggregated to optimize a lower bound for the loss of the aggregated LLMs, which can be optimized in closed-form, thus generating our ensemble predictor. Our contributions here are: (I) the MoE-F algorithm -- deployable as a plug-and-play filtering harness, (II) theoretical optimality guarantees of the proposed filtering-based gating algorithm, and (III) empirical evaluation and ablative results using state of the art foundational and MoE LLMs on a real-world Financial Market Movement task where MoE-F attains a remarkable 17% absolute and 48.5% relative F1 measure improvement over the next best performing individual LLM expert.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02969">https://arxiv.org/abs/2406.02969</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses a new model (MoE-F algorithm) that combines pre-trained Large Language Models (LLMs) for online time series predictions, which might be instrumental in controlling software or web browsers. Even though it is not explicitly stated, the potential application of this methodology aligns with your interest in large language models controlling softwares and web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03428" target="_blank">HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits</a></h3>
            <a href="https://arxiv.org/html/2406.03428v1/extracted/5646268/figures/tweets_anon.png" target="_blank"><img src="https://arxiv.org/html/2406.03428v1/extracted/5646268/figures/tweets_anon.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tim Franzmeyer, Aleksandar Shtedritski, Samuel Albanie, Philip Torr, Jo\~ao F. Henriques, Jakob N. Foerster</p>
            <p><strong>Summary:</strong> arXiv:2406.03428v1 Announce Type: new 
Abstract: Benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users. Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03428">https://arxiv.org/abs/2406.03428</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper will likely be of interest to you as it discusses the development and evaluation of large language models (LLMs) for real-world tasks, which aligns with your interest in agents based on large-language models. However, it does not specifically mention the control of software or web browsers using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03445" target="_blank">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></h3>
            <a href="https://arxiv.org/html/2406.03445v1/extracted/5646644/figures/language/skip/layer_accuracy_Language_Math.png" target="_blank"><img src="https://arxiv.org/html/2406.03445v1/extracted/5646644/figures/language/skip/layer_accuracy_Language_Math.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia</p>
            <p><strong>Summary:</strong> arXiv:2406.03445v1 Announce Type: new 
Abstract: Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03445">https://arxiv.org/abs/2406.03445</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper explores how large language models (LLMs) can be used for mathematical reasoning via Fourier features, which is related to your interest in using LLMs for software control and automation. Though it does not directly deal with controlling web browsers or software, it provides valuable insights on how LLMs behave and analyze tasks, which could be helpful in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03476" target="_blank">Does your data spark joy? Performance gains from domain upsampling at the end of training</a></h3>
            <a href="https://arxiv.org/html/2406.03476v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.03476v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, Jonathan Frankle</p>
            <p><strong>Summary:</strong> arXiv:2406.03476v1 Announce Type: new 
Abstract: Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03476">https://arxiv.org/abs/2406.03476</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper should be of interest as it discusses the use of pretraining datasets for large language models, which could be useful in creating highly efficient agents in software or web browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03482" target="_blank">QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead</a></h3>
            
            <p><strong>Authors:</strong> Amir Zandieh, Majid Daliri, Insu Han</p>
            <p><strong>Summary:</strong> arXiv:2406.03482v1 Announce Type: new 
Abstract: Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \url{https://github.com/amirzandieh/QJL}.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03482">https://arxiv.org/abs/2406.03482</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not cover use cases for large language models controlling software or browsers, it discusses a new method related to optimizing the memory usage of large language models, which contributes to the efficiency of models controlling software or web browsers. This efficiency could be a crucial consideration in building agent-based LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02577" target="_blank">Are PPO-ed Language Models Hackable?</a></h3>
            <a href="https://arxiv.org/html/2406.02577v1/extracted/5627887/figures/ppo_actor_crit.png" target="_blank"><img src="https://arxiv.org/html/2406.02577v1/extracted/5627887/figures/ppo_actor_crit.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Suraj Anand, David Getzen</p>
            <p><strong>Summary:</strong> arXiv:2406.02577v1 Announce Type: cross 
Abstract: Numerous algorithms have been proposed to $\textit{align}$ language models to remove undesirable behaviors. However, the challenges associated with a very large state space and creating a proper reward function often result in various jailbreaks. Our paper aims to examine this effect of reward in the controlled setting of positive sentiment language generation. Instead of online training of a reward model based on human feedback, we employ a statically learned sentiment classifier. We also consider a setting where our model's weights and activations are exposed to an end-user after training. We examine a pretrained GPT-2 through the lens of mechanistic interpretability before and after proximal policy optimization (PPO) has been applied to promote positive sentiment responses. Using these insights, we (1) attempt to "hack" the PPO-ed model to generate negative sentiment responses and (2) add a term to the reward function to try and alter `negative' weights.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02577">https://arxiv.org/abs/2406.02577</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may interest you as it touches upon large language models, specifically GPT-2, and looks at their alignment with human intentions. Although it doesn't directly mention software or browser control, it discusses the manipulation of such models which potentially can be applied in computer automation domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02756" target="_blank">Aligning Large Language Models via Fine-grained Supervision</a></h3>
            <a href="https://arxiv.org/html/2406.02756v1/extracted/5644243/FIG/rlhf4.png" target="_blank"><img src="https://arxiv.org/html/2406.02756v1/extracted/5644243/FIG/rlhf4.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Dehong Xu, Liang Qiu, Minseok Kim, Faisal Ladhak, Jaeyoung Do</p>
            <p><strong>Summary:</strong> arXiv:2406.02756v1 Announce Type: cross 
Abstract: Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can achieve up to an absolute improvement of $5.1\%$ in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02756">https://arxiv.org/abs/2406.02756</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper potentially aligns with your interest in 'Agents based on large-language models' as it discusses improving the alignment of large scale language models with user expectations through specific training methods. It may provide insights on controlling software or web browsers with large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02787" target="_blank">Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities</a></h3>
            <a href="https://arxiv.org/html/2406.02787v1/extracted/5644320/Figs/context_2.png" target="_blank"><img src="https://arxiv.org/html/2406.02787v1/extracted/5644320/Figs/context_2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, JinDong Wang, Yongfeng Zhang</p>
            <p><strong>Summary:</strong> arXiv:2406.02787v1 Announce Type: cross 
Abstract: This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential. The code and dataset are available at: https://github.com/agiresearch/ContextHub.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02787">https://arxiv.org/abs/2406.02787</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper delves into the reasoning capabilities of Large Language Models (LLMs), an area of your interest. Although it doesn't specifically discuss controlling software or web browsers, understanding the reasoning abilities of LLMs is beneficial for their application in automation and control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02888" target="_blank">HYDRA: Model Factorization Framework for Black-Box LLM Personalization</a></h3>
            <a href="https://arxiv.org/html/2406.02888v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02888v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yuchen Zhuang, Haotian Sun, Yue Yu, Qifan Wang, Chao Zhang, Bo Dai</p>
            <p><strong>Summary:</strong> arXiv:2406.02888v1 Announce Type: cross 
Abstract: Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at https://github.com/night-chen/HYDRA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02888">https://arxiv.org/abs/2406.02888</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models applied in user-centric scenarios, particularly in personalization, which can be considered a form of intelligent agent behavior. However, the paper doesn't directly address controlling software or web browsers, so it's not a perfect fit for your criteria.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.02959" target="_blank">Adversarial Moment-Matching Distillation of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.02959v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.02959v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chen Jia</p>
            <p><strong>Summary:</strong> arXiv:2406.02959v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions. Instead of optimizing these mandatory behaviour cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives. To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.02959">https://arxiv.org/abs/2406.02959</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses knowledge distillation of Large Language Models and the use of an imitation learning strategy, which might connect to your interest in using large language models for computer automation, but it does not directly address controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03007" target="_blank">BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents</a></h3>
            
            <p><strong>Authors:</strong> Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian</p>
            <p><strong>Summary:</strong> arXiv:2406.03007v1 Announce Type: cross 
Abstract: With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03007">https://arxiv.org/abs/2406.03007</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it discusses large language models and their applications in intelligent agents. However, instead of developing new ways to use these LLMs for control and automation, it focuses on potential vulnerabilities, making it slightly less relevant to your interests.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03030" target="_blank">From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation</a></h3>
            <a href="https://arxiv.org/html/2406.03030v1/extracted/5645309/fig/gen_hists.png" target="_blank"><img src="https://arxiv.org/html/2406.03030v1/extracted/5645309/fig/gen_hists.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ali Malik, Stephen Mayhew, Chris Piech, Klinton Bicknell</p>
            <p><strong>Summary:</strong> arXiv:2406.03030v1 Announce Type: cross 
Abstract: We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.
  Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03030">https://arxiv.org/abs/2406.03030</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is highly relevant to your third interest area, it deals with large language models (LLMs) and its control to generate text. However, it doesn't focus on controlling software or web browsers using LLMs. Hence, it's somewhat but not fully relevant.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03198" target="_blank">The Impossibility of Fair LLMs</a></h3>
            
            <p><strong>Authors:</strong> Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan</p>
            <p><strong>Summary:</strong> arXiv:2406.03198v1 Announce Type: cross 
Abstract: The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03198">https://arxiv.org/abs/2406.03198</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper delves into the workings and fairness concerns of Large Language Models (LLMs), a topic directly corresponding to your interest in agents based on LLMs. However, it seems to focus more on the fairness and ethical implications of LLMs rather on direct applications like control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03230" target="_blank">Defending Large Language Models Against Attacks With Residual Stream Activation Analysis</a></h3>
            
            <p><strong>Authors:</strong> Amelia Kawasaki, Andrew Davis, Houssam Abbas</p>
            <p><strong>Summary:</strong> arXiv:2406.03230v1 Announce Type: cross 
Abstract: The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply an established methodology for analyzing distinctive activation patterns in the residual streams for a novel result of attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03230">https://arxiv.org/abs/2406.03230</a></p>
            <p><strong>Category:</strong> cs.CR</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not directly address the usage scenarios you've provided, it nevertheless is relevant to your interest in large language models, particularly as agents, because it discusses techniques for enhancing security and resilience of these models. This could be instrumental in ensuring that they function accurately and effectively when used to control software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03441" target="_blank">Cycles of Thought: Measuring LLM Confidence through Stable Explanations</a></h3>
            <a href="https://arxiv.org/html/2406.03441v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.03441v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Evan Becker, Stefano Soatto</p>
            <p><strong>Summary:</strong> arXiv:2406.03441v1 Announce Type: cross 
Abstract: In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03441">https://arxiv.org/abs/2406.03441</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> While the paper does not directly propose a method for controlling software or browsers with Large Language Models, it does discuss an important aspect related to the implementation of such models – measuring their uncertainty. Understanding and quantifying uncertainty in LLMs can be an essential part of using them for automation and control tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.03496" target="_blank">Wings: Learning Multimodal LLMs without Text-only Forgetting</a></h3>
            <a href="https://arxiv.org/html/2406.03496v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.03496v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye</p>
            <p><strong>Summary:</strong> arXiv:2406.03496v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM. In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension. Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text. From that, we construct extra modules that act as the boosted learner to compensate for the attention shift. The complementary visual and textual learners, like "wings" on either side, are connected in parallel within each layer's attention block. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners. We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.03496">https://arxiv.org/abs/2406.03496</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper presents Wings, a novel Multimodal Large Language Model (LLM), and is centered around their application with text-only and visual commands. While it doesn't specifically cover time-series, causality or automation control, it may provide valuable insights into the workings of large language models, which is of interest to you. However, please note it doesn't exactly fit the context of controlling software or browsers, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.03285" target="_blank">S-LoRA: Serving Thousands of Concurrent LoRA Adapters</a></h3>
            <a href="https://arxiv.org/html/2311.03285v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.03285v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica</p>
            <p><strong>Summary:</strong> arXiv:2311.03285v3 Announce Type: replace 
Abstract: The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.03285">https://arxiv.org/abs/2311.03285</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper talks about the application of large language models in multi-task environments which aligns with your interest in agents powered by these models. However, it doesn't specifically touch on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.15983" target="_blank">SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification</a></h3>
            <a href="https://arxiv.org/html/2311.15983v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.15983v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Difan Jiao, Yilun Liu, Zhenwei Tang, Daniel Matter, J\"urgen Pfeffer, Ashton Anderson</p>
            <p><strong>Summary:</strong> arXiv:2311.15983v2 Announce Type: replace 
Abstract: Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. Current text classification paradigms, however, rely solely on the output of the final layer in the LLM, with the rich information contained in internal neurons largely untapped. In this study, we present SPIN: a model-agnostic framework that sparsifies and integrates internal neurons of intermediate layers of LLMs for text classification. Specifically, SPIN sparsifies internal neurons by linear probing-based salient neuron selection layer by layer, avoiding noise from unrelated neurons and ensuring efficiency. The cross-layer salient neurons are then integrated to serve as multi-layered features for the classification head. Extensive experimental results show our proposed SPIN significantly improves text classification accuracy, efficiency, and interpretability.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.15983">https://arxiv.org/abs/2311.15983</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models. It sheds light on a model-aggressive framework that integrates internal neurons of LLMs for text classification, though its focus might not directly be on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.06469" target="_blank">Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning</a></h3>
            <a href="https://arxiv.org/html/2401.06469v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.06469v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan</p>
            <p><strong>Summary:</strong> arXiv:2401.06469v3 Announce Type: replace 
Abstract: In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to the forward computation of a zero-shot query to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of ICL examples. In some cases, it even exceeds the performance of the best order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.06469">https://arxiv.org/abs/2401.06469</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper describes a new approach to in-context learning with large language models. Even though it doesn't touch directly on controlling software or web browsers, the developed method could potentially improve automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00396" target="_blank">Efficient Exploration for LLMs</a></h3>
            <a href="https://arxiv.org/html/2402.00396v2/extracted/5644077/data_efficiency_swap_axes.png" target="_blank"><img src="https://arxiv.org/html/2402.00396v2/extracted/5644077/data_efficiency_swap_axes.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</p>
            <p><strong>Summary:</strong> arXiv:2402.00396v2 Announce Type: replace 
Abstract: We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00396">https://arxiv.org/abs/2402.00396</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLM) as agents. It discusses the use of LLMs in an efficient exploration scenario, which can imply control capabilities. However, it might not directly address software control or web browser manipulation by LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.02347" target="_blank">Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models</a></h3>
            <a href="https://arxiv.org/html/2402.02347v3/extracted/5645131/vase_mainmain2.png" target="_blank"><img src="https://arxiv.org/html/2402.02347v3/extracted/5645131/vase_mainmain2.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Fangzhao Zhang, Mert Pilanci</p>
            <p><strong>Summary:</strong> arXiv:2402.02347v3 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning (PEFT) method, which proposes to freeze pretrained model weights and update an additive low-rank trainable matrix. In this work, we study the enhancement of LoRA training by introducing an $r \times r$ preconditioner in each gradient step where $r$ is the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes feature learning with LoRA under infinite-width NN setting. Empirically, the implementation of this new preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with this new preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. The new preconditioner can be derived from a novel Riemannian metric in low-rank matrix field. Code can be accessed at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.02347">https://arxiv.org/abs/2402.02347</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may be of interest because it discusses 'Low-Rank Adaptation (LoRA)', a parameter-efficient fine-tuning method used in large language models. While it doesn't specifically discuss controlling software or web browsers, the techniques to enhance the learning and adaptability of such models could be directly applicable.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10207" target="_blank">Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment</a></h3>
            <a href="https://arxiv.org/html/2402.10207v5/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10207v5/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</p>
            <p><strong>Summary:</strong> arXiv:2402.10207v5 Announce Type: replace 
Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10207">https://arxiv.org/abs/2402.10207</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest as it discusses aligning Large Language Models (LLMs) to accommodate diverse rewards. It offers a new method for fine-tuning of foundation models. Although it does not directly mention computer automation or controlling web browsers, it can still align with the more general topic of agents based on large-language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10500" target="_blank">Active Preference Optimization for Sample Efficient RLHF</a></h3>
            
            <p><strong>Authors:</strong> Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, Sayak Ray Chowdhury</p>
            <p><strong>Summary:</strong> arXiv:2402.10500v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. Although aligned generative models have shown remarkable abilities in various tasks, their reliance on high-quality human preference data creates a costly bottleneck in the practical application of RLHF. One primary reason is that current methods rely on uniformly picking prompt-generation pairs from a dataset of prompt-generations, to collect human feedback, resulting in sub-optimal alignment under a constrained budget, which highlights the criticality of adaptive strategies in efficient alignment. Recent works [Mehta et al., 2023, Muldrew et al., 2024] have tried to address this problem by designing various heuristics based on generation uncertainty. However, either the assumptions in [Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide any rigorous theoretical guarantee. To address these, we reformulate RLHF within contextual preference bandit framework, treating prompts as contexts, and develop an active-learning algorithm, $\textit{Active Preference Optimization}$ ($\texttt{APO}$), which enhances model alignment by querying preference data from the most important samples, achieving superior performance for small sample budget. We analyze the theoretical performance guarantees of $\texttt{APO}$ under the BTL preference model showing that the suboptimality gap of the policy learned via $\texttt{APO}$ scales as $O(1/\sqrt{T})$ for a budget of $T$. We also show that collecting preference data by choosing prompts randomly leads to a policy that suffers a constant sub-optimality. We perform detailed experimental evaluations on practical preference datasets to validate $\texttt{APO}$'s efficacy over the existing methods, establishing it as a sample-efficient and practical solution of alignment in a cost-effective and scalable manner.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10500">https://arxiv.org/abs/2402.10500</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The research paper proposes a new method, 'Active Preference Optimization', for aligning large language models with human preferences efficiently. Even though it doesn't directly discuss controlling software or web browsers, it's relevant to your interest in agent-based large-language models as it explores their optimized interaction with human feedback.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.10644" target="_blank">Linear Transformers with Learnable Kernel Functions are Better In-Context Models</a></h3>
            <a href="https://arxiv.org/html/2402.10644v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.10644v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov</p>
            <p><strong>Summary:</strong> arXiv:2402.10644v2 Announce Type: replace 
Abstract: Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.10644">https://arxiv.org/abs/2402.10644</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper appears to discuss improvements to language models which could be used as the basis for large-language model agents. However, it does not seem to focus directly on controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.12061" target="_blank">All Language Models Large and Small</a></h3>
            <a href="https://arxiv.org/html/2402.12061v2/extracted/5646294/Figures/scienceworld_graphic_1.png" target="_blank"><img src="https://arxiv.org/html/2402.12061v2/extracted/5646294/Figures/scienceworld_graphic_1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zhixun Chen, Yali Du, David Mguni</p>
            <p><strong>Summary:</strong> arXiv:2402.12061v2 Announce Type: replace 
Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs (i.e. LMs require less GPU usage, but may not be able to solve the problem alone) everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on LLM calls almost surely enabling it to solve various tasks while significantly lowering computational costs. We test LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs while reducing GPU usage by up to 30%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.12061">https://arxiv.org/abs/2402.12061</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper proposes a new method to control large language models with reinforcement learning, which relates to your interest in using large language models to control software.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15607" target="_blank">How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?</a></h3>
            
            <p><strong>Authors:</strong> Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, Pin-Yu Chen</p>
            <p><strong>Summary:</strong> arXiv:2402.15607v2 Announce Type: replace 
Abstract: Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors on the ICL generalization performance on the remaining unseen tasks with and without data distribution shifts. We also analyze how different components in the learned Transformers contribute to the ICL performance. Furthermore, we provide the first theoretical analysis of how model pruning affects ICL performance and prove that proper magnitude-based pruning can have a minimal impact on ICL while reducing inference costs. These theoretical findings are justified through numerical experiments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15607">https://arxiv.org/abs/2402.15607</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper dives deep into the internal workings of large language models and their performance dynamics, which would be important information for developing agents that utilize these models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.02419" target="_blank">Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems</a></h3>
            <a href="https://arxiv.org/html/2403.02419v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.02419v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou</p>
            <p><strong>Summary:</strong> arXiv:2403.02419v2 Announce Type: replace 
Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses. However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of LM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on "easy" queries, but lower performance on "hard" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of LM calls to make.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.02419">https://arxiv.org/abs/2403.02419</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the use of large language models in compound inference systems, which are similar to the notion of controlling software or automation. However, it does not directly address controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.08819" target="_blank">The Illusion of State in State-Space Models</a></h3>
            <a href="https://arxiv.org/html/2404.08819v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.08819v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William Merrill, Jackson Petty, Ashish Sabharwal</p>
            <p><strong>Summary:</strong> arXiv:2404.08819v2 Announce Type: replace 
Abstract: State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the "state" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.08819">https://arxiv.org/abs/2404.08819</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is very relevant to your interest in 'llm-agents' as it discusses the limitations of State-space models (SSMs) in large language models (LLMs), which is an area of interest for you. However, it does not seem to propose a new method, hence the score is 4 and not 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.17849" target="_blank">I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.17849v2/extracted/5646459/figures/activation.png" target="_blank"><img src="https://arxiv.org/html/2405.17849v2/extracted/5646459/figures/activation.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xing Hu, Yuan Cheng, Dawei Yang, Zhihang Yuan, Jiangyong Yu, Chen Xu, Sifan Zhou</p>
            <p><strong>Summary:</strong> arXiv:2405.17849v2 Announce Type: replace 
Abstract: Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs. We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.17849">https://arxiv.org/abs/2405.17849</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is related to your interest in large-language model agents, focusing on the topic of post-training quantization for LLMs and proposing a new integer-only fully-quantized PTQ framework. Although the paper doesn't directly discuss automation or control of software or web browsers, the methodology could potentially be used to improve the efficiency of such applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.19320" target="_blank">Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</a></h3>
            <a href="https://arxiv.org/html/2405.19320v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.19320v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai</p>
            <p><strong>Summary:</strong> arXiv:2405.19320v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.
  In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.19320">https://arxiv.org/abs/2405.19320</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is directly related to your interests in large language models and human preference. Although it is not specifically about controlling software or web browsers, the paper's focus on reinforcement learning from human feedback for aligning large language models could be highly relevant to your research area. As the paper provides a unified approach to RLHF, it could be beneficial in automating tasks using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.21018" target="_blank">Improved Techniques for Optimization-Based Jailbreaking on Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.21018v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.21018v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin</p>
            <p><strong>Summary:</strong> arXiv:2405.21018v2 Announce Type: replace 
Abstract: Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of "Sure" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate. The code is released at https://github.com/jiaxiaojunQAQ/I-GCG.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.21018">https://arxiv.org/abs/2405.21018</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your interest of 'Agents based on large-language models'. It explores optimization-based jailbreaking techniques for Large Language Models and has the potential to provide insights on how to control and use LLMs more efficiently.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01660" target="_blank">Self-Improving Robust Preference Optimization</a></h3>
            <a href="https://arxiv.org/html/2406.01660v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01660v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Eugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, Mohammad Gheshlaghi Azar</p>
            <p><strong>Summary:</strong> arXiv:2406.01660v2 Announce Type: replace 
Abstract: Both online and offline RLHF methods such as PPO and DPO have been extremely successful in aligning AI with human preferences. Despite their success, the existing methods suffer from a fundamental problem that their optimal solution is highly task-dependent (i.e., not robust to out-of-distribution (OOD) tasks). Here we address this challenge by proposing Self-Improving Robust Preference Optimization SRPO, a practical and mathematically principled offline RLHF framework that is completely robust to the changes in the task. The key idea of SRPO is to cast the problem of learning from human preferences as a self-improvement process, which can be mathematically expressed in terms of a min-max objective that aims at joint optimization of self-improvement policy and the generative policy in an adversarial fashion. The solution for this optimization problem is independent of the training task and thus it is robust to its changes. We then show that this objective can be re-expressed in the form of a non-adversarial offline loss which can be optimized using standard supervised optimization techniques at scale without any need for reward model and online inference. We show the effectiveness of SRPO in terms of AI Win-Rate (WR) against human (GOLD) completions. In particular, when SRPO is evaluated on the OOD XSUM dataset, it outperforms the celebrated DPO by a clear margin of 15% after 5 self-revisions, achieving WR of 90%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01660">https://arxiv.org/abs/2406.01660</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper seems relevant to your interests in Large Language Model (LLM)-based agents. It proposes a new framework, the Self-Improving Robust Preference Optimization (SRPO), which is used as a method of learning from human preferences in a robust way, which could be applied to control software or web browsers as per your interests. However, it doesn't directly mention applications in software automation or web control with LLMs, hence the score of 4 rather than 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2306.07629" target="_blank">SqueezeLLM: Dense-and-Sparse Quantization</a></h3>
            <a href="https://arxiv.org/html/2306.07629v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2306.07629v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt Keutzer</p>
            <p><strong>Summary:</strong> arXiv:2306.07629v4 Announce Type: replace-cross 
Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2306.07629">https://arxiv.org/abs/2306.07629</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant because it's about improving the performance of Large Language Models, which is crucial for creating agents based on them. However, it doesn't specifically discuss controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2309.08631" target="_blank">Large Language Models Can Infer Psychological Dispositions of Social Media Users</a></h3>
            <a href="https://arxiv.org/html/2309.08631v2/extracted/5646435/figures/fig_hist.png" target="_blank"><img src="https://arxiv.org/html/2309.08631v2/extracted/5646435/figures/fig_hist.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Heinrich Peters, Sandra Matz</p>
            <p><strong>Summary:</strong> arXiv:2309.08631v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks. In this paper, we investigate whether LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores - a level of accuracy that is similar to that of supervised machine learning models specifically trained to infer personality. Our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories: predictions were found to be more accurate for women and younger individuals on several traits, suggesting a potential bias stemming from the underlying training data or differences in online self-expression. The ability of LLMs to infer psychological dispositions from user-generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners. On the one hand, this democratization might facilitate large-scale research of high ecological validity and spark innovation in personalized services. On the other hand, it also raises ethical concerns regarding user privacy and self-determination, highlighting the need for stringent ethical frameworks and regulation.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2309.08631">https://arxiv.org/abs/2309.08631</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your research interest in large-language models (LLMs) given it explores the ability of LLMs in inferring psychological traits from text data. Although it doesn't explicitly discuss automation or control of software/web browsers, the demonstrated abilities of the LLMs have implications for customization and adaptivity in agent systems.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2310.11244" target="_blank">Entity Matching using Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2310.11244v3/extracted/5646488/figures/data/prompt.png" target="_blank"><img src="https://arxiv.org/html/2310.11244v3/extracted/5646488/figures/data/prompt.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ralph Peeters, Christian Bizer</p>
            <p><strong>Summary:</strong> arXiv:2310.11244v3 Announce Type: replace-cross 
Abstract: Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity and is a central step in most data integration pipelines. Many state-of-the-art entity matching methods rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. This paper investigates using generative large language models (LLMs) as a less task-specific training data-dependent and more robust alternative to PLM-based matchers. Our study covers hosted and open-source LLMs, which can be run locally. We evaluate these models in a zero-shot scenario and a scenario where task-specific training data is available. We compare different prompt designs and the prompt sensitivity of the models and show that there is no single best prompt but needs to be tuned for each model/dataset combination. We further investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning a hosted LLM using the same pool of training data. Our experiments show that the best LLMs require no or only a few training examples to perform similarly to PLMs that were fine-tuned using thousands of examples. LLM-based matchers further exhibit higher robustness to unseen entities. We show that GPT4 can generate structured explanations for matching decisions. The model can automatically identify potential causes of matching errors by analyzing explanations of wrong decisions. We demonstrate that the model can generate meaningful textual descriptions of the identified error classes, which can help data engineers improve entity matching pipelines.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2310.11244">https://arxiv.org/abs/2310.11244</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper introduces a method for using large language models in the field of entity matching, a task that requires controlling the software. Although it isn't specific to web browsers or targeting your subtopics directly, the concept could be transferable and is relevant to your interests in large language models and computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2312.17025" target="_blank">Experiential Co-Learning of Software-Developing Agents</a></h3>
            <a href="https://arxiv.org/html/2312.17025v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2312.17025v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, Maosong Sun</p>
            <p><strong>Summary:</strong> arXiv:2312.17025v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2312.17025">https://arxiv.org/abs/2312.17025</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses the use of large language models in software development. Although it does not directly mention using language models for controlling software or web browsers, it does explore a novel learning framework for improved performance in software development tasks. Therefore, it has ideas that can potentially be applied to your interests in computer automation using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.00588" target="_blank">Fairness in Serving Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2401.00588v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.00588v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph E. Gonzalez, Ion Stoica</p>
            <p><strong>Summary:</strong> arXiv:2401.00588v2 Announce Type: replace-cross 
Abstract: High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide range of requests from short chat conversations to long document reading. To ensure that all client requests are processed fairly, most major LLM inference services have request rate limits, to ensure that no client can dominate the request queue. However, this rudimentary notion of fairness also results in under-utilization of the resources and poor client experience when there is spare capacity. While there is a rich literature on fair scheduling, serving LLMs presents new challenges due to their unpredictable request lengths and their unique batching characteristics on parallel accelerators. This paper introduces the definition of LLM serving fairness based on a cost function that accounts for the number of input and output tokens processed. To achieve fairness in serving, we propose a novel scheduling algorithm, the Virtual Token Counter (VTC), a fair scheduler based on the continuous batching mechanism. We prove a 2x tight upper bound on the service difference between two backlogged clients, adhering to the requirement of work-conserving. Through extensive experiments, we demonstrate the superior performance of VTC in ensuring fairness, especially in contrast to other baseline methods, which exhibit shortcomings under various conditions. The reproducible code is available at https://github.com/Ying1123/VTC-artifact</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.00588">https://arxiv.org/abs/2401.00588</a></p>
            <p><strong>Category:</strong> cs.AI</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses LLM inference services and ways to improve their performance. Although not directly related to agent-based control or automation, it could still offer useful insight into LLM services and how they might be applied in different contexts, such as controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2401.06751" target="_blank">The Unreasonable Effectiveness of Easy Training Data for Hard Tasks</a></h3>
            <a href="https://arxiv.org/html/2401.06751v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2401.06751v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe</p>
            <p><strong>Summary:</strong> arXiv:2401.06751v2 Announce Type: replace-cross 
Abstract: How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied. Our code is available at: https://github.com/allenai/easy-to-hard-generalization</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2401.06751">https://arxiv.org/abs/2401.06751</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is in line with your interest in large language models and their capabilities. It delves into how models can understand and manage diverse complexity levels of question-answering tasks. However, its focus may not directly correlate with language models controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.04247" target="_blank">Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</a></h3>
            <a href="https://arxiv.org/html/2402.04247v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.04247v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein</p>
            <p><strong>Summary:</strong> arXiv:2402.04247v4 Announce Type: replace-cross 
Abstract: Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents, called scientific LLM agents, also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This perspective paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.04247">https://arxiv.org/abs/2402.04247</a></p>
            <p><strong>Category:</strong> cs.CY</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language model agents. It specifically discusses the risks associated with the autonomy of LLM agents in scientific contexts and proposes a framework for mitigating these risks. While it doesn't propose new methods per se, it illuminates the vulnerabilities and limitations of current models, which in turn has implications on the development of new and improved models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.14860" target="_blank">Ranking Large Language Models without Ground Truth</a></h3>
            <a href="https://arxiv.org/html/2402.14860v3/extracted/5646545/figures/triplet.png" target="_blank"><img src="https://arxiv.org/html/2402.14860v3/extracted/5646545/figures/triplet.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</p>
            <p><strong>Summary:</strong> arXiv:2402.14860v3 Announce Type: replace-cross 
Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.14860">https://arxiv.org/abs/2402.14860</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant because it discusses the evaluation and ranking of large language models, which is useful for understanding their functioning and performance. Even though it does not specifically address control or automation scenarios, the paper's content might be highly useful in setting up LLM-based agents for such tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.15337" target="_blank">Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies</a></h3>
            <a href="https://arxiv.org/html/2402.15337v2/extracted/5645730/scatter.png" target="_blank"><img src="https://arxiv.org/html/2402.15337v2/extracted/5645730/scatter.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nitesh Kumar, Usashi Chatterjee, Steven Schockaert</p>
            <p><strong>Summary:</strong> arXiv:2402.15337v2 Announce Type: replace-cross 
Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.15337">https://arxiv.org/abs/2402.15337</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the extraction of conceptual spaces from Large Language Models (LLMs), which could be indirectly related to controlling or automating software or web browsers using LLMs. However, the paper does not specifically mention these applications, hence a score of 4 instead of 5.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.17811" target="_blank">TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space</a></h3>
            <a href="https://arxiv.org/html/2402.17811v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.17811v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Shaolei Zhang, Tian Yu, Yang Feng</p>
            <p><strong>Summary:</strong> arXiv:2402.17811v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.17811">https://arxiv.org/abs/2402.17811</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method to enhance the truthfulness of language model-based responses, which can be valuable if you're considering the use of such models to control software or automate online browsing.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.15794" target="_blank">Large Language Models as In-context AI Generators for Quality-Diversity</a></h3>
            <a href="https://arxiv.org/html/2404.15794v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.15794v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bryan Lim, Manon Flageat, Antoine Cully</p>
            <p><strong>Summary:</strong> arXiv:2404.15794v2 Announce Type: replace-cross 
Abstract: Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches. While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions. As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search. In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations. We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using few-shot and many-shot prompting with quality-diverse examples from the QD archive as context. Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization. Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search. Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.15794">https://arxiv.org/abs/2404.15794</a></p>
            <p><strong>Category:</strong> cs.NE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper would fall under your interest of 'Agents based on large-language models' as it discusses the usage of large language models for generation of solutions. Although it doesn't directly discuss about control of software or web browsers, it provides an understanding on how large language models can be utilized in diverse applications, which could be useful in your research.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.00715" target="_blank">Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning</a></h3>
            <a href="https://arxiv.org/html/2405.00715v2/extracted/5644865/figure/Study_overview_step_v3.png" target="_blank"><img src="https://arxiv.org/html/2405.00715v2/extracted/5644865/figure/Study_overview_step_v3.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun</p>
            <p><strong>Summary:</strong> arXiv:2405.00715v2 Announce Type: replace-cross 
Abstract: Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). Our cost analysis for inference shows that our LLaMA-Clinic model achieves a 4.375-fold cost reduction compared to an external generic LLM service. Additionally, we highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.00715">https://arxiv.org/abs/2405.00715</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'Agents based on large-language models' topic as it discusses the task-specific adaptation process for a large language model and its application in a real-world scenario. It involves the control of a complex task (clinical note generation), which may correlate to an interest in the use of large language models in software and computer automation. However, it does not directly cover software or browser control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.12258" target="_blank">Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment</a></h3>
            
            <p><strong>Authors:</strong> Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross J. Collins, Elizabeth Bourne, Gareth W. Fearnley, Emma Tate, Holly X. Smith, Larisa N. Soldatova, Ross D. King</p>
            <p><strong>Summary:</strong> arXiv:2405.12258v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks that require human intelligence. In science, perhaps the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are 'hallucinations', and are a serious problem in many applications. However, in science, hallucinations may be useful: they are novel hypotheses whose validity may be tested by laboratory experiments. Here we experimentally test the use of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls. These combinations were itraconazole + atenolol, disulfiram + simvastatin and dipyridamole + mebendazole. GPT4 was then asked to generate new combinations after considering its initial results. It then discovered three more combinations with positive synergy scores (out of four tested), these were disulfiram + fulvestrant, mebendazole + quinacrine and disulfiram + quinacrine. A limitation of GPT4 as a generator of hypotheses was that its explanations for them were formulaic and unconvincing. We conclude that LLMs are an exciting novel source of scientific hypotheses.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.12258">https://arxiv.org/abs/2405.12258</a></p>
            <p><strong>Category:</strong> q-bio.QM</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper focuses on the application of large language models in generating scientific hypothesis which essentially entails control over web-based applications for hypothesis generation. However it doesn't propose new ways of using large language models.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.20974" target="_blank">SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</a></h3>
            <a href="https://arxiv.org/html/2405.20974v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.20974v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</p>
            <p><strong>Summary:</strong> arXiv:2405.20974v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.20974">https://arxiv.org/abs/2405.20974</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses training Large Language Models (LLMs) to express more accurate confidence estimates, which seems related to your interest in using LLMs for complex tasks like controlling software or web browsers. However, it doesn't specifically discuss these applications, hence the score of 4.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00059" target="_blank">Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution</a></h3>
            <a href="https://arxiv.org/html/2406.00059v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00059v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo</p>
            <p><strong>Summary:</strong> arXiv:2406.00059v2 Announce Type: replace-cross 
Abstract: The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00059">https://arxiv.org/abs/2406.00059</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to making software operations more efficient through the use of large language models. It discusses a new interface and request scheduler that improves request completion latency, touching your interest in using large language models for computer automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.00832" target="_blank">BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</a></h3>
            <a href="https://arxiv.org/html/2406.00832v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.00832v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Lin Gui, Cristina G\^arbacea, Victor Veitch</p>
            <p><strong>Summary:</strong> arXiv:2406.00832v2 Announce Type: replace-cross 
Abstract: This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.00832">https://arxiv.org/abs/2406.00832</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in large language models (LLMs) as it discusses aligning samples from LLMs to human preferences, and more importantly, the process of fine-tuning an LLM to mimic specific sampling distribution which could be vital for various automation functions with LLMs, including controlling software or web browsers.</p>
        </div>
        </div><div class='timestamp'>Report generated on June 06, 2024 at 21:50:13</div></body></html>