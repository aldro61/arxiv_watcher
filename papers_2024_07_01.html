
            <html>
            <head>
                <title>Report Generated on July 01, 2024</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 20px; }
                    .paper-box {
                        background-color: #f0f0f0;
                        margin-bottom: 20px;
                        padding: 15px;
                        border-radius: 5px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                    }
                    h1 { text-align: center; }
                    h2 {
                        cursor: pointer;
                        color: #333;
                        border-bottom: 2px solid #666;
                    }
                    a { color: #337ab7; text-decoration: none; }
                    a:hover { text-decoration: underline; }
                    h3 { color: #337ab7; }
                    .timestamp { text-align: center; font-size: small; margin-top: 40px; }
                    .paper-figure {
                        max-width: 200px;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 5px;
                        margin-top: 10px;
                    }
                    .papers-container { display: block; padding: 0 18px; }
                </style>
            </head>
            <body>
            <h1>Report for July 01, 2024</h1>
            <script>
                function toggleSection(id) {
                    var x = document.getElementById(id);
                    if (x.style.display === "none") {
                        x.style.display = "block";
                    } else {
                        x.style.display = "none";
                    }
                }
            </script>
            <h2 onclick="toggleSection('section_causality')">Causality</h2><div id='section_causality' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19573" target="_blank">On Counterfactual Interventions in Vector Autoregressive Models</a></h3>
            <a href="https://arxiv.org/html/2406.19573v1/extracted/5696873/Figures/interventions_cropped.png" target="_blank"><img src="https://arxiv.org/html/2406.19573v1/extracted/5696873/Figures/interventions_cropped.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Kurt Butler, Marija Iloska, Petar M. Djuric</p>
            <p><strong>Summary:</strong> arXiv:2406.19573v1 Announce Type: new 
Abstract: Counterfactual reasoning allows us to explore hypothetical scenarios in order to explain the impacts of our decisions. However, addressing such inquires is impossible without establishing the appropriate mathematical framework. In this work, we introduce the problem of counterfactual reasoning in the context of vector autoregressive (VAR) processes. We also formulate the inference of a causal model as a joint regression task where for inference we use both data with and without interventions. After learning the model, we exploit linearity of the VAR model to make exact predictions about the effects of counterfactual interventions. Furthermore, we quantify the total causal effects of past counterfactual interventions. The source code for this project is freely available at https://github.com/KurtButler/counterfactual_interventions.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19573">https://arxiv.org/abs/2406.19573</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> This paper discusses counterfactual reasoning in the context of vector autoregressive models, which relates to your interest in causal discovery. It also proposes a new model for counterfactual interventions, which aligns with your interest in new causal representation learning methods.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.08498" target="_blank">Learning Decision Policies with Instrumental Variables through Double Machine Learning</a></h3>
            <a href="https://arxiv.org/html/2405.08498v3/extracted/5698266/causal_graph.png" target="_blank"><img src="https://arxiv.org/html/2405.08498v3/extracted/5698266/causal_graph.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska</p>
            <p><strong>Summary:</strong> arXiv:2405.08498v3 Announce Type: replace 
Abstract: A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.08498">https://arxiv.org/abs/2405.08498</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> causality</p>
            <p><strong>Interest justification:</strong> The paper is highly relevant to your interest in 'Causality and machine learning.' It presents a new approach to learning causal relationships using Instrumental Variable (IV) regression, which addresses a key issue in causal discovery, one of your stated subtopics. While it doesn't directly mention large language models, it presents new methods rather than applications, which matches your preference.</p>
        </div>
        </div><h2 onclick="toggleSection('section_time-series')">Time-series</h2><div id='section_time-series' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19615" target="_blank">VarteX: Enhancing Weather Forecast through Distributed Variable Representation</a></h3>
            <a href="https://arxiv.org/html/2406.19615v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19615v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Ayumu Ueyama, Kazuhiko Kawamoto, Hiroshi Kera</p>
            <p><strong>Summary:</strong> arXiv:2406.19615v1 Announce Type: new 
Abstract: Weather forecasting is essential for various human activities. Recent data-driven models have outperformed numerical weather prediction by utilizing deep learning in forecasting performance. However, challenges remain in efficiently handling multiple meteorological variables. This study proposes a new variable aggregation scheme and an efficient learning framework for that challenge. Experiments show that VarteX outperforms the conventional model in forecast performance, requiring significantly fewer parameters and resources. The effectiveness of learning through multiple aggregations and regional split training is demonstrated, enabling more efficient and accurate deep learning-based weather forecasting.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19615">https://arxiv.org/abs/2406.19615</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interests because it introduces a new method for weather forecasting using deep learning. Although it doesn't specifically mention time-series modelling, the context of weather forecasting implies time-series data usage.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19770" target="_blank">Self-Supervised Spatial-Temporal Normality Learning for Time Series Anomaly Detection</a></h3>
            <a href="https://arxiv.org/html/2406.19770v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19770v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Yutong Chen, Hongzuo Xu, Guansong Pang, Hezhe Qiao, Yuan Zhou, Mingsheng Shang</p>
            <p><strong>Summary:</strong> arXiv:2406.19770v1 Announce Type: new 
Abstract: Time Series Anomaly Detection (TSAD) finds widespread applications across various domains such as financial markets, industrial production, and healthcare. Its primary objective is to learn the normal patterns of time series data, thereby identifying deviations in test samples. Most existing TSAD methods focus on modeling data from the temporal dimension, while ignoring the semantic information in the spatial dimension. To address this issue, we introduce a novel approach, called Spatial-Temporal Normality learning (STEN). STEN is composed of a sequence Order prediction-based Temporal Normality learning (OTN) module that captures the temporal correlations within sequences, and a Distance prediction-based Spatial Normality learning (DSN) module that learns the relative spatial relations between sequences in a feature space. By synthesizing these two modules, STEN learns expressive spatial-temporal representations for the normal patterns hidden in the time series data. Extensive experiments on five popular TSAD benchmarks show that STEN substantially outperforms state-of-the-art competing methods. Our code is available at https://github.com/mala-lab/STEN.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19770">https://arxiv.org/abs/2406.19770</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper proposes a new deep learning approach, STEN, for time series anomaly detection. It might not directly deal with forecasting, but it still provides valuable methodology on joint spatial-temporal representation learning which might be generalizable to forecasting tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19414" target="_blank">Stock Volume Forecasting with Advanced Information by Conditional Variational Auto-Encoder</a></h3>
            <a href="https://arxiv.org/html/2406.19414v1/extracted/5672425/FinalCodes/MainResult/visual/TSplot.png" target="_blank"><img src="https://arxiv.org/html/2406.19414v1/extracted/5672425/FinalCodes/MainResult/visual/TSplot.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Parley R Yang, Alexander Y Shestopaloff</p>
            <p><strong>Summary:</strong> arXiv:2406.19414v1 Announce Type: cross 
Abstract: We demonstrate the use of Conditional Variational Encoder (CVAE) to improve the forecasts of daily stock volume time series in both short and long term forecasting tasks, with the use of advanced information of input variables such as rebalancing dates. CVAE generates non-linear time series as out-of-sample forecasts, which have better accuracy and closer fit of correlation to the actual data, compared to traditional linear models. These generative forecasts can also be used for scenario generation, which aids interpretation. We further discuss correlations in non-stationary time series and other potential extensions from the CVAE forecasts.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19414">https://arxiv.org/abs/2406.19414</a></p>
            <p><strong>Category:</strong> q-fin.ST</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> This paper involves the use of a novel deep learning methodology, specifically Conditional Variational Autoencoder, for forecasting time series data. It might not propose a completely new method but it certainly proposes an advanced application for stock volume prediction. However, it leans more towards application.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2403.05743" target="_blank">Forecasting Electricity Market Signals via Generative AI</a></h3>
            <a href="https://arxiv.org/html/2403.05743v4/x1.png" target="_blank"><img src="https://arxiv.org/html/2403.05743v4/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xinyi Wang, Qing Zhao, Lang Tong</p>
            <p><strong>Summary:</strong> arXiv:2403.05743v4 Announce Type: replace-cross 
Abstract: This paper presents a generative artificial intelligence approach to probabilistic forecasting of electricity market signals, such as real-time locational marginal prices and area control error signals. Inspired by the Wiener-Kallianpur innovation representation of nonparametric time series, we propose a weak innovation autoencoder architecture and a novel deep learning algorithm that extracts the canonical independent and identically distributed innovation sequence of the time series, from which samples of future time series are generated. The validity of the proposed approach is established by proving that, under ideal training conditions, the generated samples have the same conditional probability distribution as that of the ground truth. Three applications involving highly dynamic and volatile time series in real-time market operations are considered: (i) locational marginal price forecasting for self-scheduled resources such as battery storage participants, (ii) interregional price spread forecasting for virtual bidders in interchange markets, and (iii) area control error forecasting for frequency regulations. Numerical studies based on market data from multiple independent system operators demonstrate the superior performance of the proposed generative forecaster over leading classical and modern machine learning techniques under both probabilistic and point forecasting metrics.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.05743">https://arxiv.org/abs/2403.05743</a></p>
            <p><strong>Category:</strong> eess.SP</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> time-series</p>
            <p><strong>Interest justification:</strong> The paper presents a novel deep learning algorithm for probabilistic forecasting of electricity market signals. Even though it doesn't mention new multimodal or transformer-like models specifically, it does align closely with your first subtopic of new deep learning methods for time series forecasting.</p>
        </div>
        </div><h2 onclick="toggleSection('section_llm-agents')">Llm-agents</h2><div id='section_llm-agents' class='papers-container'>
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19707" target="_blank">InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management</a></h3>
            <a href="https://arxiv.org/html/2406.19707v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19707v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Wonbeom Lee, Jungi Lee, Junghwan Seo, Jaewoong Sim</p>
            <p><strong>Summary:</strong> arXiv:2406.19707v1 Announce Type: new 
Abstract: Transformer-based large language models (LLMs) demonstrate impressive performance across various natural language processing tasks. Serving LLM inference for generating long contents, however, poses a challenge due to the enormous memory footprint of the transient state, known as the key-value (KV) cache, which scales with the sequence length and batch size. In this paper, we present InfiniGen, a novel KV cache management framework tailored for long-text generation, which synergistically works with modern offloading-based inference systems. InfiniGen leverages the key insight that a few important tokens that are essential for computing the subsequent attention layer in the Transformer can be speculated by performing a minimal rehearsal with the inputs of the current layer and part of the query weight and key cache of the subsequent layer. This allows us to prefetch only the essential KV cache entries (without fetching them all), thereby mitigating the fetch overhead from the host memory in offloading-based LLM serving systems. Our evaluation on several representative LLMs shows that InfiniGen improves the overall performance of a modern offloading-based system by up to 3.00x compared to prior KV cache management methods while offering substantially better model accuracy.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19707">https://arxiv.org/abs/2406.19707</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant as it discusses large language models (LLMs), but it does not specifically delve into the subtopics of controlling software, controlling web browsers, or computer automation using LLMs. However, its focus on LLMs may still provide useful general insights for these subtopics.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19976" target="_blank">ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting</a></h3>
            <a href="https://arxiv.org/html/2406.19976v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19976v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xiaoyu Wang, Tong Zhang</p>
            <p><strong>Summary:</strong> arXiv:2406.19976v1 Announce Type: new 
Abstract: Bilevel optimization has shown its utility across various machine learning settings, yet most algorithms in practice require second-order information, making it challenging to scale them up. Only recently, a paradigm of first-order algorithms emerged, capable of effectively addressing bilevel optimization problems. Nevertheless, the practical efficiency of this paradigm remains unverified, particularly in the context of large language models (LLMs). This paper introduces the first scalable instantiation of this paradigm called ScaleBiO, focusing on bilevel optimization for large-scale LLM data reweighting. By combining with a recently proposed memory-efficient training technique called LISA, our novel algorithm allows the paradigm to scale to 34-billion-parameter LLMs on eight A40 GPUs, marking the first successful application of bilevel optimization under practical scenarios for large-sized LLMs. Empirically, extensive experiments on data reweighting verify the effectiveness of ScaleBiO for different-scaled models, including GPT-2, LLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in filtering irrelevant data samples and selecting informative samples. Theoretically, ScaleBiO ensures the optimality of the learned data weights, along with a convergence guarantee matching the conventional first-order bilevel optimization paradigm on smooth and strongly convex objectives.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19976">https://arxiv.org/abs/2406.19976</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper showcases a novel algorithm for large-scale LLM data reweighting, suggesting potential utility for tasks involving large language models such as in controlling software or automating computer processes.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19486" target="_blank">LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models</a></h3>
            
            <p><strong>Authors:</strong> Shouchang Guo, Sonam Damani, Keng-hao Chang</p>
            <p><strong>Summary:</strong> arXiv:2406.19486v1 Announce Type: cross 
Abstract: In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the need for hand-crafted prompt engineering or explicit model fine-tuning. Prompt tuning is significantly more parameter-efficient than model fine-tuning, as it involves optimizing partial inputs of language models to produce desired outputs.
  In this work, we aim to further reduce the amount of trainable parameters required for a language model to perform well on specific tasks. We propose Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves efficient prompt optimization. The proposed method demonstrates similar outcomes to full parameter prompt tuning while reducing the number of trainable parameters by a factor of 5. It also provides promising results compared to the state-of-the-art methods that would require 10 to 20 times more parameters.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19486">https://arxiv.org/abs/2406.19486</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper may be of interest as it discusses efficient task-specific tuning of large language models. While it doesn't directly address your listed subtopics (controlling software or web browsers, automation), it introduces a method to optimize language models for specific tasks, which could potentially be applicable to these areas.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19501" target="_blank">Monitoring Latent World States in Language Models with Propositional Probes</a></h3>
            <a href="https://arxiv.org/html/2406.19501v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19501v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jiahai Feng, Stuart Russell, Jacob Steinhardt</p>
            <p><strong>Summary:</strong> arXiv:2406.19501v1 Announce Type: cross 
Abstract: Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We hypothesize that language models represent their input contexts in a latent world model, and seek to extract this latent world state from the activations. We do so with 'propositional probes', which compositionally probe tokens for lexical information and bind them into logical propositions representing the world state. For example, given the input context ''Greg is a nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg, nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to this is identifying a 'binding subspace' in which bound tokens have high similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and ''physicist''). We validate propositional probes in a closed-world setting with finitely many predicates and properties. Despite being trained on simple templated contexts, propositional probes generalize to contexts rewritten as short stories and translated to Spanish. Moreover, we find that in three settings where language models respond unfaithfully to the input context -- prompt injections, backdoor attacks, and gender bias -- the decoded propositions remain faithful. This suggests that language models often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19501">https://arxiv.org/abs/2406.19501</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper doesn't explicitly state its application to control software or web browsers, it does tackle the interpretability of large language models, which is crucial in building effective language-model-based agents. It proposes a method ('propositional probes') to understand internal states of language models, helping to correct unfaithful behaviour, an aspect that can be crucial in application areas like controlling software or automating tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19552" target="_blank">Rethinking harmless refusals when fine-tuning foundation models</a></h3>
            
            <p><strong>Authors:</strong> Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana</p>
            <p><strong>Summary:</strong> arXiv:2406.19552v1 Announce Type: cross 
Abstract: In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19552">https://arxiv.org/abs/2406.19552</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper focuses on Large Language Models (LLMs) and tries to regulate its behavior using fine-tuning, which is relevant to your interest in agents based on large-language models. It does not directly mention software or browser control but lays the groundwork for understanding and controlling LLM behavior.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19736" target="_blank">MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment</a></h3>
            <a href="https://arxiv.org/html/2406.19736v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.19736v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li</p>
            <p><strong>Summary:</strong> arXiv:2406.19736v1 Announce Type: cross 
Abstract: This paper introduces MM-Instruct, a large-scale dataset of diverse and high-quality visual instruction data designed to enhance the instruction-following capabilities of large multimodal models (LMMs). While existing visual instruction datasets often focus on question-answering, they struggle to generalize to broader application scenarios such as creative writing, summarization, or image analysis. To address these limitations, we propose a novel approach to constructing MM-Instruct that leverages the strong instruction-following capabilities of existing LLMs to generate novel visual instruction data from large-scale but conventional image captioning datasets. MM-Instruct first leverages ChatGPT to automatically generate diverse instructions from a small set of seed instructions through augmenting and summarization. It then matches these instructions with images and uses an open-sourced large language model (LLM) to generate coherent answers to the instruction-image pairs. The LLM is grounded by the detailed text descriptions of images in the whole answer generation process to guarantee the alignment of the instruction data. Moreover, we introduce a benchmark based on the generated instruction data to evaluate the instruction-following capabilities of existing LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5 model on the generated data, denoted as LLaVA-Instruct, which exhibits significant improvements in instruction-following capabilities compared to LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models are available at https://github.com/jihaonew/MM-Instruct.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19736">https://arxiv.org/abs/2406.19736</a></p>
            <p><strong>Category:</strong> cs.CV</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper is relevant to your interest in large-language model agents as it talks about training these models to follow instructions, which is a form of control. While it doesn't specifically mention software or web browser manipulation, the research could lead to advancements in the areas you're interested.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.19995" target="_blank">Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model</a></h3>
            <a href="https://arxiv.org/html/2406.19995v1/extracted/5688688/training_efficiency.png" target="_blank"><img src="https://arxiv.org/html/2406.19995v1/extracted/5688688/training_efficiency.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu</p>
            <p><strong>Summary:</strong> arXiv:2406.19995v1 Announce Type: cross 
Abstract: This paper introduces a novel method of Progressive Low Rank Decomposition (PLRD) tailored for the compression of large language models. Our approach leverages a pre-trained model, which is then incrementally decompressed to smaller sizes using progressively lower ranks. This method allows for significant reductions in computational overhead and energy consumption, as subsequent models are derived from the original without the need for retraining from scratch. We detail the implementation of PLRD, which strategically decreases the tensor ranks, thus optimizing the trade-off between model performance and resource usage. The efficacy of PLRD is demonstrated through extensive experiments showing that models trained with PLRD method on only 1B tokens maintain comparable performance with traditionally trained models while using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability to generate multiple model sizes from a single foundational model, adapting fluidly to varying computational and memory budgets. Our findings suggest that PLRD could set a new standard for the efficient scaling of LLMs, making advanced AI more feasible on diverse platforms.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.19995">https://arxiv.org/abs/2406.19995</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper presents a method for compressing large language models. While it does not directly address using Language models to control software/browsers, or specifically about automation, the efficient compression of these models can be considered a preliminary step in making these applications more convenient and viable.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.20094" target="_blank">Scaling Synthetic Data Creation with 1,000,000,000 Personas</a></h3>
            <a href="https://arxiv.org/html/2406.20094v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.20094v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</p>
            <p><strong>Summary:</strong> arXiv:2406.20094v1 Announce Type: cross 
Abstract: We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.20094">https://arxiv.org/abs/2406.20094</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although the paper is not directly related to controlling software or web browsers, it discusses the use of large language models (LLM) in generating diverse synthetic data at scale, which might be relevant to the development of intelligent LLM-based agents and automation. This paper could provide insights into how to leverage these personas, which could potentially be used to improve the performance of LLM-based agents.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.20095" target="_blank">LLaRA: Supercharging Robot Learning Data for Vision-Language Policy</a></h3>
            <a href="https://arxiv.org/html/2406.20095v1/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.20095v1/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo</p>
            <p><strong>Summary:</strong> arXiv:2406.20095v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) equipped with extensive world knowledge and strong reasoning skills can tackle diverse tasks across domains, often by posing them as conversation-style instruction-response pairs. In this paper, we propose LLaRA: Large Language and Robotics Assistant, a framework which formulates robot action policy as conversations, and provides improved responses when trained with auxiliary data that complements policy learning. LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and generate optimal policy decisions in text. To train such action policy VLMs, we first introduce an automated pipeline to generate diverse high-quality robotics instruction data from existing behavior cloning data. A VLM finetuned with the resulting collection of datasets based on a conversation-style formulation tailored for robotics tasks, can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.20095">https://arxiv.org/abs/2406.20095</a></p>
            <p><strong>Category:</strong> cs.RO</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper describes the use of Large Language Models (LLMs), specifically Vision Language Models, in generating robot action policy decisions. It aligns with your interest in computer automation via LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2311.09735" target="_blank">GEO: Generative Engine Optimization</a></h3>
            <a href="https://arxiv.org/html/2311.09735v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2311.09735v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande</p>
            <p><strong>Summary:</strong> arXiv:2311.09735v3 Announce Type: replace 
Abstract: The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improves $\textit{user}$ utility and $\textit{generative search engine}$ traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over $\textit{when}$ and $\textit{how}$ their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to $40\%$ in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2311.09735">https://arxiv.org/abs/2311.09735</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to the 'Large Language Models' topic. It discusses the use of these models in creating generative search engines that can gather, and summarize information to answer user queries in a personalized and accurate manner. However, the paper focuses more on Generative Engine Optimization (GEO) aimed at helping content creators improve their content visibility rather than controlling software or web browsers.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.08114" target="_blank">Active Preference Learning for Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2402.08114v2/extracted/5697631/figs/gpt-consistency.png" target="_blank"><img src="https://arxiv.org/html/2402.08114v2/extracted/5697631/figs/gpt-consistency.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> William Muldrew, Peter Hayes, Mingtian Zhang, David Barber</p>
            <p><strong>Summary:</strong> arXiv:2402.08114v2 Announce Type: replace 
Abstract: As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.08114">https://arxiv.org/abs/2402.08114</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper falls under your 'Agents based on large-language models' interest. It discusses how large language models can be fine-tuned to align with human intent, which can be relevant for controlling software and automating tasks. Even though it doesn't specifically mention software control or web browsers, the techniques harnessed could be potentially deployed in those domains.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.01124" target="_blank">Latent Logic Tree Extraction for Event Sequence Explanation from LLMs</a></h3>
            <a href="https://arxiv.org/html/2406.01124v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.01124v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li</p>
            <p><strong>Summary:</strong> arXiv:2406.01124v3 Announce Type: replace 
Abstract: Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.01124">https://arxiv.org/abs/2406.01124</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> Although this paper does not directly discuss using large language models to control software or web browsers, it demonstrates the application of LLMs in interpreting logic-based explanations for event sequences, which could be important for developing automation techniques using LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.12569" target="_blank">MOYU: A Theoretical Study on Massive Over-activation Yielded Uplifts in LLMs</a></h3>
            <a href="https://arxiv.org/html/2406.12569v2/extracted/5697539/image/TDA.png" target="_blank"><img src="https://arxiv.org/html/2406.12569v2/extracted/5697539/image/TDA.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Chi Ma, Mincong Huang, Chao Wang, Yujie Wang, Lei Yu</p>
            <p><strong>Summary:</strong> arXiv:2406.12569v2 Announce Type: replace 
Abstract: Massive Over-activation Yielded Uplifts(MOYU) is an inherent property of large language models, and dynamic activation(DA) based on the MOYU property is a clever yet under-explored strategy designed to accelerate inference in these models. Existing methods that utilize MOYU often face a significant 'Impossible Trinity': struggling to simultaneously maintain model performance, enhance inference speed, and extend applicability across various architectures. Due to the theoretical ambiguities surrounding MOYU, this paper elucidates the root cause of the MOYU property and outlines the mechanisms behind two primary limitations encountered by current DA methods: 1) history-related activation uncertainty, and 2) semantic-irrelevant activation inertia. Our analysis not only underscores the limitations of current dynamic activation strategies within large-scale LLaMA models but also proposes opportunities for refining the design of future sparsity schemes.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.12569">https://arxiv.org/abs/2406.12569</a></p>
            <p><strong>Category:</strong> cs.LG</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper shares useful insights into the properties of large language models (LLMs) and how to leverage them for improved performance and applicability. This is pertinent to your interest of agents based on large-language models, specifically in terms of refining model design to enhance the control of software, web browsers, and other automation tasks.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2305.14752" target="_blank">A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification</a></h3>
            <a href="https://arxiv.org/html/2305.14752v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2305.14752v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Norbert Tihanyi, Ridhi Jain, Yiannis Charalambous, Mohamed Amine Ferrag, Youcheng Sun, Lucas C. Cordeiro</p>
            <p><strong>Summary:</strong> arXiv:2305.14752v2 Announce Type: replace-cross 
Abstract: This paper introduces an innovative approach that combines Large Language Models (LLMs) with Formal Verification strategies for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. These counterexamples are supported by mathematical proofs and the stack trace of the vulnerabilities. Using a specially designed prompt, we combine the original source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50,000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI's capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2305.14752">https://arxiv.org/abs/2305.14752</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant to your interest in using large language models for computer automation. It presents the application of large language models for automatic software vulnerability repair. Although it does not focus on browser or software control, it provides useful insights into how large language models could be used for automation in software applications.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2307.10635" target="_blank">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</a></h3>
            
            <p><strong>Authors:</strong> Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</p>
            <p><strong>Summary:</strong> arXiv:2307.10635v3 Announce Type: replace-cross 
Abstract: Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2307.10635">https://arxiv.org/abs/2307.10635</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper evaluates the problem-solving abilities of Large Language Models in more advanced and complex tasks, which is relevant to your interest in the performance and capabilities of LLMs in the context of controlling software and automation. Although the paper does not directly address your specific subtopics, it provides insights into the strengths and weaknesses of LLMs.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2402.00093" target="_blank">ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation</a></h3>
            <a href="https://arxiv.org/html/2402.00093v3/x1.png" target="_blank"><img src="https://arxiv.org/html/2402.00093v3/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Bhabesh Mali, Karthik Maddala, Vatsal Gupta, Sweeya Reddy, Chandan Karfa, Ramesh Karri</p>
            <p><strong>Summary:</strong> arXiv:2402.00093v3 Announce Type: replace-cross 
Abstract: System Verilog Assertion (SVA) formulation -- a critical yet complex task is a prerequisite in the Assertion Based Verification (ABV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications, which is time-consuming and prone to human error. Recently, LLM-informed automatic assertion generation is gaining interest. We designed a novel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA from natural language specifications of a design. ChIRAAG constitutes the systematic breakdown of design specifications into a standardized format, further generating assertions from formatted specifications using LLM. Furthermore, we used few test cases to validate the LLM-generated assertions. Automatic feedback of log messages from the simulation tool to the LLM ensures that the framework can generate correct SVAs. In our experiments, only 27% of LLM-generated raw assertions had errors, which was rectified in few iterations based on the simulation log. Our results on OpenTitan designs show that LLMs can streamline and assist engineers in the assertion generation process, reshaping verification workflows.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2402.00093">https://arxiv.org/abs/2402.00093</a></p>
            <p><strong>Category:</strong> cs.SE</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models (LLMs) for the automation of System Verilog Assertion (SVA) formulation, which falls under 'computer automation using large language models'. The paper is highly relevant as it presents an application of an LLM in a software control context. However, it doesn't provide insights for controlling web browsers specifically.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2404.02319" target="_blank">Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization</a></h3>
            <a href="https://arxiv.org/html/2404.02319v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2404.02319v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Tobias Schnabel, Jennifer Neville</p>
            <p><strong>Summary:</strong> arXiv:2404.02319v2 Announce Type: replace-cross 
Abstract: In many modern LLM applications, such as retrieval augmented generation, prompts have become programs themselves. In these settings, prompt programs are repeatedly called with different user queries or data instances. A big practical challenge is optimizing such prompt programs. Recent work has mostly focused on either simple prompt programs or assumed that the general structure of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for compile-time optimizations of prompt programs. SAMMO represents prompt programs on a symbolic level which allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs. We make all code available open-source at https://github.com/microsoft/sammo .</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2404.02319">https://arxiv.org/abs/2404.02319</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> The paper discusses the use of large language models in optimizing prompt programs repeatedly called with different user queries or data instances, which is aligned with your interests in llm-agents. Specifically, it might offer insights about using large language models for complex tasks like software control or automation.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2405.14105" target="_blank">Distributed Speculative Inference of Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2405.14105v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2405.14105v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel</p>
            <p><strong>Summary:</strong> arXiv:2405.14105v2 Announce Type: replace-cross 
Abstract: Accelerating the inference of large language models (LLMs) is an important challenge in artificial intelligence. This paper introduces distributed speculative inference (DSI), a novel distributed inference algorithm that is provably faster than speculative inference (SI) [leviathan2023fast, chen2023accelerating, miao2023specinfer] and traditional autoregressive inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs, requiring no training or architectural modifications, and it preserves the target distribution.
  Prior studies on SI have demonstrated empirical speedups (compared to non-SI) but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs often do not have matching drafters that are sufficiently fast and accurate. We show a gap: SI gets slower than non-SI when using slower or less accurate drafters. We close this gap by proving that DSI is faster than both SI and non-SI given any drafters. By orchestrating multiple instances of the target and drafters, DSI is not only faster than SI but also supports LLMs that cannot be accelerated with SI.
  Our simulations show speedups of off-the-shelf LLMs in realistic settings: DSI is 1.29-1.92x faster than SI.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2405.14105">https://arxiv.org/abs/2405.14105</a></p>
            <p><strong>Category:</strong> cs.DC</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses about large language models, and introduces a new inference algorithm called the 'Distributed Speculative Inference'. Although it may not directly talk about computer automation or controlling software/browsers using large language models, its discussion and propositions about large language models could provide some foundational understanding and potential applications in your area of interest.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.15486" target="_blank">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a></h3>
            
            <p><strong>Authors:</strong> Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang</p>
            <p><strong>Summary:</strong> arXiv:2406.15486v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.15486">https://arxiv.org/abs/2406.15486</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper is relevant because it addresses the usage of large language models and proposes a technique to reduce latency. Although it doesn't specifically discuss control of software or web browsers, the reduction of latency is valuable for any application of large language models including agent control.</p>
        </div>
        
        <div class="paper-box">
            <h3><a href="https://arxiv.org/abs/2406.16783" target="_blank">M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models</a></h3>
            <a href="https://arxiv.org/html/2406.16783v2/x1.png" target="_blank"><img src="https://arxiv.org/html/2406.16783v2/x1.png" class="paper-figure" alt="Main Figure"></a>
            <p><strong>Authors:</strong> Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan</p>
            <p><strong>Summary:</strong> arXiv:2406.16783v2 Announce Type: replace-cross 
Abstract: Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. While many effective IFT datasets have been introduced recently, they predominantly focus on high-resource languages like English. To better align LLMs across a broad spectrum of languages and tasks, we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual, Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed by first selecting a diverse set of seed examples and then utilizing the proposed Evol taxonomy to convert these seeds into complex and challenging multi-turn instructions. We demonstrate the effectiveness of M2Lingual by training LLMs of varying sizes and showcasing the enhanced performance across a diverse set of languages. We contribute the 2 step Evol taxonomy with the guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the first fully synthetic, general and task-oriented, multi-turn, multilingual dataset built with Evol - M2Lingual: https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K total IFT pairs, covering 70 languages and 17+ NLP tasks.</p>
            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2406.16783">https://arxiv.org/abs/2406.16783</a></p>
            <p><strong>Category:</strong> cs.CL</p>
            <p><strong>Interest score:</strong> 4</p>
            <p><strong>Interest tag:</strong> llm-agents</p>
            <p><strong>Interest justification:</strong> This paper discusses the usage of large language models (LLMs) for instruction fine-tuning, which can be applied in controlling software or web browsers. The paper does not specifically outline automation, but the concept of using LLMs to follow instructions closely aligns with your interests.</p>
        </div>
        </div><div class='timestamp'>Report generated on July 01, 2024 at 21:32:18</div></body></html>